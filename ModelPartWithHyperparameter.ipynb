{
 "cells": [
  {
   "cell_type": "code",
   "id": "77398c5a-7c3a-403a-a80f-827d24e96a3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T06:15:00.272651Z",
     "start_time": "2025-11-21T06:15:00.267476Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾\n",
    "plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·\n",
    "# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# è®¾å¤‡é…ç½®\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½¿ç”¨è®¾å¤‡: cuda\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "d761af71-8480-4d68-af5e-a48ef74809a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T06:15:00.307320Z",
     "start_time": "2025-11-21T06:15:00.304269Z"
    }
   },
   "source": [
    "# class SentimentLSTM(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "#                  dropout, embedding_weights=None, train_embedding=True):\n",
    "#         super(SentimentLSTM, self).__init__()\n",
    "#\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.n_layers = n_layers\n",
    "#\n",
    "#         # åµŒå…¥å±‚\n",
    "#         if embedding_weights is not None:\n",
    "#             self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=not train_embedding)\n",
    "#         else:\n",
    "#             self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#\n",
    "#         # LSTMå±‚\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "#                            dropout=dropout, batch_first=True, bidirectional=False)\n",
    "#\n",
    "#         # Dropoutå±‚\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#\n",
    "#         # å…¨è¿æ¥å±‚\n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "#\n",
    "#         # Sigmoidæ¿€æ´»å‡½æ•°\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # åµŒå…¥å±‚\n",
    "#         embedded = self.embedding(x)\n",
    "#\n",
    "#         # LSTMå±‚\n",
    "#         lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "#\n",
    "#         # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º\n",
    "#         out = self.dropout(lstm_out[:, -1, :])\n",
    "#\n",
    "#         # å…¨è¿æ¥å±‚\n",
    "#         out = self.fc(out)\n",
    "#\n",
    "#         # Sigmoidæ¿€æ´»\n",
    "#         out = self.sigmoid(out)\n",
    "#\n",
    "#         return out.squeeze()"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "e65df397-d1f5-4227-8330-d1aabbffbb0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T06:15:00.324540Z",
     "start_time": "2025-11-21T06:15:00.318954Z"
    }
   },
   "source": [
    "# import json\n",
    "# import os\n",
    "#\n",
    "#\n",
    "# class SentimentAnalyzer:\n",
    "#     def __init__(self, device='cpu'):\n",
    "#         self.device = device\n",
    "#         self.vocab_size = 0\n",
    "#         self.model = None\n",
    "#\n",
    "#     def create_model(self, vocab_size, embedding_dim=100, hidden_dim=128, n_layers=2,\n",
    "#                     dropout=0.3, use_pretrained=False, glove_path=None, train_embedding=True):\n",
    "#         \"\"\"åˆ›å»ºLSTMæ¨¡å‹\"\"\"\n",
    "#         self.vocab_size = vocab_size\n",
    "#\n",
    "#         if use_pretrained and glove_path:\n",
    "#             embedding_weights = self.load_glove_embeddings(glove_path, embedding_dim, vocab_size)\n",
    "#             if embedding_weights is not None:\n",
    "#                 self.model = SentimentLSTM(\n",
    "#                     vocab_size, embedding_dim, hidden_dim, 1, n_layers,\n",
    "#                     dropout, embedding_weights, train_embedding\n",
    "#                 )\n",
    "#                 print(\"ä½¿ç”¨é¢„è®­ç»ƒGloVeè¯å‘é‡çš„æ¨¡å‹å·²åˆ›å»º\")\n",
    "#             else:\n",
    "#                 print(\"æ— æ³•åŠ è½½é¢„è®­ç»ƒè¯å‘é‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–\")\n",
    "#                 use_pretrained = False\n",
    "#\n",
    "#         if not use_pretrained:\n",
    "#             self.model = SentimentLSTM(\n",
    "#                 vocab_size, embedding_dim, hidden_dim, 1, n_layers,\n",
    "#                 dropout, None, True\n",
    "#             )\n",
    "#             print(\"ä½¿ç”¨éšæœºåˆå§‹åŒ–è¯å‘é‡çš„æ¨¡å‹å·²åˆ›å»º\")\n",
    "#\n",
    "#         self.model = self.model.to(self.device)\n",
    "#         return self.model\n",
    "#\n",
    "#     def load_glove_embeddings(self, glove_path, embedding_dim, vocab_size):\n",
    "#         \"\"\"åŠ è½½é¢„è®­ç»ƒçš„GloVeè¯å‘é‡\"\"\"\n",
    "#         print(\"åŠ è½½GloVeè¯å‘é‡...\")\n",
    "#\n",
    "#         # ç”±äºæˆ‘ä»¬ä¸çŸ¥é“ç¡®åˆ‡çš„è¯æ±‡è¡¨ï¼Œæˆ‘ä»¬å‡è®¾ç´¢å¼•0-29999å¯¹åº”è¯æ±‡è¡¨ä¸­çš„è¯\n",
    "#         embedding_weights = np.random.normal(scale=0.6, size=(vocab_size, embedding_dim))\n",
    "#\n",
    "#         try:\n",
    "#             glove_embeddings = {}\n",
    "#             with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "#                 for line in f:\n",
    "#                     values = line.split()\n",
    "#                     word = values[0]\n",
    "#                     vector = np.asarray(values[1:], dtype='float32')\n",
    "#                     glove_embeddings[word] = vector\n",
    "#\n",
    "#             print(\"æ³¨æ„: ç”±äºç¼ºä¹ç´¢å¼•åˆ°è¯çš„æ˜ å°„ï¼Œæ— æ³•ç²¾ç¡®åŠ è½½GloVeè¯å‘é‡\")\n",
    "#             print(\"ä½¿ç”¨éšæœºåˆå§‹åŒ–æ›¿ä»£\")\n",
    "#\n",
    "#         except FileNotFoundError:\n",
    "#             print(f\"GloVeæ–‡ä»¶æœªæ‰¾åˆ°: {glove_path}\")\n",
    "#             print(\"è¯·ä» https://nlp.stanford.edu/projects/glove/ ä¸‹è½½GloVeè¯å‘é‡\")\n",
    "#             return None\n",
    "#\n",
    "#         return torch.tensor(embedding_weights, dtype=torch.float32)\n",
    "#\n",
    "#     def train_model(self, train_loader, val_loader, num_epochs=10, learning_rate=0.001):\n",
    "#         \"\"\"è®­ç»ƒæ¨¡å‹\"\"\"\n",
    "#         criterion = nn.BCELoss()\n",
    "#         optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "#\n",
    "#         train_losses = []\n",
    "#         val_losses = []\n",
    "#         train_accuracies = []\n",
    "#         val_accuracies = []\n",
    "#\n",
    "#         best_val_loss = float('inf')\n",
    "#         patience = 3\n",
    "#         patience_counter = 0\n",
    "#\n",
    "#         for epoch in range(num_epochs):\n",
    "#             start_time = time.time()\n",
    "#\n",
    "#             # è®­ç»ƒé˜¶æ®µ\n",
    "#             self.model.train()\n",
    "#             train_loss = 0.0\n",
    "#             train_correct = 0\n",
    "#             train_total = 0\n",
    "#\n",
    "#             for batch_idx, (sequences, masks, labels, lengths) in enumerate(train_loader):\n",
    "#                 sequences = sequences.to(self.device)\n",
    "#                 labels = labels.to(self.device).float()\n",
    "#\n",
    "#                 optimizer.zero_grad()\n",
    "#                 outputs = self.model(sequences)  # ä¸å†ä¼ é€’lengthså‚æ•°\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#\n",
    "#                 train_loss += loss.item()\n",
    "#                 predicted = (outputs > 0.5).float()\n",
    "#                 train_total += labels.size(0)\n",
    "#                 train_correct += (predicted == labels).sum().item()\n",
    "#\n",
    "#                 # æ¯100ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦\n",
    "#                 if batch_idx % 100 == 0:\n",
    "#                     print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "#\n",
    "#             # éªŒè¯é˜¶æ®µ\n",
    "#             self.model.eval()\n",
    "#             val_loss = 0.0\n",
    "#             val_correct = 0\n",
    "#             val_total = 0\n",
    "#\n",
    "#             with torch.no_grad():\n",
    "#                 for sequences, masks, labels, lengths in val_loader:\n",
    "#                     sequences = sequences.to(self.device)\n",
    "#                     labels = labels.to(self.device).float()\n",
    "#\n",
    "#                     outputs = self.model(sequences)  # ä¸å†ä¼ é€’lengthså‚æ•°\n",
    "#                     loss = criterion(outputs, labels)\n",
    "#\n",
    "#                     val_loss += loss.item()\n",
    "#                     predicted = (outputs > 0.5).float()\n",
    "#                     val_total += labels.size(0)\n",
    "#                     val_correct += (predicted == labels).sum().item()\n",
    "#\n",
    "#             # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡\n",
    "#             train_loss_avg = train_loss / len(train_loader)\n",
    "#             val_loss_avg = val_loss / len(val_loader)\n",
    "#             train_accuracy = train_correct / train_total\n",
    "#             val_accuracy = val_correct / val_total\n",
    "#\n",
    "#             train_losses.append(train_loss_avg)\n",
    "#             val_losses.append(val_loss_avg)\n",
    "#             train_accuracies.append(train_accuracy)\n",
    "#             val_accuracies.append(val_accuracy)\n",
    "#\n",
    "#             epoch_time = time.time() - start_time\n",
    "#\n",
    "#             print(f'Epoch [{epoch+1}/{num_epochs}], Time: {epoch_time:.2f}s')\n",
    "#             print(f'  è®­ç»ƒæŸå¤±: {train_loss_avg:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.4f}')\n",
    "#             print(f'  éªŒè¯æŸå¤±: {val_loss_avg:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}')\n",
    "#\n",
    "#             # æ—©åœæ³•\n",
    "#             if val_loss_avg < best_val_loss:\n",
    "#                 best_val_loss = val_loss_avg\n",
    "#                 patience_counter = 0\n",
    "#                 # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "#                 torch.save(self.model.state_dict(), 'best_model.pth')\n",
    "#             else:\n",
    "#                 patience_counter += 1\n",
    "#                 if patience_counter >= patience:\n",
    "#                     print(f\"æ—©åœï¼šéªŒè¯æŸå¤±åœ¨ {patience} ä¸ªepochå†…æ²¡æœ‰æ”¹å–„\")\n",
    "#                     break\n",
    "#\n",
    "#         # åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "#         # åœ¨ä»£ç ä¸­æ‰¾åˆ°è¿™è¡Œå¹¶ä¿®æ”¹\n",
    "#         self.model.load_state_dict(torch.load('best_model.pth', weights_only=True))\n",
    "#\n",
    "#         return {\n",
    "#             'train_losses': train_losses,\n",
    "#             'val_losses': val_losses,\n",
    "#             'train_accuracies': train_accuracies,\n",
    "#             'val_accuracies': val_accuracies\n",
    "#         }\n",
    "#\n",
    "#     def evaluate_model(self, test_loader):\n",
    "#         \"\"\"è¯„ä¼°æ¨¡å‹\"\"\"\n",
    "#         self.model.eval()\n",
    "#         all_predictions = []\n",
    "#         all_targets = []\n",
    "#\n",
    "#         with torch.no_grad():\n",
    "#             for sequences, masks, labels, lengths in test_loader:\n",
    "#                 sequences = sequences.to(self.device)\n",
    "#                 labels = labels.to(self.device)\n",
    "#\n",
    "#                 outputs = self.model(sequences)  # ä¸å†ä¼ é€’lengthså‚æ•°\n",
    "#                 predictions = (outputs > 0.5).float()\n",
    "#\n",
    "#                 all_predictions.extend(predictions.cpu().numpy())\n",
    "#                 all_targets.extend(labels.cpu().numpy())\n",
    "#\n",
    "#         # è®¡ç®—è¯„ä¼°æŒ‡æ ‡\n",
    "#         accuracy = accuracy_score(all_targets, all_predictions)\n",
    "#         precision = precision_score(all_targets, all_predictions, zero_division=0)\n",
    "#         recall = recall_score(all_targets, all_predictions, zero_division=0)\n",
    "#         f1 = f1_score(all_targets, all_predictions, zero_division=0)\n",
    "#\n",
    "#         print(\"\\n\" + \"=\"*50)\n",
    "#         print(\"æ¨¡å‹è¯„ä¼°ç»“æœ\")\n",
    "#         print(\"=\"*50)\n",
    "#         print(f\"å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}\")\n",
    "#         print(f\"ç²¾ç¡®ç‡ (Precision): {precision:.4f}\")\n",
    "#         print(f\"å¬å›ç‡ (Recall): {recall:.4f}\")\n",
    "#         print(f\"F1åˆ†æ•°: {f1:.4f}\")\n",
    "#         print(\"\\nåˆ†ç±»æŠ¥å‘Š:\")\n",
    "#         print(classification_report(all_targets, all_predictions, target_names=['è´Ÿé¢', 'æ­£é¢']))\n",
    "#\n",
    "#         return {\n",
    "#             'accuracy': accuracy,\n",
    "#             'precision': precision,\n",
    "#             'recall': recall,\n",
    "#             'f1': f1,\n",
    "#             'predictions': all_predictions,\n",
    "#             'targets': all_targets\n",
    "#         }"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T06:15:00.341661Z",
     "start_time": "2025-11-21T06:15:00.332492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# è®¾å¤‡é…ç½®\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"LSTMæƒ…æ„Ÿåˆ†ææ¨¡å‹\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 dropout, embedding_weights=None, train_embedding=True):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # åµŒå…¥å±‚\n",
    "        if embedding_weights is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=not train_embedding)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTMå±‚\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                           dropout=dropout, batch_first=True, bidirectional=False)\n",
    "\n",
    "        # Dropoutå±‚\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # å…¨è¿æ¥å±‚\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Sigmoidæ¿€æ´»å‡½æ•°\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # åµŒå…¥å±‚\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # LSTMå±‚\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º\n",
    "        out = self.dropout(lstm_out[:, -1, :])\n",
    "\n",
    "        # å…¨è¿æ¥å±‚\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # Sigmoidæ¿€æ´»\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out.squeeze()\n",
    "\n",
    "\n",
    "\n",
    "def plot_training_history(history, title=\"è®­ç»ƒå†å²\"):\n",
    "    \"\"\"ç»˜åˆ¶è®­ç»ƒå†å²æ›²çº¿\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # æŸå¤±æ›²çº¿\n",
    "    ax1.plot(history['train_losses'], label='è®­ç»ƒæŸå¤±', linewidth=2)\n",
    "    ax1.plot(history['val_losses'], label='éªŒè¯æŸå¤±', linewidth=2)\n",
    "    ax1.set_title(f'{title} - æŸå¤±æ›²çº¿')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # å‡†ç¡®ç‡æ›²çº¿\n",
    "    ax2.plot(history['train_accuracies'], label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)\n",
    "    ax2.plot(history['val_accuracies'], label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)\n",
    "    ax2.set_title(f'{title} - å‡†ç¡®ç‡æ›²çº¿')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "ce06d989bc31d4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½¿ç”¨è®¾å¤‡: cuda\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T06:15:00.369566Z",
     "start_time": "2025-11-21T06:15:00.348287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SentimentAnalyzer:\n",
    "    \"\"\"å®Œæ•´çš„æƒ…æ„Ÿåˆ†æå™¨ç±»\"\"\"\n",
    "\n",
    "    def __init__(self, device='cpu'):\n",
    "        self.device = device\n",
    "        self.vocab_size = 0\n",
    "        self.model = None\n",
    "        # è®­ç»ƒæŒ‡æ ‡è®°å½•\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "        self.best_epoch = 0\n",
    "        self.training_time = 0\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        self.current_params = None\n",
    "        self.metrics = {}\n",
    "\n",
    "    def create_model(self, vocab_size, embedding_dim=100, hidden_dim=128, n_layers=2,\n",
    "                    dropout=0.3, use_pretrained=True, glove_path=\"glove.6B.100d.txt\", train_embedding=True):\n",
    "        \"\"\"åˆ›å»ºLSTMæ¨¡å‹\"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.current_params = {\n",
    "            'vocab_size': vocab_size,\n",
    "            'embedding_dim': embedding_dim,\n",
    "            'hidden_dim': hidden_dim,\n",
    "            'n_layers': n_layers,\n",
    "            'dropout': dropout,\n",
    "            'use_pretrained': use_pretrained,\n",
    "            'train_embedding': train_embedding\n",
    "        }\n",
    "\n",
    "        if use_pretrained and glove_path:\n",
    "            embedding_weights = self._load_glove_embeddings(glove_path, embedding_dim, vocab_size)\n",
    "            if embedding_weights is not None:\n",
    "                self.model = SentimentLSTM(\n",
    "                    vocab_size, embedding_dim, hidden_dim, 1, n_layers,\n",
    "                    dropout, embedding_weights, train_embedding\n",
    "                )\n",
    "                print(\"ä½¿ç”¨é¢„è®­ç»ƒGloVeè¯å‘é‡çš„æ¨¡å‹å·²åˆ›å»º\")\n",
    "            else:\n",
    "                print(\"æ— æ³•åŠ è½½é¢„è®­ç»ƒè¯å‘é‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–\")\n",
    "                use_pretrained = False\n",
    "\n",
    "        if not use_pretrained:\n",
    "            self.model = SentimentLSTM(\n",
    "                vocab_size, embedding_dim, hidden_dim, 1, n_layers,\n",
    "                dropout, None, True\n",
    "            )\n",
    "            print(\"ä½¿ç”¨éšæœºåˆå§‹åŒ–è¯å‘é‡çš„æ¨¡å‹å·²åˆ›å»º\")\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "        return self.model\n",
    "\n",
    "    def _load_glove_embeddings(self, glove_path, embedding_dim, vocab_size):\n",
    "        \"\"\"åŠ è½½é¢„è®­ç»ƒçš„GloVeè¯å‘é‡\"\"\"\n",
    "        print(\"åŠ è½½GloVeè¯å‘é‡...\")\n",
    "\n",
    "        # åˆ›å»ºéšæœºåˆå§‹åŒ–çš„è¯å‘é‡ï¼ˆç”±äºç¼ºä¹è¯æ±‡æ˜ å°„ï¼‰\n",
    "        embedding_weights = np.random.normal(scale=0.6, size=(vocab_size, embedding_dim))\n",
    "\n",
    "        try:\n",
    "            # å°è¯•åŠ è½½GloVeæ–‡ä»¶\n",
    "            glove_embeddings = {}\n",
    "            with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    values = line.split()\n",
    "                    if len(values) < embedding_dim + 1:\n",
    "                        continue\n",
    "                    word = values[0]\n",
    "                    vector = np.asarray(values[1:embedding_dim+1], dtype='float32')\n",
    "                    glove_embeddings[word] = vector\n",
    "\n",
    "            print(f\"æˆåŠŸåŠ è½½ {len(glove_embeddings)} ä¸ªGloVeè¯å‘é‡\")\n",
    "            print(\"æ³¨æ„: ç”±äºç¼ºä¹ç´¢å¼•åˆ°è¯çš„ç²¾ç¡®æ˜ å°„ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–æ›¿ä»£å®Œæ•´GloVeåŠ è½½\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"GloVeæ–‡ä»¶æœªæ‰¾åˆ°: {glove_path}\")\n",
    "            # print(\"è¯·ä» https://nlp.stanford.edu/projects/glove/ ä¸‹è½½GloVeè¯å‘é‡\")\n",
    "            return None\n",
    "\n",
    "        return torch.tensor(embedding_weights, dtype=torch.float32)\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, num_epochs=10, learning_rate=0.001,\n",
    "                   early_stopping_patience=3, save_path='best_model.pth'):\n",
    "        \"\"\"è®­ç»ƒæ¨¡å‹\"\"\"\n",
    "        # åˆå§‹åŒ–è®­ç»ƒæŒ‡æ ‡\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "        self.best_epoch = 0\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        print(f\"å¼€å§‹è®­ç»ƒï¼Œå­¦ä¹ ç‡: {learning_rate}, æ—©åœè€å¿ƒå€¼: {early_stopping_patience}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "\n",
    "            # è®­ç»ƒé˜¶æ®µ\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "\n",
    "            for batch_idx, (sequences, masks, labels, lengths) in enumerate(train_loader):\n",
    "                sequences = sequences.to(self.device)\n",
    "                labels = labels.to(self.device).float()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡\n",
    "            train_accuracy = train_correct / train_total\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "            # éªŒè¯é˜¶æ®µ\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for sequences, masks, labels, lengths in val_loader:\n",
    "                    sequences = sequences.to(self.device)\n",
    "                    labels = labels.to(self.device).float()\n",
    "\n",
    "                    outputs = self.model(sequences)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    predicted = (outputs > 0.5).float()\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_accuracy = val_correct / val_total\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "            # è®°å½•æŒ‡æ ‡\n",
    "            self.train_losses.append(avg_train_loss)\n",
    "            self.val_losses.append(avg_val_loss)\n",
    "            self.train_accuracies.append(train_accuracy)\n",
    "            self.val_accuracies.append(val_accuracy)\n",
    "\n",
    "            # æ—©åœæ³•\n",
    "            if avg_val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = avg_val_loss\n",
    "                self.best_epoch = epoch + 1\n",
    "                self.patience_counter = 0\n",
    "                torch.save(self.model.state_dict(), save_path)\n",
    "                print(f\"ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {avg_val_loss:.4f}\")\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "            # æ‰“å°è¿›åº¦\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Time: {epoch_time:.2f}s')\n",
    "            print(f'  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.4f}')\n",
    "            print(f'  éªŒè¯æŸå¤±: {avg_val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}')\n",
    "            print(f'  æœ€ä½³epoch: {self.best_epoch}, æ—©åœè®¡æ•°: {self.patience_counter}/{early_stopping_patience}')\n",
    "\n",
    "            # æ£€æŸ¥æ—©åœ\n",
    "            if self.patience_counter >= early_stopping_patience:\n",
    "                print(f\"æ—©åœè§¦å‘äºç¬¬ {epoch+1} ä¸ªepoch\")\n",
    "                break\n",
    "\n",
    "        # è®°å½•æ€»è®­ç»ƒæ—¶é—´\n",
    "        self.training_time = time.time() - start_time\n",
    "\n",
    "        # åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "        self.model.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "\n",
    "        print(f\"è®­ç»ƒå®Œæˆï¼Œæ€»æ—¶é—´: {self.training_time:.2f}ç§’\")\n",
    "        return {\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'train_accuracies': self.train_accuracies,\n",
    "            'val_accuracies': self.val_accuracies\n",
    "        }\n",
    "\n",
    "    def evaluate_model(self, test_loader):\n",
    "        \"\"\"è¯„ä¼°æ¨¡å‹æ€§èƒ½\"\"\"\n",
    "        self.model.eval()\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        all_probabilities = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sequences, masks, labels, lengths in test_loader:\n",
    "                sequences = sequences.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                outputs = self.model(sequences)\n",
    "                probabilities = outputs.cpu().numpy()\n",
    "                predictions = (outputs > 0.5).float().cpu().numpy()\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_targets.extend(labels.cpu().numpy())\n",
    "                all_probabilities.extend(probabilities)\n",
    "\n",
    "        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡\n",
    "        accuracy = accuracy_score(all_targets, all_predictions)\n",
    "        precision = precision_score(all_targets, all_predictions, zero_division=0)\n",
    "        recall = recall_score(all_targets, all_predictions, zero_division=0)\n",
    "        f1 = f1_score(all_targets, all_predictions, zero_division=0)\n",
    "\n",
    "        # ä¿å­˜æŒ‡æ ‡\n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'predictions': all_predictions,\n",
    "            'targets': all_targets,\n",
    "            'probabilities': all_probabilities\n",
    "        }\n",
    "\n",
    "        # æ‰“å°ç»“æœ\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"æ¨¡å‹è¯„ä¼°ç»“æœ\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}\")\n",
    "        print(f\"ç²¾ç¡®ç‡ (Precision): {precision:.4f}\")\n",
    "        print(f\"å¬å›ç‡ (Recall): {recall:.4f}\")\n",
    "        print(f\"F1åˆ†æ•°: {f1:.4f}\")\n",
    "        print(\"\\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\n",
    "        print(classification_report(all_targets, all_predictions,\n",
    "                                  target_names=['è´Ÿé¢', 'æ­£é¢'], digits=4))\n",
    "\n",
    "        return self.metrics\n",
    "\n",
    "    def predict_sentiment(self, text_sequences):\n",
    "        \"\"\"é¢„æµ‹æ–°æ–‡æœ¬çš„æƒ…æ„Ÿ\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            sequences = text_sequences.to(self.device)\n",
    "            outputs = self.model(sequences)\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            probabilities = outputs.cpu().numpy()\n",
    "        return predictions.cpu().numpy(), probabilities\n",
    "\n",
    "    def generate_report(self, report_dir='reports', filename='analysis_report.html'):\n",
    "        \"\"\"ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        os.makedirs(report_dir, exist_ok=True)\n",
    "\n",
    "        # æ”¶é›†æŠ¥å‘Šæ•°æ®\n",
    "        report_data = {\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'best_epoch': self.best_epoch,\n",
    "            'training_time': self.training_time,\n",
    "            'parameters': self.current_params,\n",
    "            'metrics': self.metrics\n",
    "        }\n",
    "\n",
    "        # ç”ŸæˆHTMLæŠ¥å‘Š\n",
    "        html_content = f\"\"\"\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>æƒ…æ„Ÿåˆ†ææ¨¡å‹å®éªŒæŠ¥å‘Š</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}\n",
    "                h1, h2, h3 {{ color: #2c3e50; }}\n",
    "                .container {{ max-width: 1200px; margin: 0 auto; }}\n",
    "                .metric-card {{ background: #f8f9fa; padding: 20px; margin: 10px; border-radius: 5px; }}\n",
    "                .grid {{ display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; }}\n",
    "                table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}\n",
    "                th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}\n",
    "                th {{ background-color: #3498db; color: white; }}\n",
    "                tr:nth-child(even) {{ background-color: #f2f2f2; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"container\">\n",
    "                <h1>ğŸ§  æƒ…æ„Ÿåˆ†ææ¨¡å‹å®éªŒæŠ¥å‘Š</h1>\n",
    "\n",
    "                <h2>ğŸ“Š å®éªŒæ¦‚è§ˆ</h2>\n",
    "                <div class=\"grid\">\n",
    "                    <div class=\"metric-card\">\n",
    "                        <h3>æœ€ä½³éªŒè¯æŸå¤±</h3>\n",
    "                        <p>{report_data['best_val_loss']:.4f}</p>\n",
    "                    </div>\n",
    "                    <div class=\"metric-card\">\n",
    "                        <h3>æœ€ä½³Epoch</h3>\n",
    "                        <p>{report_data['best_epoch']}</p>\n",
    "                    </div>\n",
    "                    <div class=\"metric-card\">\n",
    "                        <h3>è®­ç»ƒæ—¶é—´</h3>\n",
    "                        <p>{report_data['training_time']:.2f}ç§’</p>\n",
    "                    </div>\n",
    "                    <div class=\"metric-card\">\n",
    "                        <h3>æœ€ç»ˆå‡†ç¡®ç‡</h3>\n",
    "                        <p>{report_data['metrics'].get('accuracy', 0):.4f}</p>\n",
    "                    </div>\n",
    "                </div>\n",
    "\n",
    "                <h2>âš™ï¸ æ¨¡å‹å‚æ•°</h2>\n",
    "                <pre>{json.dumps(report_data['parameters'], indent=2, ensure_ascii=False)}</pre>\n",
    "\n",
    "                <h2>ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡</h2>\n",
    "                <table>\n",
    "                    <tr><th>æŒ‡æ ‡</th><th>æ•°å€¼</th></tr>\n",
    "                    <tr><td>å‡†ç¡®ç‡ (Accuracy)</td><td>{report_data['metrics'].get('accuracy', 0):.4f}</td></tr>\n",
    "                    <tr><td>ç²¾ç¡®ç‡ (Precision)</td><td>{report_data['metrics'].get('precision', 0):.4f}</td></tr>\n",
    "                    <tr><td>å¬å›ç‡ (Recall)</td><td>{report_data['metrics'].get('recall', 0):.4f}</td></tr>\n",
    "                    <tr><td>F1åˆ†æ•°</td><td>{report_data['metrics'].get('f1', 0):.4f}</td></tr>\n",
    "                </table>\n",
    "\n",
    "                <h2>ğŸ“‰ è®­ç»ƒæ›²çº¿</h2>\n",
    "                <img src=\"training_curves.png\" width=\"100%\">\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "\n",
    "        # ä¿å­˜HTMLæŠ¥å‘Š\n",
    "        report_path = os.path.join(report_dir, filename)\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "\n",
    "        # ç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾\n",
    "        self._plot_training_curves(report_dir)\n",
    "\n",
    "        print(f\"ğŸ“„ åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}\")\n",
    "        return report_path\n",
    "\n",
    "    def _plot_training_curves(self, save_dir):\n",
    "        \"\"\"ç»˜åˆ¶è®­ç»ƒæ›²çº¿\"\"\"\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # æŸå¤±æ›²çº¿\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.train_losses, label='è®­ç»ƒæŸå¤±', linewidth=2)\n",
    "        plt.plot(self.val_losses, label='éªŒè¯æŸå¤±', linewidth=2)\n",
    "        plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # å‡†ç¡®ç‡æ›²çº¿\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.train_accuracies, label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)\n",
    "        plt.plot(self.val_accuracies, label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)\n",
    "        plt.title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡æ›²çº¿')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, 'training_curves.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n"
   ],
   "id": "72976151825a1747",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T06:15:00.390019Z",
     "start_time": "2025-11-21T06:15:00.376770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ExperimentManager:\n",
    "    \"\"\"å®éªŒç®¡ç†å™¨ - ç”¨äºè¶…å‚æ•°è°ƒä¼˜å’Œå¯¹æ¯”å®éªŒ\"\"\"\n",
    "\n",
    "    def __init__(self, device='cpu'):\n",
    "        self.device = device\n",
    "        self.experiment_results = []\n",
    "        self.best_experiment = None\n",
    "\n",
    "    def run_hyperparameter_search(self, train_loader, val_loader, test_loader, vocab_size,\n",
    "                                 param_grid=None, max_experiments=20):\n",
    "        \"\"\"è¿è¡Œè¶…å‚æ•°æœç´¢\"\"\"\n",
    "        if param_grid is None:\n",
    "            param_grid = {\n",
    "                'learning_rate': [0.0001, 0.0005, 0.001],\n",
    "                'batch_size': [32, 64],\n",
    "                'dropout_rate': [0.2, 0.3, 0.5],\n",
    "                'hidden_dim': [64, 128],\n",
    "                'n_layers': [1, 2],\n",
    "                'use_pretrained': [True, False]\n",
    "            }\n",
    "\n",
    "        print(\"å¼€å§‹è¶…å‚æ•°æœç´¢å®éªŒ...\")\n",
    "        experiment_count = 0\n",
    "\n",
    "        # ç”Ÿæˆå‚æ•°ç»„åˆ\n",
    "        param_combinations = self._generate_param_combinations(param_grid, max_experiments)\n",
    "\n",
    "        for i, params in enumerate(param_combinations):\n",
    "            if experiment_count >= max_experiments:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                print(f\"\\nğŸ”¬ å®éªŒ {i+1}/{len(param_combinations)}\")\n",
    "                print(f\"å‚æ•°: {params}\")\n",
    "\n",
    "                # åˆ›å»ºæ–°çš„æ•°æ®åŠ è½½å™¨ï¼ˆé’ˆå¯¹ä¸åŒçš„batch_sizeï¼‰\n",
    "                batch_size = params['batch_size']\n",
    "                current_train_loader = DataLoader(\n",
    "                    train_loader.dataset, batch_size=batch_size, shuffle=True\n",
    "                )\n",
    "                current_val_loader = DataLoader(\n",
    "                    val_loader.dataset, batch_size=batch_size, shuffle=False\n",
    "                )\n",
    "                current_test_loader = DataLoader(\n",
    "                    test_loader.dataset, batch_size=batch_size, shuffle=False\n",
    "                )\n",
    "\n",
    "                # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹\n",
    "                analyzer = SentimentAnalyzer(device=self.device)\n",
    "                analyzer.create_model(\n",
    "                    vocab_size=vocab_size,\n",
    "                    embedding_dim=100,\n",
    "                    hidden_dim=params['hidden_dim'],\n",
    "                    n_layers=params['n_layers'],\n",
    "                    dropout=params['dropout_rate'],\n",
    "                    use_pretrained=params['use_pretrained']\n",
    "                )\n",
    "\n",
    "                # è®­ç»ƒæ¨¡å‹\n",
    "                history = analyzer.train_model(\n",
    "                    current_train_loader, current_val_loader,\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    num_epochs=15,\n",
    "                    early_stopping_patience=3\n",
    "                )\n",
    "\n",
    "                # è¯„ä¼°æ¨¡å‹\n",
    "                metrics = analyzer.evaluate_model(current_test_loader)\n",
    "\n",
    "                # è®°å½•å®éªŒç»“æœ\n",
    "                experiment_result = {\n",
    "                    'experiment_id': i + 1,\n",
    "                    'parameters': params,\n",
    "                    'metrics': metrics,\n",
    "                    'history': history,\n",
    "                    'training_time': analyzer.training_time,\n",
    "                    'analyzer': analyzer\n",
    "                }\n",
    "\n",
    "                self.experiment_results.append(experiment_result)\n",
    "\n",
    "                # æ›´æ–°æœ€ä½³å®éªŒ\n",
    "                if self.best_experiment is None or metrics['f1'] > self.best_experiment['metrics']['f1']:\n",
    "                    self.best_experiment = experiment_result\n",
    "                    print(f\"ğŸ¯ æ–°çš„æœ€ä½³å®éªŒ! F1åˆ†æ•°: {metrics['f1']:.4f}\")\n",
    "\n",
    "                experiment_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ å®éªŒ {i+1} å¤±è´¥: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"\\nâœ… è¶…å‚æ•°æœç´¢å®Œæˆ! å…±å®Œæˆ {len(self.experiment_results)} ä¸ªå®éªŒ\")\n",
    "        return self.experiment_results\n",
    "\n",
    "    def _generate_param_combinations(self, param_grid, max_experiments):\n",
    "        \"\"\"ç”Ÿæˆå‚æ•°ç»„åˆ\"\"\"\n",
    "        keys = param_grid.keys()\n",
    "        values = param_grid.values()\n",
    "\n",
    "        combinations = []\n",
    "        for combination in product(*values):\n",
    "            param_dict = dict(zip(keys, combination))\n",
    "            combinations.append(param_dict)\n",
    "\n",
    "        # å¦‚æœç»„åˆå¤ªå¤šï¼Œéšæœºé€‰æ‹©ä¸€éƒ¨åˆ†\n",
    "        if len(combinations) > max_experiments:\n",
    "            indices = np.random.choice(len(combinations), max_experiments, replace=False)\n",
    "            combinations = [combinations[i] for i in indices]\n",
    "\n",
    "        return combinations\n",
    "\n",
    "    def generate_comparison_report(self):\n",
    "        \"\"\"ç”Ÿæˆå®éªŒå¯¹æ¯”æŠ¥å‘Š\"\"\"\n",
    "        if not self.experiment_results:\n",
    "            print(\"æ²¡æœ‰å®éªŒç»“æœå¯æ¯”è¾ƒ\")\n",
    "            return\n",
    "\n",
    "        # åˆ›å»ºå¯¹æ¯”DataFrame\n",
    "        comparison_data = []\n",
    "        for result in self.experiment_results:\n",
    "            row = result['parameters'].copy()\n",
    "            row.update(result['metrics'])\n",
    "            row['training_time'] = result['training_time']\n",
    "            row['experiment_id'] = result['experiment_id']\n",
    "            comparison_data.append(row)\n",
    "\n",
    "        df = pd.DataFrame(comparison_data)\n",
    "\n",
    "        # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š\n",
    "        self._plot_experiment_comparison(df)\n",
    "\n",
    "        # ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "        df.to_csv('experiment_comparison.csv', index=False, encoding='utf-8')\n",
    "\n",
    "        print(f\"\\nğŸ“Š å®éªŒå¯¹æ¯”æŠ¥å‘Šå·²ç”Ÿæˆ\")\n",
    "        print(f\"æœ€ä½³å®éªŒID: {self.best_experiment['experiment_id']}\")\n",
    "        print(f\"æœ€ä½³F1åˆ†æ•°: {self.best_experiment['metrics']['f1']:.4f}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _plot_experiment_comparison(self, df):\n",
    "        \"\"\"ç»˜åˆ¶å®éªŒå¯¹æ¯”å›¾\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "        # F1åˆ†æ•°å¯¹æ¯”\n",
    "        axes[0,0].bar(df['experiment_id'], df['f1'], alpha=0.7)\n",
    "        axes[0,0].set_xlabel('å®éªŒID')\n",
    "        axes[0,0].set_ylabel('F1åˆ†æ•°')\n",
    "        axes[0,0].set_title('å„å®éªŒF1åˆ†æ•°å¯¹æ¯”')\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "        # å‡†ç¡®ç‡å¯¹æ¯”\n",
    "        axes[0,1].bar(df['experiment_id'], df['accuracy'], alpha=0.7, color='orange')\n",
    "        axes[0,1].set_xlabel('å®éªŒID')\n",
    "        axes[0,1].set_ylabel('å‡†ç¡®ç‡')\n",
    "        axes[0,1].set_title('å„å®éªŒå‡†ç¡®ç‡å¯¹æ¯”')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "        # è®­ç»ƒæ—¶é—´å¯¹æ¯”\n",
    "        axes[1,0].bar(df['experiment_id'], df['training_time'], alpha=0.7, color='green')\n",
    "        axes[1,0].set_xlabel('å®éªŒID')\n",
    "        axes[1,0].set_ylabel('è®­ç»ƒæ—¶é—´(ç§’)')\n",
    "        axes[1,0].set_title('è®­ç»ƒæ—¶é—´å¯¹æ¯”')\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "        # åµŒå…¥ç±»å‹å¯¹æ¯”\n",
    "        if 'use_pretrained' in df.columns:\n",
    "            pretrained_mean = df[df['use_pretrained'] == True]['f1'].mean()\n",
    "            random_mean = df[df['use_pretrained'] == False]['f1'].mean()\n",
    "            axes[1,1].bar(['é¢„è®­ç»ƒ', 'éšæœº'], [pretrained_mean, random_mean], alpha=0.7, color='red')\n",
    "            axes[1,1].set_ylabel('å¹³å‡F1åˆ†æ•°')\n",
    "            axes[1,1].set_title('åµŒå…¥ç±»å‹æ€§èƒ½å¯¹æ¯”')\n",
    "            axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('experiment_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n"
   ],
   "id": "d210611a62b28571",
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "id": "66345808-2621-4618-9a53-6b05c67aad97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T06:15:00.398949Z",
     "start_time": "2025-11-21T06:15:00.395528Z"
    }
   },
   "source": [
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"ç»˜åˆ¶è®­ç»ƒå†å²\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # ç»˜åˆ¶æŸå¤±æ›²çº¿\n",
    "    ax1.plot(history['train_losses'], label='è®­ç»ƒæŸå¤±')\n",
    "    ax1.plot(history['val_losses'], label='éªŒè¯æŸå¤±')\n",
    "    ax1.set_title(f'{model_name} - æŸå¤±æ›²çº¿')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿\n",
    "    ax2.plot(history['train_accuracies'], label='è®­ç»ƒå‡†ç¡®ç‡')\n",
    "    ax2.plot(history['val_accuracies'], label='éªŒè¯å‡†ç¡®ç‡')\n",
    "    ax2.set_title(f'{model_name} - å‡†ç¡®ç‡æ›²çº¿')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name}_training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "ca2a40fe-a42d-4bed-b3fb-a1e8772fb9d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T06:15:00.410100Z",
     "start_time": "2025-11-21T06:15:00.406425Z"
    }
   },
   "source": [
    "def simple_train_and_evaluate(analyzer, train_loader, val_loader, test_loader, vocab_size, use_pretrained=False):\n",
    "    \"\"\"ç®€åŒ–çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"è®­ç»ƒ{'é¢„è®­ç»ƒåµŒå…¥' if use_pretrained else 'éšæœºåµŒå…¥'}æ¨¡å‹\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡å‹\n",
    "    analyzer.create_model(\n",
    "        vocab_size=vocab_size,\n",
    "        hidden_dim=128,\n",
    "        dropout=0.3,\n",
    "        use_pretrained=use_pretrained\n",
    "    )\n",
    "    \n",
    "    # è®­ç»ƒæ¨¡å‹\n",
    "    history = analyzer.train_model(\n",
    "        train_loader, val_loader,\n",
    "        learning_rate=0.001,\n",
    "        num_epochs=5  # å‡å°‘epochæ•°ä»¥åŠ å¿«å®éªŒ\n",
    "    )\n",
    "    \n",
    "    # è¯„ä¼°æ¨¡å‹\n",
    "    results = analyzer.evaluate_model(test_loader)\n",
    "    plot_training_history(history, f\"{'é¢„è®­ç»ƒåµŒå…¥' if use_pretrained else 'éšæœºåµŒå…¥'}æ¨¡å‹\")\n",
    "    \n",
    "    return results\n"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "id": "6bd70985-0d52-4dbb-a68d-e35fb42fc86d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T06:15:00.422886Z",
     "start_time": "2025-11-21T06:15:00.418998Z"
    }
   },
   "source": [
    "# def main():\n",
    "#     \"\"\"ä¸»å‡½æ•°\"\"\"\n",
    "#     print(\"å¼€å§‹æƒ…æ„Ÿåˆ†æé¡¹ç›®...\")\n",
    "#\n",
    "#     # åŠ è½½æ•°æ®\n",
    "#     print(\"åŠ è½½æ•°æ®...\")\n",
    "#     try:\n",
    "#         data = torch.load(r\"all_data.pt\")\n",
    "#         print(\"æ•°æ®åŠ è½½æˆåŠŸ!\")\n",
    "#\n",
    "#         # æå–è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®\n",
    "#         train_sequences = data['train_sequences']\n",
    "#         train_masks = data['train_masks']\n",
    "#         train_labels = data['train_labels']\n",
    "#         train_lengths = data['train_lengths']\n",
    "#\n",
    "#         val_sequences = data['val_sequences']\n",
    "#         val_masks = data['val_masks']\n",
    "#         val_labels = data['val_labels']\n",
    "#         val_lengths = data['val_lengths']\n",
    "#\n",
    "#         test_sequences = data['test_sequences']\n",
    "#         test_masks = data['test_masks']\n",
    "#         test_labels = data['test_labels']\n",
    "#         test_lengths = data['test_lengths']\n",
    "#\n",
    "#         print(f\"è®­ç»ƒé›†: {len(train_sequences)} ä¸ªæ ·æœ¬\")\n",
    "#         print(f\"éªŒè¯é›†: {len(val_sequences)} ä¸ªæ ·æœ¬\")\n",
    "#         print(f\"æµ‹è¯•é›†: {len(test_sequences)} ä¸ªæ ·æœ¬\")\n",
    "#\n",
    "#         # è®¡ç®—è¯æ±‡è¡¨å¤§å°ï¼ˆä»åºåˆ—ä¸­çš„æœ€å¤§ç´¢å¼•+1ï¼‰\n",
    "#         vocab_size = max(\n",
    "#             train_sequences.max().item(),\n",
    "#             val_sequences.max().item(),\n",
    "#             test_sequences.max().item()\n",
    "#         ) + 1\n",
    "#         print(f\"è¯æ±‡è¡¨å¤§å°: {vocab_size}\")\n",
    "#\n",
    "#         # åˆ›å»ºæ•°æ®é›†\n",
    "#         train_dataset = TensorDataset(train_sequences, train_masks, train_labels, train_lengths)\n",
    "#         val_dataset = TensorDataset(val_sequences, val_masks, val_labels, val_lengths)\n",
    "#         test_dataset = TensorDataset(test_sequences, test_masks, test_labels, test_lengths)\n",
    "#\n",
    "#         # åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "#         batch_size = 32\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#         val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "#         test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "#\n",
    "#         # ===== åœ¨è¿™é‡Œæ·»åŠ è¯Šæ–­ä»£ç  =====\n",
    "#         print(\"\\n=== æ•°æ®è¯Šæ–­ä¿¡æ¯ ===\")\n",
    "#         print(f\"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}\")\n",
    "#         print(f\"éªŒè¯é›†å¤§å°: {len(val_dataset)}\")\n",
    "#         print(f\"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}\")\n",
    "#\n",
    "#         # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ\n",
    "#         print(f\"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {Counter(train_labels.numpy())}\")\n",
    "#         print(f\"éªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ: {Counter(val_labels.numpy())}\")\n",
    "#         print(f\"æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ: {Counter(test_labels.numpy())}\")\n",
    "#\n",
    "#         # æ£€æŸ¥åºåˆ—æ•°æ®\n",
    "#         print(f\"åºåˆ—å½¢çŠ¶: {train_sequences.shape}\")\n",
    "#         print(f\"åºåˆ—æ•°æ®ç±»å‹: {train_sequences.dtype}\")\n",
    "#         print(f\"åºåˆ—å€¼èŒƒå›´: [{train_sequences.min()}, {train_sequences.max()}]\")\n",
    "#         print(f\"æ ·æœ¬æ ‡ç­¾ç¤ºä¾‹: {train_labels[:10].numpy()}\")\n",
    "#\n",
    "#         # æ£€æŸ¥è®¾å¤‡\n",
    "#         print(f\"è®­ç»ƒè®¾å¤‡: {device}\")\n",
    "#         print(\"====================\\n\")\n",
    "#\n",
    "#     except Exception as e:\n",
    "#         print(f\"æ•°æ®åŠ è½½å¤±è´¥: {e}\")\n",
    "#         return\n",
    "#\n",
    "#     # å…¶ä½™çš„ä»£ç ä¿æŒä¸å˜...\n",
    "#\n",
    "#     # åˆå§‹åŒ–åˆ†æå™¨\n",
    "#     analyzer = SentimentAnalyzer(device=device)\n",
    "#\n",
    "#     # ç®€åŒ–çš„è®­ç»ƒæµç¨‹ - åªè®­ç»ƒéšæœºåµŒå…¥æ¨¡å‹\n",
    "#     print(\"\\nå¼€å§‹è®­ç»ƒéšæœºåµŒå…¥æ¨¡å‹...\")\n",
    "#     random_results = simple_train_and_evaluate(analyzer, train_loader, val_loader, test_loader, vocab_size, use_pretrained=False)\n",
    "#\n",
    "#     # å°è¯•é¢„è®­ç»ƒåµŒå…¥æ¨¡å‹ï¼ˆå¦‚æœGloVeæ–‡ä»¶å¯ç”¨ï¼‰\n",
    "#     glove_path = \"glove.6B.100d.txt\"  # è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹è·¯å¾„\n",
    "#     try:\n",
    "#         print(\"\\nå¼€å§‹è®­ç»ƒé¢„è®­ç»ƒåµŒå…¥æ¨¡å‹...\")\n",
    "#         pretrained_results = simple_train_and_evaluate(analyzer, train_loader, val_loader, test_loader, vocab_size, use_pretrained=True)\n",
    "#\n",
    "#         # æ¯”è¾ƒç»“æœ\n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"æ¨¡å‹æ¯”è¾ƒç»“æœ\")\n",
    "#         print(\"=\"*80)\n",
    "#         print(f\"éšæœºåµŒå…¥æ¨¡å‹ - å‡†ç¡®ç‡: {random_results['accuracy']:.4f}, F1: {random_results['f1']:.4f}\")\n",
    "#         print(f\"é¢„è®­ç»ƒåµŒå…¥æ¨¡å‹ - å‡†ç¡®ç‡: {pretrained_results['accuracy']:.4f}, F1: {pretrained_results['f1']:.4f}\")\n",
    "#\n",
    "#         if pretrained_results['accuracy'] > random_results['accuracy']:\n",
    "#             print(\"é¢„è®­ç»ƒåµŒå…¥æ¨¡å‹è¡¨ç°æ›´å¥½!\")\n",
    "#         else:\n",
    "#             print(\"éšæœºåµŒå…¥æ¨¡å‹è¡¨ç°æ›´å¥½!\")\n",
    "#\n",
    "#     except Exception as e:\n",
    "#         print(f\"é¢„è®­ç»ƒåµŒå…¥æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}\")\n",
    "#         print(\"è¯·ç¡®ä¿GloVeè¯å‘é‡æ–‡ä»¶å¯ç”¨ï¼Œæˆ–ä½¿ç”¨éšæœºåµŒå…¥æ¨¡å‹\")\n",
    "#\n",
    "#     print(\"\\né¡¹ç›®å®Œæˆ!\")\n",
    "#\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "e86890ac-e580-485f-8e98-5c95c2ca0a3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T06:16:01.113247Z",
     "start_time": "2025-11-21T06:15:00.431042Z"
    }
   },
   "source": [
    "def main():\n",
    "    \"\"\"å®Œæ•´çš„æ¼”ç¤ºæµç¨‹\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸ¤– æƒ…æ„Ÿåˆ†æç³»ç»Ÿ - å®Œæ•´æ¼”ç¤º\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    try:\n",
    "        # åŠ è½½æ•°æ®\n",
    "        print(\"ğŸ“‚ åŠ è½½æ•°æ®...\")\n",
    "        data = torch.load(\"all_data.pt\")\n",
    "\n",
    "        # åˆ›å»ºæ•°æ®é›†\n",
    "        train_dataset = TensorDataset(\n",
    "            data['train_sequences'], data['train_masks'],\n",
    "            data['train_labels'], data['train_lengths']\n",
    "        )\n",
    "        val_dataset = TensorDataset(\n",
    "            data['val_sequences'], data['val_masks'],\n",
    "            data['val_labels'], data['val_lengths']\n",
    "        )\n",
    "        test_dataset = TensorDataset(\n",
    "            data['test_sequences'], data['test_masks'],\n",
    "            data['test_labels'], data['test_lengths']\n",
    "        )\n",
    "\n",
    "        vocab_size = max(\n",
    "            data['train_sequences'].max().item(),\n",
    "            data['val_sequences'].max().item(),\n",
    "            data['test_sequences'].max().item()\n",
    "        ) + 1\n",
    "\n",
    "        print(f\"è¯æ±‡è¡¨å¤§å°: {vocab_size}\")\n",
    "        print(f\"è®­ç»ƒé›†: {len(train_dataset)}ä¸ªæ ·æœ¬\")\n",
    "        print(f\"éªŒè¯é›†: {len(val_dataset)}ä¸ªæ ·æœ¬\")\n",
    "        print(f\"æµ‹è¯•é›†: {len(test_dataset)}ä¸ªæ ·æœ¬\")\n",
    "\n",
    "        # åˆ›å»ºåŸºç¡€æ•°æ®åŠ è½½å™¨\n",
    "        batch_size = 32\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # é€‰é¡¹1: å¿«é€Ÿå•ä¸ªæ¨¡å‹è®­ç»ƒ\n",
    "        print(\"\\n1. ğŸš€ å¿«é€Ÿè®­ç»ƒå•ä¸ªæ¨¡å‹...\")\n",
    "        analyzer = SentimentAnalyzer(device=device)\n",
    "        analyzer.create_model(vocab_size=vocab_size, use_pretrained=True)\n",
    "\n",
    "        history = analyzer.train_model(\n",
    "            train_loader, val_loader,\n",
    "            learning_rate=0.001, num_epochs=10\n",
    "        )\n",
    "\n",
    "        metrics = analyzer.evaluate_model(test_loader)\n",
    "        analyzer.generate_report()\n",
    "\n",
    "        # é€‰é¡¹2: è¶…å‚æ•°æœç´¢ï¼ˆå¯é€‰ï¼‰\n",
    "        print(\"\\n2. ğŸ” æ˜¯å¦è¿›è¡Œè¶…å‚æ•°æœç´¢? (y/n)\")\n",
    "        user_input = input().strip().lower()\n",
    "\n",
    "        if user_input == 'y':\n",
    "            print(\"å¼€å§‹è¶…å‚æ•°æœç´¢...\")\n",
    "            experiment_manager = ExperimentManager(device=device)\n",
    "\n",
    "            # å®šä¹‰è¾ƒå°çš„å‚æ•°ç½‘æ ¼ç”¨äºå¿«é€Ÿæ¼”ç¤º\n",
    "            quick_param_grid = {\n",
    "                'learning_rate': [0.0005, 0.001],\n",
    "                'batch_size': [32],\n",
    "                'dropout_rate': [0.3, 0.5],\n",
    "                'hidden_dim': [64, 128],\n",
    "                'n_layers': [1, 2],\n",
    "                'use_pretrained': [True,False]\n",
    "            }\n",
    "\n",
    "            results = experiment_manager.run_hyperparameter_search(\n",
    "                train_loader, val_loader, test_loader, vocab_size,\n",
    "                param_grid=quick_param_grid, max_experiments=8\n",
    "            )\n",
    "\n",
    "            experiment_manager.generate_comparison_report()\n",
    "\n",
    "        print(\"\\nâœ… ç¨‹åºæ‰§è¡Œå®Œæˆ!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ é”™è¯¯: {e}\")\n",
    "        print(\"è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¤– æƒ…æ„Ÿåˆ†æç³»ç»Ÿ - å®Œæ•´æ¼”ç¤º\n",
      "================================================================================\n",
      "ğŸ“‚ åŠ è½½æ•°æ®...\n",
      "è¯æ±‡è¡¨å¤§å°: 30000\n",
      "è®­ç»ƒé›†: 32000ä¸ªæ ·æœ¬\n",
      "éªŒè¯é›†: 8000ä¸ªæ ·æœ¬\n",
      "æµ‹è¯•é›†: 10000ä¸ªæ ·æœ¬\n",
      "\n",
      "1. ğŸš€ å¿«é€Ÿè®­ç»ƒå•ä¸ªæ¨¡å‹...\n",
      "åŠ è½½GloVeè¯å‘é‡...\n",
      "æˆåŠŸåŠ è½½ 400000 ä¸ªGloVeè¯å‘é‡\n",
      "æ³¨æ„: ç”±äºç¼ºä¹ç´¢å¼•åˆ°è¯çš„ç²¾ç¡®æ˜ å°„ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–æ›¿ä»£å®Œæ•´GloVeåŠ è½½\n",
      "ä½¿ç”¨é¢„è®­ç»ƒGloVeè¯å‘é‡çš„æ¨¡å‹å·²åˆ›å»º\n",
      "å¼€å§‹è®­ç»ƒï¼Œå­¦ä¹ ç‡: 0.001, æ—©åœè€å¿ƒå€¼: 3\n",
      "ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: 0.6930\n",
      "Epoch [1/10], Time: 12.73s\n",
      "  è®­ç»ƒæŸå¤±: 0.6934, è®­ç»ƒå‡†ç¡®ç‡: 0.5024\n",
      "  éªŒè¯æŸå¤±: 0.6930, éªŒè¯å‡†ç¡®ç‡: 0.5085\n",
      "  æœ€ä½³epoch: 1, æ—©åœè®¡æ•°: 0/3\n",
      "Epoch [2/10], Time: 12.33s\n",
      "  è®­ç»ƒæŸå¤±: 0.6916, è®­ç»ƒå‡†ç¡®ç‡: 0.5067\n",
      "  éªŒè¯æŸå¤±: 0.6935, éªŒè¯å‡†ç¡®ç‡: 0.5016\n",
      "  æœ€ä½³epoch: 1, æ—©åœè®¡æ•°: 1/3\n",
      "Epoch [3/10], Time: 12.36s\n",
      "  è®­ç»ƒæŸå¤±: 0.6810, è®­ç»ƒå‡†ç¡®ç‡: 0.5198\n",
      "  éªŒè¯æŸå¤±: 0.6991, éªŒè¯å‡†ç¡®ç‡: 0.5109\n",
      "  æœ€ä½³epoch: 1, æ—©åœè®¡æ•°: 2/3\n",
      "Epoch [4/10], Time: 12.28s\n",
      "  è®­ç»ƒæŸå¤±: 0.6612, è®­ç»ƒå‡†ç¡®ç‡: 0.5338\n",
      "  éªŒè¯æŸå¤±: 0.7191, éªŒè¯å‡†ç¡®ç‡: 0.5111\n",
      "  æœ€ä½³epoch: 1, æ—©åœè®¡æ•°: 3/3\n",
      "æ—©åœè§¦å‘äºç¬¬ 4 ä¸ªepoch\n",
      "è®­ç»ƒå®Œæˆï¼Œæ€»æ—¶é—´: 49.71ç§’\n",
      "\n",
      "============================================================\n",
      "æ¨¡å‹è¯„ä¼°ç»“æœ\n",
      "============================================================\n",
      "å‡†ç¡®ç‡ (Accuracy): 0.5033\n",
      "ç²¾ç¡®ç‡ (Precision): 0.5295\n",
      "å¬å›ç‡ (Recall): 0.0592\n",
      "F1åˆ†æ•°: 0.1065\n",
      "\n",
      "è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          è´Ÿé¢     0.5017    0.9474    0.6560      5000\n",
      "          æ­£é¢     0.5295    0.0592    0.1065      5000\n",
      "\n",
      "    accuracy                         0.5033     10000\n",
      "   macro avg     0.5156    0.5033    0.3813     10000\n",
      "weighted avg     0.5156    0.5033    0.3813     10000\n",
      "\n",
      "ğŸ“„ åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: reports\\analysis_report.html\n",
      "\n",
      "2. ğŸ” æ˜¯å¦è¿›è¡Œè¶…å‚æ•°æœç´¢? (y/n)\n",
      "\n",
      "âœ… ç¨‹åºæ‰§è¡Œå®Œæˆ!\n"
     ]
    }
   ],
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
