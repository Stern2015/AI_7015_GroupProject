{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab47aa03",
   "metadata": {},
   "source": [
    "# Data Preprocess\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a900734c-4d56-4923-ad51-4349706592f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import html\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478321d-d359-403a-a86a-ce8071f5179f",
   "metadata": {},
   "source": [
    "## DataDownloader - Data Download and Validation Module\n",
    "\n",
    "**Function Methods:**\n",
    "- download_imdb() - Download IMDb dataset archive and extract\n",
    "- validate_data_directory() - Validate data directory structure integrity\n",
    "- setup_data() - Main entry point to ensure data availability (download or use existing data)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c4ee9f-a990-43bf-bc51-8988b31ce3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDownloader:\n",
    "    \n",
    "    @staticmethod\n",
    "    def download_imdb() -> Path:\n",
    "        data_dir = Path('./aclImdb')\n",
    "        \n",
    "        if data_dir.exists():\n",
    "            print(\"IMDb dataset already exists, skipping download\")\n",
    "            return data_dir\n",
    "        \n",
    "        print(\"Downloading IMDb dataset...\")\n",
    "        url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "        tar_path = Path('./aclImdb_v1.tar.gz')\n",
    "        \n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, tar_path)\n",
    "            print(\"Download completed, extracting...\")\n",
    "            \n",
    "            with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "                tar.extractall(path='./')\n",
    "            \n",
    "            tar_path.unlink()\n",
    "            print(\"IMDb dataset preparation completed\")\n",
    "            return data_dir\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Dataset download failed: {e}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_data_directory(data_dir: Path) -> bool:\n",
    "        required_dirs = [\n",
    "            data_dir / 'train' / 'pos',\n",
    "            data_dir / 'train' / 'neg', \n",
    "            data_dir / 'test' / 'pos',\n",
    "            data_dir / 'test' / 'neg'\n",
    "        ]\n",
    "        \n",
    "        for dir_path in required_dirs:\n",
    "            if not dir_path.exists():\n",
    "                return False\n",
    "            if len(list(dir_path.glob('*.txt'))) == 0:\n",
    "                return False\n",
    "                \n",
    "        return True\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_data() -> Path:\n",
    "        data_dir = Path('./aclImdb')\n",
    "        \n",
    "        if DataDownloader.validate_data_directory(data_dir):\n",
    "            print(\"‚úÖ Data directory validation passed\")\n",
    "            return data_dir\n",
    "        \n",
    "        print(\"Data directory incomplete, re-downloading...\")\n",
    "        data_dir = DataDownloader.download_imdb()\n",
    "        \n",
    "        if not DataDownloader.validate_data_directory(data_dir):\n",
    "            raise Exception(\"Data directory validation failed, please check dataset manually\")\n",
    "            \n",
    "        return data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd27fd7-702e-4e70-badf-3302b20904d7",
   "metadata": {},
   "source": [
    "## TextProcessor - Text Processing Module\n",
    "\n",
    "**Function Methods:**\n",
    "\n",
    "- robust_text_cleaning(text) - Core text cleaning, includes:\n",
    "\n",
    "    1. HTML entity decoding and tag removal\n",
    "    2. URL processing\n",
    "    3. Contraction expansion (e.g., \"can't\" ‚Üí \"can not\")\n",
    "    4. Punctuation standardization\n",
    "    5. Case normalization\n",
    "\n",
    "- process_in_batches() - Batch process text to avoid memory overflow\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c6ef375-2e6b-4057-bf48-7c0b8822574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    \n",
    "    @staticmethod\n",
    "    def robust_text_cleaning(text: str) -> str:\n",
    "        \n",
    "        # Decode HTML entities\n",
    "        text = html.unescape(text)\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<[^>]+>', ' ', text)\n",
    "        \n",
    "        # Process URLs\n",
    "        text = re.sub(r'http\\S+', ' <URL> ', text)\n",
    "        \n",
    "        # Process common contractions and negations\n",
    "        contractions = {\n",
    "            r\"won't\": \"will not\", r\"can't\": \"can not\", r\"n't\": \" not\",\n",
    "            r\"'re\": \" are\", r\"'s\": \" is\", r\"'d\": \" would\", \n",
    "            r\"'ll\": \" will\", r\"'t\": \" not\", r\"'ve\": \" have\",\n",
    "            r\"'m\": \" am\"\n",
    "        }\n",
    "        \n",
    "        for pattern, replacement in contractions.items():\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        \n",
    "        # Preserve basic punctuation for sentiment analysis\n",
    "        text = re.sub(r'[^a-zA-Z\\s\\.!?,;:\\']', ' ', text)\n",
    "        \n",
    "        # Handle repeated punctuation\n",
    "        text = re.sub(r'([!?.]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'[!?.]+', r' \\0 ', text)\n",
    "        \n",
    "        # Standardize whitespace handling\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text.strip().lower()\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_in_batches(texts: List[str], process_func, batch_size: int = 1000, process_name: str = \"Processing\") -> List[str]:\n",
    "        processed = []\n",
    "        total = len(texts)\n",
    "        \n",
    "        for i in range(0, total, batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            processed_batch = [process_func(text) for text in batch]\n",
    "            processed.extend(processed_batch)\n",
    "            \n",
    "            if i % 5000 == 0 and i > 0:\n",
    "                print(f\"  {process_name} {i}/{total} samples\")\n",
    "        \n",
    "        print(f\"‚úÖ {process_name} completed\")\n",
    "        return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7939994-4d01-4ade-aa3e-7f25e28541d9",
   "metadata": {},
   "source": [
    "## LabelProcessor - Label Processing Module\n",
    "\n",
    "**Function Methods:**\n",
    "- parse_rating_from_filename() - Parse star rating from filename\n",
    "- convert_to_binary() - Convert star rating to binary classification\n",
    "- get_label_schema_config() - Get label schema configurationÁΩÆ\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2bb8b76-9d50-4469-8f46-cada52225ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelProcessor:\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_rating_from_filename(file_path: Path) -> Optional[int]:\n",
    "        stem = file_path.stem\n",
    "        parts = stem.split('_')\n",
    "        if len(parts) < 2:\n",
    "            return None\n",
    "        \n",
    "        rating_str = parts[-1]\n",
    "        try:\n",
    "            return int(rating_str)  # 1..10\n",
    "        except ValueError:\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_binary(rating: int) -> int:\n",
    "        if rating <= 4:\n",
    "            return 0  # Negative\n",
    "        else:  # rating >= 7\n",
    "            return 1  # Positive\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_label_schema_config(schema_type: str) -> Dict:\n",
    "        schemas = {\n",
    "            \"binary\": {\n",
    "                \"name\": \"Binary Classification\",\n",
    "                \"num_classes\": 2,\n",
    "                \"output_dir\": \"processed_data_binary\",\n",
    "                \"label_range\": \"0-1 (Negative/Positive)\"\n",
    "            },\n",
    "            \"multiclass\": {\n",
    "                \"name\": \"Multi-class (1-10 stars)\",\n",
    "                \"num_classes\": 8,  # Actually only 8 classes (1-4, 7-10)\n",
    "                \"output_dir\": \"processed_data_multiclass\", \n",
    "                \"label_range\": \"1-10 stars (Missing 5-6 stars)\"\n",
    "            }\n",
    "        }\n",
    "        return schemas.get(schema_type, schemas[\"binary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7baa71-8a82-4148-afe4-56c5dc27675b",
   "metadata": {},
   "source": [
    "## DataLoaderManager - Data Loading Manager\n",
    "\n",
    "**Function Methods:**\n",
    "- load_raw_data() - Load raw IMDb data according to label schema\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f865927f-8828-495c-b1c0-a031bc9c8bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderManager:\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_raw_data(data_dir: Path, label_schema: str) -> Tuple[List[str], List[int], List[str], List[int]]:\n",
    "        print(f\"Loading IMDb dataset - {LabelProcessor.get_label_schema_config(label_schema)['name']}...\")\n",
    "        \n",
    "        def load_from_directory(directory: Path, schema: str) -> Tuple[List[str], List[int]]:\n",
    "            texts, labels = [], []\n",
    "            \n",
    "            for label_type in ['pos', 'neg']:\n",
    "                dir_name = directory / label_type\n",
    "                if not dir_name.exists():\n",
    "                    print(f\"Warning: Missing directory: {dir_name}\")\n",
    "                    continue\n",
    "                    \n",
    "                for file_path in dir_name.glob('*.txt'):\n",
    "                    # Parse star rating\n",
    "                    rating = LabelProcessor.parse_rating_from_filename(file_path)\n",
    "                    if rating is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Select label based on schema\n",
    "                    if schema == \"binary\":\n",
    "                        label = LabelProcessor.convert_to_binary(rating)\n",
    "                    else:  # multiclass\n",
    "                        label = rating\n",
    "                    \n",
    "                    # Read text\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            text = f.read().strip()\n",
    "                        texts.append(text)\n",
    "                        labels.append(label)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipping unreadable file: {file_path.name} ({e})\")\n",
    "                        continue\n",
    "            \n",
    "            # Distribution check\n",
    "            dist = Counter(labels)\n",
    "            dist_sorted = dict(sorted(dist.items()))\n",
    "            print(f\"{directory.name} label distribution: {dist_sorted}\")\n",
    "            return texts, labels\n",
    "        \n",
    "        train_dir = data_dir / 'train'\n",
    "        test_dir = data_dir / 'test'\n",
    "        \n",
    "        train_texts, train_labels = load_from_directory(train_dir, label_schema)\n",
    "        test_texts, test_labels = load_from_directory(test_dir, label_schema)\n",
    "        \n",
    "        print(f\"Loaded {len(train_texts)} training samples, {len(test_texts)} test samples\")\n",
    "        return train_texts, train_labels, test_texts, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f3c8d1-a165-414b-8c32-119630d5db98",
   "metadata": {},
   "source": [
    "## DataSplitter - Data Splitter\n",
    "\n",
    "**Function Methods:**\n",
    "- split_data_with_seeds() - Use random seeds for reproducible data splitting\n",
    "\n",
    "    1. Automatic stratified sampling protection\n",
    "    2. Handle small sample classes\n",
    "    3. Train/validation/test set split (60%/20%/20%)\n",
    "    4. Distribution statistics output\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "031611e6-0fb9-40c8-b38f-55a827775909",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplitter:\n",
    "\n",
    "    @staticmethod\n",
    "    def split_data_with_seeds(texts: List[str], labels: List[int], seed: int = 42,\n",
    "                             test_size: float = 0.2, val_size: float = 0.2) -> Tuple:\n",
    "        \n",
    "        def _print_dist(name: str, y: List[int]):\n",
    "            c = Counter(y)\n",
    "            print(f\"{name} class distribution: {dict(sorted(c.items()))}\")\n",
    "        \n",
    "        print(\"Performing reproducible data splitting (with class checking)...\")\n",
    "        counts = Counter(labels)\n",
    "        min_count = min(counts.values()) if counts else 0\n",
    "        use_stratify = labels if min_count >= 2 else None\n",
    "        \n",
    "        if use_stratify is None:\n",
    "            print(f\"Warning: Minimum class sample count {min_count} < 2, initial split not using stratified sampling\")\n",
    "        \n",
    "        # First split test set\n",
    "        train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "            texts, labels, test_size=test_size, random_state=seed,\n",
    "            stratify=use_stratify\n",
    "        )\n",
    "        \n",
    "        # Then split validation set from training set\n",
    "        train_counts = Counter(train_labels)\n",
    "        min_train_count = min(train_counts.values()) if train_counts else 0\n",
    "        use_stratify_val = train_labels if min_train_count >= 2 else None\n",
    "        \n",
    "        if use_stratify_val is None:\n",
    "            print(f\"Warning: Minimum class sample count in training set {min_train_count} < 2, validation split not using stratified sampling\")\n",
    "        \n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            train_texts, train_labels, test_size=val_size, random_state=seed,\n",
    "            stratify=use_stratify_val\n",
    "        )\n",
    "        \n",
    "        print(f\"Data split: Training set={len(train_texts)}, Validation set={len(val_texts)}, Test set={len(test_texts)}\")\n",
    "        _print_dist(\"Training set\", train_labels)\n",
    "        _print_dist(\"Validation set\", val_labels)\n",
    "        _print_dist(\"Test set\", test_labels)\n",
    "        \n",
    "        return train_texts, train_labels, val_texts, val_labels, test_texts, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f1ae6c-ce66-4cdc-b2f0-aa8dc3dcd24e",
   "metadata": {},
   "source": [
    "## VocabularyBuilder - Vocabulary Management Module\n",
    "\n",
    "**Function Methods:**\n",
    "- build_vocabulary(texts) - Build vocabulary based on training texts:\n",
    "\n",
    "    1. Word frequency statistics\n",
    "    2. Filter low-frequency words\n",
    "    3. Add special tokens (PAD/UNK/BOS/EOS)\n",
    "    4. Calculate vocabulary coverage\n",
    "\n",
    "- text_to_sequence(text) - Convert text to numerical sequence:\n",
    "\n",
    "    1. Add start/end tokens\n",
    "    2. Handle unknown words (OOV)\n",
    "    3. Sequence padding/truncation\n",
    "    4. Generate attention masks\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "394556fd-0eb8-489a-9742-543c5ac9391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyBuilder:\n",
    "    \n",
    "    def __init__(self, max_vocab_size: int = 30000, min_freq: int = 2):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_freq = min_freq\n",
    "        self.special_tokens = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    def build_vocabulary(self, texts: List[str]) -> None:\n",
    "        print(\"Building vocabulary...\")\n",
    "        \n",
    "        # Count word frequency\n",
    "        word_freq = Counter()\n",
    "        for text in texts:\n",
    "            tokens = text.split()\n",
    "            word_freq.update(tokens)\n",
    "        \n",
    "        # Filter low-frequency words\n",
    "        filtered_words = [(word, freq) for word, freq in word_freq.items() \n",
    "                         if freq >= self.min_freq]\n",
    "        \n",
    "        # Sort by frequency and select top N words\n",
    "        sorted_words = sorted(filtered_words, key=lambda x: x[1], reverse=True)\n",
    "        selected_words = [word for word, freq in sorted_words[:self.max_vocab_size - len(self.special_tokens)]]\n",
    "        \n",
    "        # Build vocabulary\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "        \n",
    "        # Add special tokens\n",
    "        for idx, token in enumerate(self.special_tokens):\n",
    "            self.token_to_id[token] = idx\n",
    "            self.id_to_token[idx] = token\n",
    "        \n",
    "        # Add regular vocabulary\n",
    "        for idx, word in enumerate(selected_words, start=len(self.special_tokens)):\n",
    "            self.token_to_id[word] = idx\n",
    "            self.id_to_token[idx] = word\n",
    "        \n",
    "        self.vocab_size = len(self.token_to_id)\n",
    "        \n",
    "        # OOV analysis\n",
    "        total_tokens = sum(word_freq.values())\n",
    "        covered_tokens = sum(freq for word, freq in word_freq.items() \n",
    "                           if word in self.token_to_id)\n",
    "        coverage = covered_tokens / total_tokens * 100\n",
    "        \n",
    "        print(f\"Vocabulary building completed: {self.vocab_size} tokens\")\n",
    "        print(f\"Vocabulary coverage: {coverage:.2f}%\")\n",
    "    \n",
    "    def text_to_sequence(self, text: str, max_length: int = 512, add_special_tokens: bool = True) -> Tuple[List[int], List[int], int]:\n",
    "\n",
    "        tokens = text.split()\n",
    "        sequence = []\n",
    "        \n",
    "        # Add beginning token\n",
    "        if add_special_tokens:\n",
    "            sequence.append(self.token_to_id['<BOS>'])\n",
    "        \n",
    "        # Convert tokens\n",
    "        for token in tokens:\n",
    "            sequence.append(self.token_to_id.get(token, self.token_to_id['<UNK>']))\n",
    "        \n",
    "        # Add end token\n",
    "        if add_special_tokens:\n",
    "            sequence.append(self.token_to_id['<EOS>'])\n",
    "        \n",
    "        original_length = len(sequence)\n",
    "        \n",
    "        # Padding or truncation\n",
    "        if len(sequence) < max_length:\n",
    "            sequence.extend([self.token_to_id['<PAD>']] * (max_length - len(sequence)))\n",
    "            attention_mask = [1] * original_length + [0] * (max_length - original_length)\n",
    "        else:\n",
    "            sequence = sequence[:max_length]\n",
    "            attention_mask = [1] * max_length\n",
    "        \n",
    "        return sequence, attention_mask, original_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf84183-a006-4f07-8181-60f5dc69d6a2",
   "metadata": {},
   "source": [
    "## IMDBDataset - Dataset Interface Module\n",
    "\n",
    "**Function Methods:**\n",
    "- __init__() - Initialize dataset, validate data consistency\n",
    "- __len__() - Return dataset size\n",
    "- __getitem__() - Get single sample, return dictionary format\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "499dafae-f48c-42b2-b3b3-3c2fc5f96aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, sequences: List[List[int]], attention_masks: List[List[int]], \n",
    "                 labels: List[int], lengths: List[int]):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "        self.attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "        \n",
    "        # Validate data shapes\n",
    "        assert len(self.sequences) == len(self.labels), \"Sequence and label count mismatch\"\n",
    "        assert len(self.sequences) == len(self.attention_masks), \"Sequence and attention mask count mismatch\"\n",
    "        assert len(self.sequences) == len(self.lengths), \"Sequence and length count mismatch\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.sequences[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx],\n",
    "            'lengths': self.lengths[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fdb0d3-a10b-4094-a182-162acc0aa8b5",
   "metadata": {},
   "source": [
    "## DataSaver - Data Saving Module\n",
    "\n",
    "**Function Methods:**\n",
    "- save_processed_data() - Main save function\n",
    "- _save_vocabulary() - Save vocabulary as JSON\n",
    "- _save_config() - Save preprocessing configuration\n",
    "- _save_tensor_data() - Save tensor data as .pt files\n",
    "- _save_metadata() - Save statistical metadata\n",
    "- _verify_files() - Verify generated files\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5da1e614-5d4d-4f45-8a7e-cce4aa480016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSaver:\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_processed_data(datasets: Dict, vocab, metadata: Dict, output_dir: str, label_schema: str):\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        schema_config = LabelProcessor.get_label_schema_config(label_schema)\n",
    "        print(f\"Starting to save {schema_config['name']} data to: {output_path}\")\n",
    "        \n",
    "        # 1. Save vocabulary\n",
    "        DataSaver._save_vocabulary(vocab, output_path)\n",
    "        \n",
    "        # 2. Save configuration\n",
    "        DataSaver._save_config(output_path, label_schema, schema_config)\n",
    "        \n",
    "        # 3. Save tensor data\n",
    "        DataSaver._save_tensor_data(datasets, output_path)\n",
    "        \n",
    "        # 4. Save metadata\n",
    "        DataSaver._save_metadata(metadata, output_path)\n",
    "        \n",
    "        # 5. Verify files\n",
    "        DataSaver._verify_files(output_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _save_vocabulary(vocab, output_path):\n",
    "        vocab_path = output_path / 'vocabulary.json'\n",
    "        with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(vocab, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"‚úÖ Vocabulary saved to: {vocab_path}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _save_config(output_path, label_schema, schema_config):\n",
    "        config_path = output_path / 'preprocessing_config.json'\n",
    "        config = {\n",
    "            'label_schema': label_schema,\n",
    "            'schema_name': schema_config['name'],\n",
    "            'num_classes': schema_config['num_classes'],\n",
    "            'label_range': schema_config['label_range'],\n",
    "            'max_vocab_size': 30000,\n",
    "            'max_length': 512,\n",
    "            'min_freq': 2,\n",
    "            'seed': 42,\n",
    "            'saved_time': str(pd.Timestamp.now())\n",
    "        }\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        print(f\"‚úÖ Configuration saved to: {config_path}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _save_tensor_data(datasets, output_path):\n",
    "        data_path = output_path / 'all_data.pt'\n",
    "        \n",
    "        save_data = {\n",
    "            'train_sequences': datasets['train'].sequences,\n",
    "            'train_masks': datasets['train'].attention_masks,\n",
    "            'train_labels': datasets['train'].labels,\n",
    "            'train_lengths': datasets['train'].lengths,\n",
    "            \n",
    "            'val_sequences': datasets['val'].sequences,\n",
    "            'val_masks': datasets['val'].attention_masks,\n",
    "            'val_labels': datasets['val'].labels,\n",
    "            'val_lengths': datasets['val'].lengths,\n",
    "            \n",
    "            'test_sequences': datasets['test'].sequences,\n",
    "            'test_masks': datasets['test'].attention_masks,\n",
    "            'test_labels': datasets['test'].labels,\n",
    "            'test_lengths': datasets['test'].lengths,\n",
    "        }\n",
    "        \n",
    "        torch.save(save_data, data_path)\n",
    "        file_size = data_path.stat().st_size / 1024 / 1024\n",
    "        print(f\"‚úÖ Tensor data saved to: {data_path} (Size: {file_size:.2f} MB)\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _save_metadata(metadata, output_path):\n",
    "        metadata_path = output_path / 'preprocessing_metadata.json'\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str, ensure_ascii=False)\n",
    "        print(f\"‚úÖ Metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _verify_files(output_path):\n",
    "        print(\"üìÅ Verifying generated files:\")\n",
    "        for file in output_path.iterdir():\n",
    "            size_kb = file.stat().st_size / 1024\n",
    "            print(f\"  {file.name} ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3593ed-7193-427e-9df1-684fe97d82d3",
   "metadata": {},
   "source": [
    "## UnifiedDataProcessingPipeline - Process Coordination Module\n",
    "\n",
    "**Function Methods:**\n",
    "- run_complete_pipeline() - Execute complete multi-schema data processing pipeline\n",
    "- process_single_schema() - Process data for single label schema\n",
    "- _ensure_data_ready() - Ensure data preparation is complete (execute only once)\n",
    "- _vectorize_texts() - Batch vectorize texts\n",
    "- _create_datasets() - Create train/validation/test datasets\n",
    "- _check_data_size() - Check data scale\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5b34c6b-730e-4553-8ba1-0b2f9b4e93c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedDataProcessingPipeline:\n",
    "    \n",
    "    def __init__(self, max_vocab_size: int = 30000, max_length: int = 512, \n",
    "                 min_freq: int = 2, seed: int = 42):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.min_freq = min_freq\n",
    "        self.seed = seed\n",
    "        self.data_dir = None  # Add data directory cache\n",
    "    \n",
    "    def _ensure_data_ready(self) -> bool:\n",
    "        if self.data_dir is not None:\n",
    "            return True\n",
    "            \n",
    "        try:\n",
    "            print(\"üîç Checking data source...\")\n",
    "            self.data_dir = DataDownloader.setup_data()\n",
    "            print(\"‚úÖ Data source preparation completed\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Data source preparation failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def process_single_schema(self, label_schema: str) -> bool:\n",
    "        try:\n",
    "            # 0. Ensure data is ready\n",
    "            if not self._ensure_data_ready():\n",
    "                return False\n",
    "                \n",
    "            schema_config = LabelProcessor.get_label_schema_config(label_schema)\n",
    "            print(f\"{'='*50}\")\n",
    "            print(f\"=== Processing {schema_config['name']} Data ===\")\n",
    "            print(f\"Label range: {schema_config['label_range']}\")\n",
    "            print(f\"Number of classes: {schema_config['num_classes']}\")\n",
    "            print(f\"Output directory: {schema_config['output_dir']}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            # 1. Initialize components\n",
    "            vocab_builder = VocabularyBuilder(self.max_vocab_size, self.min_freq)\n",
    "            text_processor = TextProcessor()\n",
    "            \n",
    "            # 2. Load raw data\n",
    "            print(\"üì• Loading raw data...\")\n",
    "            train_texts, train_labels, test_texts, test_labels = DataLoaderManager.load_raw_data(\n",
    "                self.data_dir, label_schema\n",
    "            )\n",
    "            \n",
    "            # 3. Merge and split data\n",
    "            print(\"üìä Data splitting...\")\n",
    "            all_texts = train_texts + test_texts\n",
    "            all_labels = train_labels + test_labels\n",
    "            \n",
    "            train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = \\\n",
    "                DataSplitter.split_data_with_seeds(all_texts, all_labels, self.seed)\n",
    "            \n",
    "            # 4. Text cleaning\n",
    "            print(\"üßπ Cleaning text data...\")\n",
    "            train_texts_clean = text_processor.process_in_batches(\n",
    "                train_texts, text_processor.robust_text_cleaning, process_name=\"Text cleaning\"\n",
    "            )\n",
    "            val_texts_clean = text_processor.process_in_batches(\n",
    "                val_texts, text_processor.robust_text_cleaning, process_name=\"Text cleaning\"\n",
    "            )\n",
    "            test_texts_clean = text_processor.process_in_batches(\n",
    "                test_texts, text_processor.robust_text_cleaning, process_name=\"Text cleaning\"\n",
    "            )\n",
    "            \n",
    "            # 5. Build vocabulary\n",
    "            print(\"üìö Building vocabulary...\")\n",
    "            vocab_builder.build_vocabulary(train_texts_clean)\n",
    "            \n",
    "            # 6. Text vectorization\n",
    "            print(\"üî¢ Vectorizing data...\")\n",
    "            train_sequences, train_masks, train_lengths = self._vectorize_texts(\n",
    "                train_texts_clean, vocab_builder, self.max_length\n",
    "            )\n",
    "            val_sequences, val_masks, val_lengths = self._vectorize_texts(\n",
    "                val_texts_clean, vocab_builder, self.max_length\n",
    "            )\n",
    "            test_sequences, test_masks, test_lengths = self._vectorize_texts(\n",
    "                test_texts_clean, vocab_builder, self.max_length\n",
    "            )\n",
    "            \n",
    "            # 7. Create datasets\n",
    "            print(\"üóÇÔ∏è Creating datasets...\")\n",
    "            datasets = self._create_datasets(\n",
    "                train_sequences, train_masks, train_labels, train_lengths,\n",
    "                val_sequences, val_masks, val_labels, val_lengths,\n",
    "                test_sequences, test_masks, test_labels, test_lengths\n",
    "            )\n",
    "            \n",
    "            # 8. Prepare data for saving\n",
    "            vocab_info = {\n",
    "                'token_to_id': vocab_builder.token_to_id,\n",
    "                'id_to_token': vocab_builder.id_to_token,\n",
    "                'vocab_size': vocab_builder.vocab_size,\n",
    "                'special_tokens': vocab_builder.special_tokens\n",
    "            }\n",
    "            \n",
    "            metadata = {\n",
    "                'label_schema': label_schema,\n",
    "                'schema_config': schema_config,\n",
    "                'sequence_length_analysis': {\n",
    "                    'mean_length': np.mean(train_lengths),\n",
    "                    'max_length': np.max(train_lengths),\n",
    "                    'min_length': np.min(train_lengths),\n",
    "                    'total_samples': len(train_sequences)\n",
    "                },\n",
    "                'class_distribution': {\n",
    "                    'train': dict(Counter(train_labels)),\n",
    "                    'val': dict(Counter(val_labels)),\n",
    "                    'test': dict(Counter(test_labels))\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # 9. Save data\n",
    "            DataSaver.save_processed_data(\n",
    "                datasets, vocab_info, metadata, \n",
    "                f\"./{schema_config['output_dir']}\", \n",
    "                label_schema\n",
    "            )\n",
    "            \n",
    "            print(f\"üéâ {schema_config['name']} data processing completed!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {label_schema} processing failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def run_complete_pipeline(self) -> bool:\n",
    "        print(\"üöÄ Starting unified data processing pipeline\")\n",
    "        \n",
    "        # First uniformly check data source\n",
    "        print(\"üì¶ Preparing data source...\")\n",
    "        if not self._ensure_data_ready():\n",
    "            print(\"‚ùå Data source preparation failed, terminating processing\")\n",
    "            return False\n",
    "            \n",
    "        success_count = 0\n",
    "        schemas = [\"binary\", \"multiclass\"]\n",
    "        \n",
    "        for schema in schemas:\n",
    "            success = self.process_single_schema(schema)\n",
    "            if success:\n",
    "                success_count += 1\n",
    "                self._check_data_size(schema)\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"üéØ Data processing completion summary:\")\n",
    "        print(f\"Successfully processed: {success_count}/{len(schemas)} data schemas\")\n",
    "        \n",
    "        if success_count == len(schemas):\n",
    "            print(\"‚úÖ All data schemas processed successfully!\")\n",
    "            print(\"üìÅ Generated data directories:\")\n",
    "            for schema in schemas:\n",
    "                config = LabelProcessor.get_label_schema_config(schema)\n",
    "                print(f\"  - {config['output_dir']}: {config['name']}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Partial data processing failed\")\n",
    "            return False\n",
    "    \n",
    "    def _vectorize_texts(self, texts: List[str], vocab_builder: VocabularyBuilder, max_length: int):\n",
    "        sequences, masks, lengths = [], [], []\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            seq, mask, length = vocab_builder.text_to_sequence(text, max_length)\n",
    "            sequences.append(seq)\n",
    "            masks.append(mask)\n",
    "            lengths.append(length)\n",
    "            \n",
    "            if i % 5000 == 0 and i > 0:\n",
    "                print(f\"  Vectorized {i}/{len(texts)} samples\")\n",
    "        \n",
    "        return sequences, masks, lengths\n",
    "    \n",
    "    def _create_datasets(self, train_sequences, train_masks, train_labels, train_lengths,\n",
    "                        val_sequences, val_masks, val_labels, val_lengths,\n",
    "                        test_sequences, test_masks, test_labels, test_lengths):\n",
    "        train_dataset = IMDBDataset(train_sequences, train_masks, train_labels, train_lengths)\n",
    "        val_dataset = IMDBDataset(val_sequences, val_masks, val_labels, val_lengths)\n",
    "        test_dataset = IMDBDataset(test_sequences, test_masks, test_labels, test_lengths)\n",
    "        \n",
    "        return {\n",
    "            'train': train_dataset,\n",
    "            'val': val_dataset,\n",
    "            'test': test_dataset\n",
    "        }\n",
    "    \n",
    "    def _check_data_size(self, label_schema: str):\n",
    "        config = LabelProcessor.get_label_schema_config(label_schema)\n",
    "        data_path = f\"./{config['output_dir']}/all_data.pt\"\n",
    "        \n",
    "        try:\n",
    "            data = torch.load(data_path)\n",
    "            \n",
    "            print(f\"=== {config['name']} Data Scale Check ===\")\n",
    "            print(f\"Training sequences: {data['train_sequences'].shape}\")\n",
    "            print(f\"Training labels: {data['train_labels'].shape}\")\n",
    "            print(f\"Validation sequences: {data['val_sequences'].shape}\")\n",
    "            print(f\"Test sequences: {data['test_sequences'].shape}\")\n",
    "            \n",
    "            total_samples = (data['train_sequences'].shape[0] + \n",
    "                            data['val_sequences'].shape[0] + \n",
    "                            data['test_sequences'].shape[0])\n",
    "            \n",
    "            print(f\"Total samples: {total_samples}\")\n",
    "            \n",
    "            if total_samples >= 40000:\n",
    "                print(\"‚úÖ Processed complete dataset\")\n",
    "            else:\n",
    "                print(\"‚ùå Abnormal data volume, please check processing\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Check failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d222375-a31b-43ad-bd47-6d8d0d30f049",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "**Function Methods:**\n",
    "Check processed data scale and statistical information\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdc50dad-baf5-4d09-914d-24a28ba3b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_size(data_path: str):\n",
    "    try:\n",
    "        data = torch.load(data_path)\n",
    "        \n",
    "        print(\"=== Data Scale Check ===\")\n",
    "        print(f\"Training sequences: {data['train_sequences'].shape}\")\n",
    "        print(f\"Training labels: {data['train_labels'].shape}\")\n",
    "        print(f\"Validation sequences: {data['val_sequences'].shape}\") \n",
    "        print(f\"Test sequences: {data['test_sequences'].shape}\")\n",
    "        \n",
    "        total_samples = (data['train_sequences'].shape[0] + \n",
    "                        data['val_sequences'].shape[0] + \n",
    "                        data['test_sequences'].shape[0])\n",
    "        \n",
    "        print(f\"Total samples: {total_samples}\")\n",
    "        \n",
    "        if total_samples >= 40000:\n",
    "            print(\"‚úÖ Processed complete dataset\")\n",
    "        elif total_samples >= 20000:\n",
    "            print(\"‚ö†Ô∏è  Processed partial dataset\")  \n",
    "        else:\n",
    "            print(\"‚ùå Abnormal data volume, please check processing\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Check failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820dfb2c-faa7-4c25-be0e-2fa021e136bf",
   "metadata": {},
   "source": [
    "## Main Program Entry\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "126b164e-e36b-4132-98be-3e3646e53142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting unified data processing pipeline\n",
      "üì¶ Preparing data source...\n",
      "üîç Checking data source...\n",
      "Data directory incomplete, re-downloading...\n",
      "Downloading IMDb dataset...\n",
      "Download completed, extracting...\n",
      "IMDb dataset preparation completed\n",
      "‚úÖ Data source preparation completed\n",
      "==================================================\n",
      "=== Processing Binary Classification Data ===\n",
      "Label range: 0-1 (Negative/Positive)\n",
      "Number of classes: 2\n",
      "Output directory: processed_data_binary\n",
      "==================================================\n",
      "üì• Loading raw data...\n",
      "Loading IMDb dataset - Binary Classification...\n",
      "train label distribution: {0: 12500, 1: 12500}\n",
      "test label distribution: {0: 12500, 1: 12500}\n",
      "Loaded 25000 training samples, 25000 test samples\n",
      "üìä Data splitting...\n",
      "Performing reproducible data splitting (with class checking)...\n",
      "Data split: Training set=32000, Validation set=8000, Test set=10000\n",
      "Training set class distribution: {0: 16000, 1: 16000}\n",
      "Validation set class distribution: {0: 4000, 1: 4000}\n",
      "Test set class distribution: {0: 5000, 1: 5000}\n",
      "üßπ Cleaning text data...\n",
      "  Text cleaning 5000/32000 samples\n",
      "  Text cleaning 10000/32000 samples\n",
      "  Text cleaning 15000/32000 samples\n",
      "  Text cleaning 20000/32000 samples\n",
      "  Text cleaning 25000/32000 samples\n",
      "  Text cleaning 30000/32000 samples\n",
      "‚úÖ Text cleaning completed\n",
      "  Text cleaning 5000/8000 samples\n",
      "‚úÖ Text cleaning completed\n",
      "  Text cleaning 5000/10000 samples\n",
      "‚úÖ Text cleaning completed\n",
      "üìö Building vocabulary...\n",
      "Building vocabulary...\n",
      "Vocabulary building completed: 30000 tokens\n",
      "Vocabulary coverage: 97.51%\n",
      "üî¢ Vectorizing data...\n",
      "  Vectorized 5000/32000 samples\n",
      "  Vectorized 10000/32000 samples\n",
      "  Vectorized 15000/32000 samples\n",
      "  Vectorized 20000/32000 samples\n",
      "  Vectorized 25000/32000 samples\n",
      "  Vectorized 30000/32000 samples\n",
      "  Vectorized 5000/8000 samples\n",
      "  Vectorized 5000/10000 samples\n",
      "üóÇÔ∏è Creating datasets...\n",
      "Starting to save Binary Classification data to: processed_data_binary\n",
      "‚úÖ Vocabulary saved to: processed_data_binary/vocabulary.json\n",
      "‚úÖ Configuration saved to: processed_data_binary/preprocessing_config.json\n",
      "‚úÖ Tensor data saved to: processed_data_binary/all_data.pt (Size: 391.39 MB)\n",
      "‚úÖ Metadata saved to: processed_data_binary/preprocessing_metadata.json\n",
      "üìÅ Verifying generated files:\n",
      "  vocabulary.json (1332.5 KB)\n",
      "  preprocessing_config.json (0.3 KB)\n",
      "  preprocessing_metadata.json (0.5 KB)\n",
      "  all_data.pt (400785.3 KB)\n",
      "üéâ Binary Classification data processing completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3l/d2wgnjyn1llgs_42vz3xhrw40000gn/T/ipykernel_49187/803091352.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(data_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Binary Classification Data Scale Check ===\n",
      "Training sequences: torch.Size([32000, 512])\n",
      "Training labels: torch.Size([32000])\n",
      "Validation sequences: torch.Size([8000, 512])\n",
      "Test sequences: torch.Size([10000, 512])\n",
      "Total samples: 50000\n",
      "‚úÖ Processed complete dataset\n",
      "==================================================\n",
      "=== Processing Multi-class (1-10 stars) Data ===\n",
      "Label range: 1-10 stars (Missing 5-6 stars)\n",
      "Number of classes: 8\n",
      "Output directory: processed_data_multiclass\n",
      "==================================================\n",
      "üì• Loading raw data...\n",
      "Loading IMDb dataset - Multi-class (1-10 stars)...\n",
      "train label distribution: {1: 5100, 2: 2284, 3: 2420, 4: 2696, 7: 2496, 8: 3009, 9: 2263, 10: 4732}\n",
      "test label distribution: {1: 5022, 2: 2302, 3: 2541, 4: 2635, 7: 2307, 8: 2850, 9: 2344, 10: 4999}\n",
      "Loaded 25000 training samples, 25000 test samples\n",
      "üìä Data splitting...\n",
      "Performing reproducible data splitting (with class checking)...\n",
      "Data split: Training set=32000, Validation set=8000, Test set=10000\n",
      "Training set class distribution: {1: 6478, 2: 2935, 3: 3175, 4: 3412, 7: 3074, 8: 3750, 9: 2948, 10: 6228}\n",
      "Validation set class distribution: {1: 1620, 2: 734, 3: 794, 4: 853, 7: 768, 8: 937, 9: 737, 10: 1557}\n",
      "Test set class distribution: {1: 2024, 2: 917, 3: 992, 4: 1066, 7: 961, 8: 1172, 9: 922, 10: 1946}\n",
      "üßπ Cleaning text data...\n",
      "  Text cleaning 5000/32000 samples\n",
      "  Text cleaning 10000/32000 samples\n",
      "  Text cleaning 15000/32000 samples\n",
      "  Text cleaning 20000/32000 samples\n",
      "  Text cleaning 25000/32000 samples\n",
      "  Text cleaning 30000/32000 samples\n",
      "‚úÖ Text cleaning completed\n",
      "  Text cleaning 5000/8000 samples\n",
      "‚úÖ Text cleaning completed\n",
      "  Text cleaning 5000/10000 samples\n",
      "‚úÖ Text cleaning completed\n",
      "üìö Building vocabulary...\n",
      "Building vocabulary...\n",
      "Vocabulary building completed: 30000 tokens\n",
      "Vocabulary coverage: 97.53%\n",
      "üî¢ Vectorizing data...\n",
      "  Vectorized 5000/32000 samples\n",
      "  Vectorized 10000/32000 samples\n",
      "  Vectorized 15000/32000 samples\n",
      "  Vectorized 20000/32000 samples\n",
      "  Vectorized 25000/32000 samples\n",
      "  Vectorized 30000/32000 samples\n",
      "  Vectorized 5000/8000 samples\n",
      "  Vectorized 5000/10000 samples\n",
      "üóÇÔ∏è Creating datasets...\n",
      "Starting to save Multi-class (1-10 stars) data to: processed_data_multiclass\n",
      "‚úÖ Vocabulary saved to: processed_data_multiclass/vocabulary.json\n",
      "‚úÖ Configuration saved to: processed_data_multiclass/preprocessing_config.json\n",
      "‚úÖ Tensor data saved to: processed_data_multiclass/all_data.pt (Size: 391.39 MB)\n",
      "‚úÖ Metadata saved to: processed_data_multiclass/preprocessing_metadata.json\n",
      "üìÅ Verifying generated files:\n",
      "  vocabulary.json (1332.8 KB)\n",
      "  preprocessing_config.json (0.3 KB)\n",
      "  preprocessing_metadata.json (0.8 KB)\n",
      "  all_data.pt (400785.3 KB)\n",
      "üéâ Multi-class (1-10 stars) data processing completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3l/d2wgnjyn1llgs_42vz3xhrw40000gn/T/ipykernel_49187/803091352.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(data_path)\n",
      "/var/folders/3l/d2wgnjyn1llgs_42vz3xhrw40000gn/T/ipykernel_49187/1294867128.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(data_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multi-class (1-10 stars) Data Scale Check ===\n",
      "Training sequences: torch.Size([32000, 512])\n",
      "Training labels: torch.Size([32000])\n",
      "Validation sequences: torch.Size([8000, 512])\n",
      "Test sequences: torch.Size([10000, 512])\n",
      "Total samples: 50000\n",
      "‚úÖ Processed complete dataset\n",
      "============================================================\n",
      "üéØ Data processing completion summary:\n",
      "Successfully processed: 2/2 data schemas\n",
      "‚úÖ All data schemas processed successfully!\n",
      "üìÅ Generated data directories:\n",
      "  - processed_data_binary: Binary Classification\n",
      "  - processed_data_multiclass: Multi-class (1-10 stars)\n",
      "=== Data Scale Check ===\n",
      "Training sequences: torch.Size([32000, 512])\n",
      "Training labels: torch.Size([32000])\n",
      "Validation sequences: torch.Size([8000, 512])\n",
      "Test sequences: torch.Size([10000, 512])\n",
      "Total samples: 50000\n",
      "‚úÖ Processed complete dataset\n",
      "=== Data Scale Check ===\n",
      "Training sequences: torch.Size([32000, 512])\n",
      "Training labels: torch.Size([32000])\n",
      "Validation sequences: torch.Size([8000, 512])\n",
      "Test sequences: torch.Size([10000, 512])\n",
      "Total samples: 50000\n",
      "‚úÖ Processed complete dataset\n",
      "üéâ All data processing completed! Now you can start training models for different tasks!\n",
      "   - Binary sentiment analysis: Use processed_data_binary/\n",
      "   - Multi-class star rating prediction: Use processed_data_multiclass/\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create unified data processing pipeline\n",
    "    pipeline = UnifiedDataProcessingPipeline(\n",
    "        max_vocab_size=30000,\n",
    "        max_length=512, \n",
    "        min_freq=2,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Run complete multi-schema processing\n",
    "    success = pipeline.run_complete_pipeline()\n",
    "    \n",
    "    if success:\n",
    "        check_data_size('./processed_data_binary/all_data.pt')\n",
    "        check_data_size('./processed_data_multiclass/all_data.pt')\n",
    "        print(\"üéâ All data processing completed! Now you can start training models for different tasks!\")\n",
    "        print(\"   - Binary sentiment analysis: Use processed_data_binary/\")\n",
    "        print(\"   - Multi-class star rating prediction: Use processed_data_multiclass/\")\n",
    "    else:\n",
    "        print(\"‚ùå Data processing failed, please check error messages\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
