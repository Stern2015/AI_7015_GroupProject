{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab47aa03",
   "metadata": {},
   "source": [
    "# Group Project: æ•°æ®é¢„åŠ è½½&æ¸…æ´—\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a900734c-4d56-4923-ad51-4349706592f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import html\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478321d-d359-403a-a86a-ce8071f5179f",
   "metadata": {},
   "source": [
    "## DataDownloader - æ•°æ®ä¸‹è½½ä¸éªŒè¯æ¨¡å—\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "- download_imdb() - ä¸‹è½½IMDbæ•°æ®é›†å‹ç¼©åŒ…å¹¶è§£å‹\n",
    "\n",
    "- validate_data_directory() - éªŒè¯æ•°æ®ç›®å½•ç»“æ„æ˜¯å¦å®Œæ•´\n",
    "\n",
    "- setup_data() - ä¸»å…¥å£ï¼Œç¡®ä¿æ•°æ®å¯ç”¨ï¼ˆä¸‹è½½æˆ–ä½¿ç”¨ç°æœ‰æ•°æ®ï¼‰\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c4ee9f-a990-43bf-bc51-8988b31ce3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDownloader:\n",
    "    \"\"\"æ•°æ®ä¸‹è½½ä¸éªŒè¯æ¨¡å—\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def download_imdb() -> Path:\n",
    "        \"\"\"ä¸‹è½½IMDbæ•°æ®é›†\"\"\"\n",
    "        data_dir = Path('./aclImdb')\n",
    "        \n",
    "        if data_dir.exists():\n",
    "            print(\"IMDbæ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½\")\n",
    "            return data_dir\n",
    "        \n",
    "        print(\"ä¸‹è½½IMDbæ•°æ®é›†...\")\n",
    "        url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "        tar_path = Path('./aclImdb_v1.tar.gz')\n",
    "        \n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, tar_path)\n",
    "            print(\"ä¸‹è½½å®Œæˆï¼Œè§£å‹ä¸­...\")\n",
    "            \n",
    "            with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "                tar.extractall(path='./')\n",
    "            \n",
    "            tar_path.unlink()\n",
    "            print(\"IMDbæ•°æ®é›†å‡†å¤‡å®Œæˆ\")\n",
    "            return data_dir\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"æ•°æ®é›†ä¸‹è½½å¤±è´¥: {e}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_data_directory(data_dir: Path) -> bool:\n",
    "        \"\"\"éªŒè¯æ•°æ®ç›®å½•ç»“æ„\"\"\"\n",
    "        required_dirs = [\n",
    "            data_dir / 'train' / 'pos',\n",
    "            data_dir / 'train' / 'neg', \n",
    "            data_dir / 'test' / 'pos',\n",
    "            data_dir / 'test' / 'neg'\n",
    "        ]\n",
    "        \n",
    "        for dir_path in required_dirs:\n",
    "            if not dir_path.exists():\n",
    "                return False\n",
    "            if len(list(dir_path.glob('*.txt'))) == 0:\n",
    "                return False\n",
    "                \n",
    "        return True\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_data() -> Path:\n",
    "        \"\"\"è®¾ç½®æ•°æ®ç›®å½•ï¼Œç¡®ä¿æ•°æ®å¯ç”¨\"\"\"\n",
    "        data_dir = Path('./aclImdb')\n",
    "        \n",
    "        if DataDownloader.validate_data_directory(data_dir):\n",
    "            print(\"âœ… æ•°æ®ç›®å½•éªŒè¯é€šè¿‡\")\n",
    "            return data_dir\n",
    "        \n",
    "        print(\"æ•°æ®ç›®å½•ä¸å®Œæ•´ï¼Œé‡æ–°ä¸‹è½½...\")\n",
    "        data_dir = DataDownloader.download_imdb()\n",
    "        \n",
    "        if not DataDownloader.validate_data_directory(data_dir):\n",
    "            raise Exception(\"æ•°æ®ç›®å½•éªŒè¯å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥æ•°æ®é›†\")\n",
    "            \n",
    "        return data_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd27fd7-702e-4e70-badf-3302b20904d7",
   "metadata": {},
   "source": [
    "## TextProcessor - æ–‡æœ¬å¤„ç†æ¨¡å—\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "- robust_text_cleaning(text) - æ ¸å¿ƒæ–‡æœ¬æ¸…æ´—ï¼ŒåŒ…å«ï¼š\n",
    "\n",
    "    1. HTMLå®ä½“è§£ç å’Œæ ‡ç­¾ç§»é™¤\n",
    "    \n",
    "    2. URLå¤„ç†\n",
    "    \n",
    "    3. ç¼©å†™è¯å±•å¼€ï¼ˆå¦‚ \"can't\" â†’ \"can not\"ï¼‰\n",
    "    \n",
    "    4. æ ‡ç‚¹ç¬¦å·æ ‡å‡†åŒ–\n",
    "    \n",
    "    5. å¤§å°å†™ç»Ÿä¸€\n",
    "\n",
    "- process_in_batches() - æ‰¹é‡å¤„ç†æ–‡æœ¬ï¼Œé¿å…å†…å­˜æº¢å‡º\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c6ef375-2e6b-4057-bf48-7c0b8822574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    \"\"\"æ–‡æœ¬å¤„ç†æ¨¡å—\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def robust_text_cleaning(text: str) -> str:\n",
    "        \"\"\"é²æ£’çš„æ–‡æœ¬æ¸…æ´—ç®¡é“\"\"\"\n",
    "        # è§£ç HTMLå®ä½“\n",
    "        text = html.unescape(text)\n",
    "        \n",
    "        # ç§»é™¤HTMLæ ‡ç­¾\n",
    "        text = re.sub(r'<[^>]+>', ' ', text)\n",
    "        \n",
    "        # å¤„ç†URL\n",
    "        text = re.sub(r'http\\S+', ' <URL> ', text)\n",
    "        \n",
    "        # å¤„ç†å¸¸è§çš„ç¼©å†™å’Œå¦å®šå½¢å¼\n",
    "        contractions = {\n",
    "            r\"won't\": \"will not\", r\"can't\": \"can not\", r\"n't\": \" not\",\n",
    "            r\"'re\": \" are\", r\"'s\": \" is\", r\"'d\": \" would\", \n",
    "            r\"'ll\": \" will\", r\"'t\": \" not\", r\"'ve\": \" have\",\n",
    "            r\"'m\": \" am\"\n",
    "        }\n",
    "        \n",
    "        for pattern, replacement in contractions.items():\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        \n",
    "        # ä¿ç•™åŸºæœ¬çš„æ ‡ç‚¹ç”¨äºæƒ…æ„Ÿåˆ†æ\n",
    "        text = re.sub(r'[^a-zA-Z\\s\\.!?,;:\\']', ' ', text)\n",
    "        \n",
    "        # å¤„ç†é‡å¤çš„æ ‡ç‚¹\n",
    "        text = re.sub(r'([!?.]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'[!?.]+', r' \\0 ', text)\n",
    "        \n",
    "        # ç»Ÿä¸€ç©ºæ ¼å¤„ç†\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text.strip().lower()\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_in_batches(texts: List[str], process_func, batch_size: int = 1000, process_name: str = \"å¤„ç†\") -> List[str]:\n",
    "        \"\"\"æ‰¹é‡å¤„ç†æ–‡æœ¬æ•°æ®\"\"\"\n",
    "        processed = []\n",
    "        total = len(texts)\n",
    "        \n",
    "        for i in range(0, total, batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            processed_batch = [process_func(text) for text in batch]\n",
    "            processed.extend(processed_batch)\n",
    "            \n",
    "            if i % 5000 == 0 and i > 0:\n",
    "                print(f\"  {process_name} {i}/{total} ä¸ªæ ·æœ¬\")\n",
    "        \n",
    "        print(f\"âœ… {process_name}å®Œæˆ\")\n",
    "        return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7939994-4d01-4ade-aa3e-7f25e28541d9",
   "metadata": {},
   "source": [
    "## LabelProcessor - æ ‡ç­¾å¤„ç†æ¨¡å—\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "\n",
    "- parse_rating_from_filename() - ä»æ–‡ä»¶åè§£ææ˜Ÿçº§è¯„åˆ†\n",
    "- \r\n",
    "convert_to_binary() - å°†æ˜Ÿçº§è½¬æ¢ä¸ºäºŒåˆ†ç±»\n",
    "- get_label_schema_configig() - è·å–æ ‡ç­¾\n",
    "æ¨¡å¼é…ç½®æ¨¡å¼ç½®\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2bb8b76-9d50-4469-8f46-cada52225ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelProcessor:\n",
    "    \"\"\"æ ‡ç­¾å¤„ç†æ¨¡å— - æ”¯æŒä¸¤ç§æ ‡ç­¾æ ¼å¼\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_rating_from_filename(file_path: Path) -> Optional[int]:\n",
    "        \"\"\"ä»æ–‡ä»¶åè§£ææ˜Ÿçº§è¯„åˆ†\"\"\"\n",
    "        stem = file_path.stem\n",
    "        parts = stem.split('_')\n",
    "        if len(parts) < 2:\n",
    "            return None\n",
    "        \n",
    "        rating_str = parts[-1]\n",
    "        try:\n",
    "            return int(rating_str)  # 1..10\n",
    "        except ValueError:\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_binary(rating: int) -> int:\n",
    "        \"\"\"å°†æ˜Ÿçº§è½¬æ¢ä¸ºäºŒåˆ†ç±»æ ‡ç­¾\"\"\"\n",
    "        if rating <= 4:\n",
    "            return 0  # è´Ÿé¢\n",
    "        else:  # rating >= 7\n",
    "            return 1  # æ­£é¢\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_label_schema_config(schema_type: str) -> Dict:\n",
    "        \"\"\"è·å–æ ‡ç­¾æ¨¡å¼é…ç½®\"\"\"\n",
    "        schemas = {\n",
    "            \"binary\": {\n",
    "                \"name\": \"äºŒåˆ†ç±»\",\n",
    "                \"num_classes\": 2,\n",
    "                \"output_dir\": \"processed_data_binary\",\n",
    "                \"label_range\": \"0-1 (è´Ÿé¢/æ­£é¢)\"\n",
    "            },\n",
    "            \"multiclass\": {\n",
    "                \"name\": \"å¤šåˆ†ç±»(1-10æ˜Ÿ)\",\n",
    "                \"num_classes\": 8,  # å®é™…åªæœ‰8ä¸ªç±»åˆ« (1-4, 7-10)\n",
    "                \"output_dir\": \"processed_data_multiclass\", \n",
    "                \"label_range\": \"1-10æ˜Ÿ (ç¼ºå¤±5-6æ˜Ÿ)\"\n",
    "            }\n",
    "        }\n",
    "        return schemas.get(schema_type, schemas[\"binary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7baa71-8a82-4148-afe4-56c5dc27675b",
   "metadata": {},
   "source": [
    "## DataLoaderManager - æ•°æ®åŠ è½½ç®¡ç†å™¨\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "\n",
    "- load_raw_data() - æ ¹æ®æ ‡ç­¾æ¨¡å¼åŠ è½½åŸå§‹IMDbæ•°æ®\n",
    "åŒ–æ ¼å¼\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f865927f-8828-495c-b1c0-a031bc9c8bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderManager:\n",
    "    \"\"\"æ•°æ®åŠ è½½ç®¡ç†æ¨¡å—\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_raw_data(data_dir: Path, label_schema: str) -> Tuple[List[str], List[int], List[str], List[int]]:\n",
    "        \"\"\"åŠ è½½åŸå§‹IMDbæ•°æ®ï¼Œæ”¯æŒä¸åŒæ ‡ç­¾æ¨¡å¼\"\"\"\n",
    "        print(f\"åŠ è½½IMDbæ•°æ®é›† - {LabelProcessor.get_label_schema_config(label_schema)['name']}...\")\n",
    "        \n",
    "        def load_from_directory(directory: Path, schema: str) -> Tuple[List[str], List[int]]:\n",
    "            texts, labels = [], []\n",
    "            \n",
    "            for label_type in ['pos', 'neg']:\n",
    "                dir_name = directory / label_type\n",
    "                if not dir_name.exists():\n",
    "                    print(f\"Warning: ç¼ºå°‘ç›®å½•: {dir_name}\")\n",
    "                    continue\n",
    "                    \n",
    "                for file_path in dir_name.glob('*.txt'):\n",
    "                    # è§£ææ˜Ÿçº§è¯„åˆ†\n",
    "                    rating = LabelProcessor.parse_rating_from_filename(file_path)\n",
    "                    if rating is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # æ ¹æ®æ¨¡å¼é€‰æ‹©æ ‡ç­¾\n",
    "                    if schema == \"binary\":\n",
    "                        label = LabelProcessor.convert_to_binary(rating)\n",
    "                    else:  # multiclass\n",
    "                        label = rating\n",
    "                    \n",
    "                    # è¯»å–æ–‡æœ¬\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            text = f.read().strip()\n",
    "                        texts.append(text)\n",
    "                        labels.append(label)\n",
    "                    except Exception as e:\n",
    "                        print(f\"è·³è¿‡æ— æ³•è¯»å–æ–‡ä»¶: {file_path.name} ({e})\")\n",
    "                        continue\n",
    "            \n",
    "            # åˆ†å¸ƒæ£€æŸ¥\n",
    "            dist = Counter(labels)\n",
    "            dist_sorted = dict(sorted(dist.items()))\n",
    "            print(f\"{directory.name} æ ‡ç­¾åˆ†å¸ƒ: {dist_sorted}\")\n",
    "            return texts, labels\n",
    "        \n",
    "        train_dir = data_dir / 'train'\n",
    "        test_dir = data_dir / 'test'\n",
    "        \n",
    "        train_texts, train_labels = load_from_directory(train_dir, label_schema)\n",
    "        test_texts, test_labels = load_from_directory(test_dir, label_schema)\n",
    "        \n",
    "        print(f\"åŠ è½½äº† {len(train_texts)} è®­ç»ƒæ ·æœ¬, {len(test_texts)} æµ‹è¯•æ ·æœ¬\")\n",
    "        return train_texts, train_labels, test_texts, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f3c8d1-a165-414b-8c32-119630d5db98",
   "metadata": {},
   "source": [
    "## DataSplitter - æ•°æ®åˆ’åˆ†å™¨\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "- split_data_with_seeds()ï¼šä½¿ç”¨éšæœºç§å­è¿›è¡Œå¯å¤ç°çš„æ•°æ®åˆ’åˆ†\n",
    "  \r",
    "    1.  è‡ªåŠ¨åˆ†å±‚æŠ½æ ·ä¿æŠ¤    2. â”€ å¤„ç†å°æ ·æœ¬ç±»    3. â”€â”€ è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†(60%/20%/20    4. â””â”€â”€ åˆ†å¸ƒç»Ÿè®¡è¾“å‡º\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "031611e6-0fb9-40c8-b38f-55a827775909",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplitter:\n",
    "    \"\"\"æ•°æ®åˆ’åˆ†æ¨¡å—\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_data_with_seeds(texts: List[str], labels: List[int], seed: int = 42,\n",
    "                             test_size: float = 0.2, val_size: float = 0.2) -> Tuple:\n",
    "        \"\"\"ä½¿ç”¨éšæœºç§å­è¿›è¡Œå¯å¤ç°çš„æ•°æ®åˆ’åˆ†\"\"\"\n",
    "        \n",
    "        def _print_dist(name: str, y: List[int]):\n",
    "            c = Counter(y)\n",
    "            print(f\"{name} ç±»åˆ«åˆ†å¸ƒ: {dict(sorted(c.items()))}\")\n",
    "        \n",
    "        print(\"æ‰§è¡Œå¯å¤ç°çš„æ•°æ®åˆ’åˆ†ï¼ˆå¸¦ç±»åˆ«æ£€æŸ¥ï¼‰...\")\n",
    "        counts = Counter(labels)\n",
    "        min_count = min(counts.values()) if counts else 0\n",
    "        use_stratify = labels if min_count >= 2 else None\n",
    "        \n",
    "        if use_stratify is None:\n",
    "            print(f\"Warning: æœ€å°ç±»åˆ«æ ·æœ¬æ•° {min_count} < 2ï¼Œåˆæ¬¡åˆ’åˆ†ä¸ä½¿ç”¨åˆ†å±‚æŠ½æ ·\")\n",
    "        \n",
    "        # å…ˆåˆ’åˆ†æµ‹è¯•é›†\n",
    "        train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "            texts, labels, test_size=test_size, random_state=seed,\n",
    "            stratify=use_stratify\n",
    "        )\n",
    "        \n",
    "        # å†ä»è®­ç»ƒé›†åˆ’åˆ†éªŒè¯é›†\n",
    "        train_counts = Counter(train_labels)\n",
    "        min_train_count = min(train_counts.values()) if train_counts else 0\n",
    "        use_stratify_val = train_labels if min_train_count >= 2 else None\n",
    "        \n",
    "        if use_stratify_val is None:\n",
    "            print(f\"Warning: è®­ç»ƒé›†ä¸­æœ€å°ç±»åˆ«æ ·æœ¬æ•° {min_train_count} < 2ï¼ŒéªŒè¯é›†åˆ’åˆ†ä¸ä½¿ç”¨åˆ†å±‚æŠ½æ ·\")\n",
    "        \n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            train_texts, train_labels, test_size=val_size, random_state=seed,\n",
    "            stratify=use_stratify_val\n",
    "        )\n",
    "        \n",
    "        print(f\"æ•°æ®åˆ’åˆ†: è®­ç»ƒé›†={len(train_texts)}, éªŒè¯é›†={len(val_texts)}, æµ‹è¯•é›†={len(test_texts)}\")\n",
    "        _print_dist(\"è®­ç»ƒé›†\", train_labels)\n",
    "        _print_dist(\"éªŒè¯é›†\", val_labels)\n",
    "        _print_dist(\"æµ‹è¯•é›†\", test_labels)\n",
    "        \n",
    "        return train_texts, train_labels, val_texts, val_labels, test_texts, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f1ae6c-ce66-4cdc-b2f0-aa8dc3dcd24e",
   "metadata": {},
   "source": [
    "## VocabularyBuilder - è¯æ±‡è¡¨ç®¡ç†æ¨¡å—\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "- build_vocabulary(texts) - åŸºäºè®­ç»ƒæ–‡æœ¬æ„å»ºè¯æ±‡è¡¨ï¼š\n",
    "\n",
    "    1. ç»Ÿè®¡è¯é¢‘\n",
    "    \n",
    "    2. è¿‡æ»¤ä½é¢‘è¯\n",
    "    \n",
    "    3. æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼ˆPAD/UNK/BOS/EOSï¼‰\n",
    "    \n",
    "    4. è®¡ç®—è¯æ±‡è¡¨è¦†ç›–ç‡\n",
    "\n",
    "- text_to_sequence(text) - å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—åºåˆ—ï¼š\n",
    "\n",
    "    1. æ·»åŠ å¼€å§‹/ç»“æŸæ ‡è®°\n",
    "    \n",
    "    2. å¤„ç†æœªçŸ¥è¯ï¼ˆOOVï¼‰\n",
    "    \n",
    "    3. åºåˆ—å¡«å……/æˆªæ–­\n",
    "    \n",
    "    4. ç”Ÿæˆæ³¨æ„åŠ›æ©ç \n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "394556fd-0eb8-489a-9742-543c5ac9391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyBuilder:\n",
    "    \"\"\"è¯æ±‡è¡¨ç®¡ç†æ¨¡å—\"\"\"\n",
    "    \n",
    "    def __init__(self, max_vocab_size: int = 30000, min_freq: int = 2):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_freq = min_freq\n",
    "        self.special_tokens = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    def build_vocabulary(self, texts: List[str]) -> None:\n",
    "        \"\"\"æ„å»ºè¯æ±‡è¡¨\"\"\"\n",
    "        print(\"æ„å»ºè¯æ±‡è¡¨...\")\n",
    "        \n",
    "        # ç»Ÿè®¡è¯é¢‘\n",
    "        word_freq = Counter()\n",
    "        for text in texts:\n",
    "            tokens = text.split()\n",
    "            word_freq.update(tokens)\n",
    "        \n",
    "        # è¿‡æ»¤ä½é¢‘è¯\n",
    "        filtered_words = [(word, freq) for word, freq in word_freq.items() \n",
    "                         if freq >= self.min_freq]\n",
    "        \n",
    "        # æŒ‰é¢‘ç‡æ’åºå¹¶é€‰æ‹©å‰Nä¸ªè¯\n",
    "        sorted_words = sorted(filtered_words, key=lambda x: x[1], reverse=True)\n",
    "        selected_words = [word for word, freq in sorted_words[:self.max_vocab_size - len(self.special_tokens)]]\n",
    "        \n",
    "        # æ„å»ºè¯æ±‡è¡¨\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "        \n",
    "        # æ·»åŠ ç‰¹æ®Šæ ‡è®°\n",
    "        for idx, token in enumerate(self.special_tokens):\n",
    "            self.token_to_id[token] = idx\n",
    "            self.id_to_token[idx] = token\n",
    "        \n",
    "        # æ·»åŠ æ™®é€šè¯æ±‡\n",
    "        for idx, word in enumerate(selected_words, start=len(self.special_tokens)):\n",
    "            self.token_to_id[word] = idx\n",
    "            self.id_to_token[idx] = word\n",
    "        \n",
    "        self.vocab_size = len(self.token_to_id)\n",
    "        \n",
    "        # OOVåˆ†æ\n",
    "        total_tokens = sum(word_freq.values())\n",
    "        covered_tokens = sum(freq for word, freq in word_freq.items() \n",
    "                           if word in self.token_to_id)\n",
    "        coverage = covered_tokens / total_tokens * 100\n",
    "        \n",
    "        print(f\"è¯æ±‡è¡¨æ„å»ºå®Œæˆ: {self.vocab_size} ä¸ªtoken\")\n",
    "        print(f\"è¯æ±‡è¡¨è¦†ç›–ç‡: {coverage:.2f}%\")\n",
    "    \n",
    "    def text_to_sequence(self, text: str, max_length: int = 512, \n",
    "                        add_special_tokens: bool = True) -> Tuple[List[int], List[int], int]:\n",
    "        \"\"\"å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•´æ•°åºåˆ—\"\"\"\n",
    "        tokens = text.split()\n",
    "        sequence = []\n",
    "        \n",
    "        # æ·»åŠ å¼€å§‹æ ‡è®°\n",
    "        if add_special_tokens:\n",
    "            sequence.append(self.token_to_id['<BOS>'])\n",
    "        \n",
    "        # è½¬æ¢tokens\n",
    "        for token in tokens:\n",
    "            sequence.append(self.token_to_id.get(token, self.token_to_id['<UNK>']))\n",
    "        \n",
    "        # æ·»åŠ ç»“æŸæ ‡è®°\n",
    "        if add_special_tokens:\n",
    "            sequence.append(self.token_to_id['<EOS>'])\n",
    "        \n",
    "        original_length = len(sequence)\n",
    "        \n",
    "        # å¡«å……æˆ–æˆªæ–­\n",
    "        if len(sequence) < max_length:\n",
    "            sequence.extend([self.token_to_id['<PAD>']] * (max_length - len(sequence)))\n",
    "            attention_mask = [1] * original_length + [0] * (max_length - original_length)\n",
    "        else:\n",
    "            sequence = sequence[:max_length]\n",
    "            attention_mask = [1] * max_length\n",
    "        \n",
    "        return sequence, attention_mask, original_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf84183-a006-4f07-8181-60f5dc69d6a2",
   "metadata": {},
   "source": [
    "##  IMDBDataset - æ•°æ®é›†æ¥å£æ¨¡å—\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "\n",
    "- __init__() - åˆå§‹åŒ–æ•°æ®é›†ï¼ŒéªŒè¯æ•°æ®ä¸€è‡´æ€§\n",
    "\n",
    "- __len__() - è¿”å›æ•°æ®é›†å¤§å°\n",
    "\n",
    "- __getitem__() - è·å–å•ä¸ªæ ·æœ¬ï¼Œè¿”å›å­—å…¸æ ¼å¼\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "499dafae-f48c-42b2-b3b3-3c2fc5f96aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"IMDbæ•°æ®é›†ç±»\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences: List[List[int]], attention_masks: List[List[int]], \n",
    "                 labels: List[int], lengths: List[int]):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "        self.attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "        \n",
    "        # éªŒè¯æ•°æ®å½¢çŠ¶\n",
    "        assert len(self.sequences) == len(self.labels), \"åºåˆ—å’Œæ ‡ç­¾æ•°é‡ä¸åŒ¹é…\"\n",
    "        assert len(self.sequences) == len(self.attention_masks), \"åºåˆ—å’Œæ³¨æ„åŠ›æ©ç æ•°é‡ä¸åŒ¹é…\"\n",
    "        assert len(self.sequences) == len(self.lengths), \"åºåˆ—å’Œé•¿åº¦æ•°é‡ä¸åŒ¹é…\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.sequences[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx],\n",
    "            'lengths': self.lengths[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fdb0d3-a10b-4094-a182-162acc0aa8b5",
   "metadata": {},
   "source": [
    "## DataSaver - æ•°æ®ä¿å­˜æ¨¡å—\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "\n",
    "- save_processed_data() - ä¸»ä¿å­˜å‡½æ•°\n",
    "\n",
    "- _save_vocabulary() - ä¿å­˜è¯æ±‡è¡¨ä¸ºJSON\n",
    "\n",
    "- _save_config() - ä¿å­˜é¢„å¤„ç†é…ç½®\n",
    "\n",
    "- _save_tensor_data() - ä¿å­˜Tensoræ•°æ®ä¸º.ptæ–‡ä»¶\n",
    "\n",
    "- _save_metadata() - ä¿å­˜ç»Ÿè®¡å…ƒæ•°æ®\n",
    "\n",
    "- _verify_files() - éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5da1e614-5d4d-4f45-8a7e-cce4aa480016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSaver:\n",
    "    \"\"\"æ•°æ®ä¿å­˜æ¨¡å—\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_processed_data(datasets: Dict, vocab, metadata: Dict, output_dir: str, label_schema: str):\n",
    "        \"\"\"ä¿å­˜æ‰€æœ‰å¤„ç†åçš„æ•°æ®\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        schema_config = LabelProcessor.get_label_schema_config(label_schema)\n",
    "        print(f\"å¼€å§‹ä¿å­˜{schema_config['name']}æ•°æ®åˆ°: {output_path}\")\n",
    "        \n",
    "        # 1. ä¿å­˜è¯æ±‡è¡¨\n",
    "        DataSaver._save_vocabulary(vocab, output_path)\n",
    "        \n",
    "        # 2. ä¿å­˜é…ç½®\n",
    "        DataSaver._save_config(output_path, label_schema, schema_config)\n",
    "        \n",
    "        # 3. ä¿å­˜ tensor æ•°æ®\n",
    "        DataSaver._save_tensor_data(datasets, output_path)\n",
    "        \n",
    "        # 4. ä¿å­˜å…ƒæ•°æ®\n",
    "        DataSaver._save_metadata(metadata, output_path)\n",
    "        \n",
    "        # 5. éªŒè¯æ–‡ä»¶\n",
    "        DataSaver._verify_files(output_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _save_vocabulary(vocab, output_path):\n",
    "        \"\"\"ä¿å­˜è¯æ±‡è¡¨\"\"\"\n",
    "        vocab_path = output_path / 'vocabulary.json'\n",
    "        with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(vocab, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"âœ… è¯æ±‡è¡¨ä¿å­˜åˆ°: {vocab_path}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _save_config(output_path, label_schema, schema_config):\n",
    "        \"\"\"ä¿å­˜é…ç½®\"\"\"\n",
    "        config_path = output_path / 'preprocessing_config.json'\n",
    "        config = {\n",
    "            'label_schema': label_schema,\n",
    "            'schema_name': schema_config['name'],\n",
    "            'num_classes': schema_config['num_classes'],\n",
    "            'label_range': schema_config['label_range'],\n",
    "            'max_vocab_size': 30000,\n",
    "            'max_length': 512,\n",
    "            'min_freq': 2,\n",
    "            'seed': 42,\n",
    "            'saved_time': str(pd.Timestamp.now())\n",
    "        }\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        print(f\"âœ… é…ç½®ä¿å­˜åˆ°: {config_path}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _save_tensor_data(datasets, output_path):\n",
    "        \"\"\"ä¿å­˜ tensor æ•°æ®\"\"\"\n",
    "        data_path = output_path / 'all_data.pt'\n",
    "        \n",
    "        save_data = {\n",
    "            'train_sequences': datasets['train'].sequences,\n",
    "            'train_masks': datasets['train'].attention_masks,\n",
    "            'train_labels': datasets['train'].labels,\n",
    "            'train_lengths': datasets['train'].lengths,\n",
    "            \n",
    "            'val_sequences': datasets['val'].sequences,\n",
    "            'val_masks': datasets['val'].attention_masks,\n",
    "            'val_labels': datasets['val'].labels,\n",
    "            'val_lengths': datasets['val'].lengths,\n",
    "            \n",
    "            'test_sequences': datasets['test'].sequences,\n",
    "            'test_masks': datasets['test'].attention_masks,\n",
    "            'test_labels': datasets['test'].labels,\n",
    "            'test_lengths': datasets['test'].lengths,\n",
    "        }\n",
    "        \n",
    "        torch.save(save_data, data_path)\n",
    "        file_size = data_path.stat().st_size / 1024 / 1024\n",
    "        print(f\"âœ… Tensoræ•°æ®ä¿å­˜åˆ°: {data_path} (å¤§å°: {file_size:.2f} MB)\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _save_metadata(metadata, output_path):\n",
    "        \"\"\"ä¿å­˜å…ƒæ•°æ®\"\"\"\n",
    "        metadata_path = output_path / 'preprocessing_metadata.json'\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str, ensure_ascii=False)\n",
    "        print(f\"âœ… å…ƒæ•°æ®ä¿å­˜åˆ°: {metadata_path}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _verify_files(output_path):\n",
    "        \"\"\"éªŒè¯æ–‡ä»¶æ˜¯å¦åˆ›å»ºæˆåŠŸ\"\"\"\n",
    "        print(\"\\nğŸ“ éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶:\")\n",
    "        for file in output_path.iterdir():\n",
    "            size_kb = file.stat().st_size / 1024\n",
    "            print(f\"  {file.name} ({size_kb:.1f} KB)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3593ed-7193-427e-9df1-684fe97d82d3",
   "metadata": {},
   "source": [
    "## UnifiedDataProcessingPipeline - æµç¨‹åè°ƒæ¨¡å—\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "\n",
    "- run_complete_pipeline() - æ‰§è¡Œå®Œæ•´çš„å¤šæ¨¡å¼æ•°æ®å¤„ç†æµç¨‹\n",
    "\n",
    "- process_single_schema() - å¤„ç†å•ä¸ªæ ‡ç­¾æ¨¡å¼çš„æ•°æ®\n",
    "\n",
    "- _ensure_data_ready() - ç¡®ä¿æ•°æ®å‡†å¤‡å°±ç»ªï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰\n",
    "\n",
    "- _vectorize_texts() - æ‰¹é‡å‘é‡åŒ–æ–‡æœ¬\n",
    "\n",
    "- _create_datasets() - åˆ›å»ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®é›†\n",
    "\n",
    "- _check_data_size() - æ£€æŸ¥æ•°æ®è§„æ¨¡\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5b34c6b-730e-4553-8ba1-0b2f9b4e93c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedDataProcessingPipeline:\n",
    "    \"\"\"ç»Ÿä¸€çš„æ•°æ®å¤„ç†æµæ°´çº¿\"\"\"\n",
    "    \n",
    "    def __init__(self, max_vocab_size: int = 30000, max_length: int = 512, \n",
    "                 min_freq: int = 2, seed: int = 42):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.min_freq = min_freq\n",
    "        self.seed = seed\n",
    "        self.data_dir = None  # æ·»åŠ æ•°æ®ç›®å½•ç¼“å­˜\n",
    "    \n",
    "    def _ensure_data_ready(self) -> bool:\n",
    "        \"\"\"ç¡®ä¿æ•°æ®å‡†å¤‡å°±ç»ªï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰\"\"\"\n",
    "        if self.data_dir is not None:\n",
    "            return True\n",
    "            \n",
    "        try:\n",
    "            print(\"ğŸ” æ£€æŸ¥æ•°æ®æº...\")\n",
    "            self.data_dir = DataDownloader.setup_data()\n",
    "            print(\"âœ… æ•°æ®æºå‡†å¤‡å®Œæˆ\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ•°æ®æºå‡†å¤‡å¤±è´¥: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def process_single_schema(self, label_schema: str) -> bool:\n",
    "        \"\"\"å¤„ç†å•ä¸ªæ ‡ç­¾æ¨¡å¼çš„æ•°æ®\"\"\"\n",
    "        try:\n",
    "            # 0. ç¡®ä¿æ•°æ®å°±ç»ª\n",
    "            if not self._ensure_data_ready():\n",
    "                return False\n",
    "                \n",
    "            schema_config = LabelProcessor.get_label_schema_config(label_schema)\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"=== å¤„ç† {schema_config['name']} æ•°æ® ===\")\n",
    "            print(f\"æ ‡ç­¾èŒƒå›´: {schema_config['label_range']}\")\n",
    "            print(f\"ç±»åˆ«æ•°é‡: {schema_config['num_classes']}\")\n",
    "            print(f\"è¾“å‡ºç›®å½•: {schema_config['output_dir']}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            # 1. åˆå§‹åŒ–ç»„ä»¶\n",
    "            vocab_builder = VocabularyBuilder(self.max_vocab_size, self.min_freq)\n",
    "            text_processor = TextProcessor()\n",
    "            \n",
    "            # 2. åŠ è½½åŸå§‹æ•°æ®\n",
    "            print(\"\\nğŸ“¥ åŠ è½½åŸå§‹æ•°æ®...\")\n",
    "            train_texts, train_labels, test_texts, test_labels = DataLoaderManager.load_raw_data(\n",
    "                self.data_dir, label_schema\n",
    "            )\n",
    "            \n",
    "            # 3. åˆå¹¶å¹¶åˆ’åˆ†æ•°æ®\n",
    "            print(\"ğŸ“Š æ•°æ®åˆ’åˆ†...\")\n",
    "            all_texts = train_texts + test_texts\n",
    "            all_labels = train_labels + test_labels\n",
    "            \n",
    "            train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = \\\n",
    "                DataSplitter.split_data_with_seeds(all_texts, all_labels, self.seed)\n",
    "            \n",
    "            # 4. æ–‡æœ¬æ¸…æ´—\n",
    "            print(\"ğŸ§¹ æ¸…æ´—æ–‡æœ¬æ•°æ®...\")\n",
    "            train_texts_clean = text_processor.process_in_batches(\n",
    "                train_texts, text_processor.robust_text_cleaning, process_name=\"æ–‡æœ¬æ¸…æ´—\"\n",
    "            )\n",
    "            val_texts_clean = text_processor.process_in_batches(\n",
    "                val_texts, text_processor.robust_text_cleaning, process_name=\"æ–‡æœ¬æ¸…æ´—\"\n",
    "            )\n",
    "            test_texts_clean = text_processor.process_in_batches(\n",
    "                test_texts, text_processor.robust_text_cleaning, process_name=\"æ–‡æœ¬æ¸…æ´—\"\n",
    "            )\n",
    "            \n",
    "            # 5. æ„å»ºè¯æ±‡è¡¨\n",
    "            print(\"ğŸ“š æ„å»ºè¯æ±‡è¡¨...\")\n",
    "            vocab_builder.build_vocabulary(train_texts_clean)\n",
    "            \n",
    "            # 6. æ–‡æœ¬å‘é‡åŒ–\n",
    "            print(\"ğŸ”¢ å‘é‡åŒ–æ•°æ®...\")\n",
    "            train_sequences, train_masks, train_lengths = self._vectorize_texts(\n",
    "                train_texts_clean, vocab_builder, self.max_length\n",
    "            )\n",
    "            val_sequences, val_masks, val_lengths = self._vectorize_texts(\n",
    "                val_texts_clean, vocab_builder, self.max_length\n",
    "            )\n",
    "            test_sequences, test_masks, test_lengths = self._vectorize_texts(\n",
    "                test_texts_clean, vocab_builder, self.max_length\n",
    "            )\n",
    "            \n",
    "            # 7. åˆ›å»ºæ•°æ®é›†\n",
    "            print(\"ğŸ—‚ï¸ åˆ›å»ºæ•°æ®é›†...\")\n",
    "            datasets = self._create_datasets(\n",
    "                train_sequences, train_masks, train_labels, train_lengths,\n",
    "                val_sequences, val_masks, val_labels, val_lengths,\n",
    "                test_sequences, test_masks, test_labels, test_lengths\n",
    "            )\n",
    "            \n",
    "            # 8. å‡†å¤‡ä¿å­˜æ•°æ®\n",
    "            vocab_info = {\n",
    "                'token_to_id': vocab_builder.token_to_id,\n",
    "                'id_to_token': vocab_builder.id_to_token,\n",
    "                'vocab_size': vocab_builder.vocab_size,\n",
    "                'special_tokens': vocab_builder.special_tokens\n",
    "            }\n",
    "            \n",
    "            metadata = {\n",
    "                'label_schema': label_schema,\n",
    "                'schema_config': schema_config,\n",
    "                'sequence_length_analysis': {\n",
    "                    'mean_length': np.mean(train_lengths),\n",
    "                    'max_length': np.max(train_lengths),\n",
    "                    'min_length': np.min(train_lengths),\n",
    "                    'total_samples': len(train_sequences)\n",
    "                },\n",
    "                'class_distribution': {\n",
    "                    'train': dict(Counter(train_labels)),\n",
    "                    'val': dict(Counter(val_labels)),\n",
    "                    'test': dict(Counter(test_labels))\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # 9. ä¿å­˜æ•°æ®\n",
    "            DataSaver.save_processed_data(\n",
    "                datasets, vocab_info, metadata, \n",
    "                f\"./{schema_config['output_dir']}\", \n",
    "                label_schema\n",
    "            )\n",
    "            \n",
    "            print(f\"ğŸ‰ {schema_config['name']}æ•°æ®å¤„ç†å®Œæˆï¼\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {label_schema}å¤„ç†å¤±è´¥: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def run_complete_pipeline(self) -> bool:\n",
    "        \"\"\"è¿è¡Œå®Œæ•´çš„å¤šæ¨¡å¼æ•°æ®å¤„ç†æµæ°´çº¿\"\"\"\n",
    "        print(\"ğŸš€ å¼€å§‹ç»Ÿä¸€æ•°æ®å¤„ç†æµæ°´çº¿\")\n",
    "        \n",
    "        # å…ˆç»Ÿä¸€æ£€æŸ¥æ•°æ®æº\n",
    "        print(\"ğŸ“¦ å‡†å¤‡æ•°æ®æº...\")\n",
    "        if not self._ensure_data_ready():\n",
    "            print(\"âŒ æ•°æ®æºå‡†å¤‡å¤±è´¥ï¼Œç»ˆæ­¢å¤„ç†\")\n",
    "            return False\n",
    "            \n",
    "        success_count = 0\n",
    "        schemas = [\"binary\", \"multiclass\"]\n",
    "        \n",
    "        for schema in schemas:\n",
    "            success = self.process_single_schema(schema)\n",
    "            if success:\n",
    "                success_count += 1\n",
    "                self._check_data_size(schema)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ¯ æ•°æ®å¤„ç†å®Œæˆæ€»ç»“:\")\n",
    "        print(f\"æˆåŠŸå¤„ç†: {success_count}/{len(schemas)} ç§æ•°æ®æ¨¡å¼\")\n",
    "        \n",
    "        if success_count == len(schemas):\n",
    "            print(\"âœ… æ‰€æœ‰æ•°æ®æ¨¡å¼å¤„ç†å®Œæˆï¼\")\n",
    "            print(\"ğŸ“ ç”Ÿæˆçš„æ•°æ®ç›®å½•:\")\n",
    "            for schema in schemas:\n",
    "                config = LabelProcessor.get_label_schema_config(schema)\n",
    "                print(f\"  - {config['output_dir']}: {config['name']}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ éƒ¨åˆ†æ•°æ®å¤„ç†å¤±è´¥\")\n",
    "            return False\n",
    "    \n",
    "    def _vectorize_texts(self, texts: List[str], vocab_builder: VocabularyBuilder, max_length: int):\n",
    "        \"\"\"å‘é‡åŒ–æ–‡æœ¬æ•°æ®\"\"\"\n",
    "        sequences, masks, lengths = [], [], []\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            seq, mask, length = vocab_builder.text_to_sequence(text, max_length)\n",
    "            sequences.append(seq)\n",
    "            masks.append(mask)\n",
    "            lengths.append(length)\n",
    "            \n",
    "            if i % 5000 == 0 and i > 0:\n",
    "                print(f\"  å·²å‘é‡åŒ– {i}/{len(texts)} ä¸ªæ ·æœ¬\")\n",
    "        \n",
    "        return sequences, masks, lengths\n",
    "    \n",
    "    def _create_datasets(self, train_sequences, train_masks, train_labels, train_lengths,\n",
    "                        val_sequences, val_masks, val_labels, val_lengths,\n",
    "                        test_sequences, test_masks, test_labels, test_lengths):\n",
    "        \"\"\"åˆ›å»ºæ•°æ®é›†\"\"\"\n",
    "        train_dataset = IMDBDataset(train_sequences, train_masks, train_labels, train_lengths)\n",
    "        val_dataset = IMDBDataset(val_sequences, val_masks, val_labels, val_lengths)\n",
    "        test_dataset = IMDBDataset(test_sequences, test_masks, test_labels, test_lengths)\n",
    "        \n",
    "        return {\n",
    "            'train': train_dataset,\n",
    "            'val': val_dataset,\n",
    "            'test': test_dataset\n",
    "        }\n",
    "    \n",
    "    def _check_data_size(self, label_schema: str):\n",
    "        \"\"\"æ£€æŸ¥æ•°æ®è§„æ¨¡\"\"\"\n",
    "        config = LabelProcessor.get_label_schema_config(label_schema)\n",
    "        data_path = f\"./{config['output_dir']}/all_data.pt\"\n",
    "        \n",
    "        try:\n",
    "            data = torch.load(data_path)\n",
    "            \n",
    "            print(f\"\\n=== {config['name']}æ•°æ®è§„æ¨¡æ£€æŸ¥ ===\")\n",
    "            print(f\"è®­ç»ƒåºåˆ—: {data['train_sequences'].shape}\")\n",
    "            print(f\"è®­ç»ƒæ ‡ç­¾: {data['train_labels'].shape}\")\n",
    "            print(f\"éªŒè¯åºåˆ—: {data['val_sequences'].shape}\")\n",
    "            print(f\"æµ‹è¯•åºåˆ—: {data['test_sequences'].shape}\")\n",
    "            \n",
    "            total_samples = (data['train_sequences'].shape[0] + \n",
    "                            data['val_sequences'].shape[0] + \n",
    "                            data['test_sequences'].shape[0])\n",
    "            \n",
    "            print(f\"æ€»æ ·æœ¬æ•°: {total_samples}\")\n",
    "            \n",
    "            if total_samples >= 40000:\n",
    "                print(\"âœ… å¤„ç†äº†å®Œæ•´æ•°æ®é›†\")\n",
    "            else:\n",
    "                print(\"âŒ æ•°æ®é‡å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥å¤„ç†è¿‡ç¨‹\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"æ£€æŸ¥å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d222375-a31b-43ad-bd47-6d8d0d30f049",
   "metadata": {},
   "source": [
    "## å·¥å…·å‡½æ•°\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "\n",
    "æ£€æŸ¥å¤„ç†åçš„æ•°æ®è§„æ¨¡å’Œç»Ÿè®¡ä¿¡æ¯\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdc50dad-baf5-4d09-914d-24a28ba3b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_size(data_path: str = './processed_data/all_data.pt'):\n",
    "    \"\"\"æ£€æŸ¥å¤„ç†äº†å¤šå°‘æ•°æ®\"\"\"\n",
    "    try:\n",
    "        data = torch.load(data_path)\n",
    "        \n",
    "        print(\"\\n=== æ•°æ®è§„æ¨¡æ£€æŸ¥ ===\")\n",
    "        print(f\"è®­ç»ƒåºåˆ—: {data['train_sequences'].shape}\")\n",
    "        print(f\"è®­ç»ƒæ ‡ç­¾: {data['train_labels'].shape}\")\n",
    "        print(f\"éªŒè¯åºåˆ—: {data['val_sequences'].shape}\") \n",
    "        print(f\"æµ‹è¯•åºåˆ—: {data['test_sequences'].shape}\")\n",
    "        \n",
    "        total_samples = (data['train_sequences'].shape[0] + \n",
    "                        data['val_sequences'].shape[0] + \n",
    "                        data['test_sequences'].shape[0])\n",
    "        \n",
    "        print(f\"\\næ€»æ ·æœ¬æ•°: {total_samples}\")\n",
    "        \n",
    "        if total_samples >= 40000:\n",
    "            print(\"âœ… å¤„ç†äº†å®Œæ•´æ•°æ®é›†\")\n",
    "        elif total_samples >= 20000:\n",
    "            print(\"âš ï¸  å¤„ç†äº†éƒ¨åˆ†æ•°æ®é›†\")  \n",
    "        else:\n",
    "            print(\"âŒ æ•°æ®é‡å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥å¤„ç†è¿‡ç¨‹\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"æ£€æŸ¥å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820dfb2c-faa7-4c25-be0e-2fa021e136bf",
   "metadata": {},
   "source": [
    "## ä¸»ç¨‹åºå…¥å£\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "126b164e-e36b-4132-98be-3e3646e53142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹ç»Ÿä¸€æ•°æ®å¤„ç†æµæ°´çº¿\n",
      "ğŸ“¦ å‡†å¤‡æ•°æ®æº...\n",
      "ğŸ” æ£€æŸ¥æ•°æ®æº...\n",
      "æ•°æ®ç›®å½•ä¸å®Œæ•´ï¼Œé‡æ–°ä¸‹è½½...\n",
      "ä¸‹è½½IMDbæ•°æ®é›†...\n",
      "ä¸‹è½½å®Œæˆï¼Œè§£å‹ä¸­...\n",
      "IMDbæ•°æ®é›†å‡†å¤‡å®Œæˆ\n",
      "âœ… æ•°æ®æºå‡†å¤‡å®Œæˆ\n",
      "\n",
      "==================================================\n",
      "=== å¤„ç† äºŒåˆ†ç±» æ•°æ® ===\n",
      "æ ‡ç­¾èŒƒå›´: 0-1 (è´Ÿé¢/æ­£é¢)\n",
      "ç±»åˆ«æ•°é‡: 2\n",
      "è¾“å‡ºç›®å½•: processed_data_binary\n",
      "==================================================\n",
      "\n",
      "ğŸ“¥ åŠ è½½åŸå§‹æ•°æ®...\n",
      "åŠ è½½IMDbæ•°æ®é›† - äºŒåˆ†ç±»...\n",
      "train æ ‡ç­¾åˆ†å¸ƒ: {0: 12500, 1: 12500}\n",
      "test æ ‡ç­¾åˆ†å¸ƒ: {0: 12500, 1: 12500}\n",
      "åŠ è½½äº† 25000 è®­ç»ƒæ ·æœ¬, 25000 æµ‹è¯•æ ·æœ¬\n",
      "ğŸ“Š æ•°æ®åˆ’åˆ†...\n",
      "æ‰§è¡Œå¯å¤ç°çš„æ•°æ®åˆ’åˆ†ï¼ˆå¸¦ç±»åˆ«æ£€æŸ¥ï¼‰...\n",
      "æ•°æ®åˆ’åˆ†: è®­ç»ƒé›†=32000, éªŒè¯é›†=8000, æµ‹è¯•é›†=10000\n",
      "è®­ç»ƒé›† ç±»åˆ«åˆ†å¸ƒ: {0: 16000, 1: 16000}\n",
      "éªŒè¯é›† ç±»åˆ«åˆ†å¸ƒ: {0: 4000, 1: 4000}\n",
      "æµ‹è¯•é›† ç±»åˆ«åˆ†å¸ƒ: {0: 5000, 1: 5000}\n",
      "ğŸ§¹ æ¸…æ´—æ–‡æœ¬æ•°æ®...\n",
      "  æ–‡æœ¬æ¸…æ´— 5000/32000 ä¸ªæ ·æœ¬\n",
      "  æ–‡æœ¬æ¸…æ´— 10000/32000 ä¸ªæ ·æœ¬\n",
      "  æ–‡æœ¬æ¸…æ´— 15000/32000 ä¸ªæ ·æœ¬\n",
      "  æ–‡æœ¬æ¸…æ´— 20000/32000 ä¸ªæ ·æœ¬\n",
      "  æ–‡æœ¬æ¸…æ´— 25000/32000 ä¸ªæ ·æœ¬\n",
      "  æ–‡æœ¬æ¸…æ´— 30000/32000 ä¸ªæ ·æœ¬\n",
      "âœ… æ–‡æœ¬æ¸…æ´—å®Œæˆ\n",
      "  æ–‡æœ¬æ¸…æ´— 5000/8000 ä¸ªæ ·æœ¬\n",
      "âœ… æ–‡æœ¬æ¸…æ´—å®Œæˆ\n",
      "  æ–‡æœ¬æ¸…æ´— 5000/10000 ä¸ªæ ·æœ¬\n",
      "âœ… æ–‡æœ¬æ¸…æ´—å®Œæˆ\n",
      "ğŸ“š æ„å»ºè¯æ±‡è¡¨...\n",
      "æ„å»ºè¯æ±‡è¡¨...\n",
      "è¯æ±‡è¡¨æ„å»ºå®Œæˆ: 30000 ä¸ªtoken\n",
      "è¯æ±‡è¡¨è¦†ç›–ç‡: 97.52%\n",
      "ğŸ”¢ å‘é‡åŒ–æ•°æ®...\n",
      "  å·²å‘é‡åŒ– 5000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 10000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 15000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 20000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 25000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 30000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 5000/8000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 5000/10000 ä¸ªæ ·æœ¬\n",
      "ğŸ—‚ï¸ åˆ›å»ºæ•°æ®é›†...\n",
      "å¼€å§‹ä¿å­˜äºŒåˆ†ç±»æ•°æ®åˆ°: processed_data_binary\n",
      "âœ… è¯æ±‡è¡¨ä¿å­˜åˆ°: processed_data_binary\\vocabulary.json\n",
      "âœ… é…ç½®ä¿å­˜åˆ°: processed_data_binary\\preprocessing_config.json\n",
      "âœ… Tensoræ•°æ®ä¿å­˜åˆ°: processed_data_binary\\all_data.pt (å¤§å°: 391.39 MB)\n",
      "âœ… å…ƒæ•°æ®ä¿å­˜åˆ°: processed_data_binary\\preprocessing_metadata.json\n",
      "\n",
      "ğŸ“ éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶:\n",
      "  all_data.pt (400785.3 KB)\n",
      "  preprocessing_config.json (0.3 KB)\n",
      "  preprocessing_metadata.json (0.5 KB)\n",
      "  vocabulary.json (1391.2 KB)\n",
      "ğŸ‰ äºŒåˆ†ç±»æ•°æ®å¤„ç†å®Œæˆï¼\n",
      "\n",
      "=== äºŒåˆ†ç±»æ•°æ®è§„æ¨¡æ£€æŸ¥ ===\n",
      "è®­ç»ƒåºåˆ—: torch.Size([32000, 512])\n",
      "è®­ç»ƒæ ‡ç­¾: torch.Size([32000])\n",
      "éªŒè¯åºåˆ—: torch.Size([8000, 512])\n",
      "æµ‹è¯•åºåˆ—: torch.Size([10000, 512])\n",
      "æ€»æ ·æœ¬æ•°: 50000\n",
      "âœ… å¤„ç†äº†å®Œæ•´æ•°æ®é›†\n",
      "\n",
      "==================================================\n",
      "=== å¤„ç† å¤šåˆ†ç±»(1-10æ˜Ÿ) æ•°æ® ===\n",
      "æ ‡ç­¾èŒƒå›´: 1-10æ˜Ÿ (ç¼ºå¤±5-6æ˜Ÿ)\n",
      "ç±»åˆ«æ•°é‡: 8\n",
      "è¾“å‡ºç›®å½•: processed_data_multiclass\n",
      "==================================================\n",
      "\n",
      "ğŸ“¥ åŠ è½½åŸå§‹æ•°æ®...\n",
      "åŠ è½½IMDbæ•°æ®é›† - å¤šåˆ†ç±»(1-10æ˜Ÿ)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\f5428489\\AppData\\Local\\Temp\\ipykernel_4412\\2919244761.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(data_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train æ ‡ç­¾åˆ†å¸ƒ: {1: 5100, 2: 2284, 3: 2420, 4: 2696, 7: 2496, 8: 3009, 9: 2263, 10: 4732}\n",
      "test æ ‡ç­¾åˆ†å¸ƒ: {1: 5022, 2: 2302, 3: 2541, 4: 2635, 7: 2307, 8: 2850, 9: 2344, 10: 4999}\n",
      "åŠ è½½äº† 25000 è®­ç»ƒæ ·æœ¬, 25000 æµ‹è¯•æ ·æœ¬\n",
      "ğŸ“Š æ•°æ®åˆ’åˆ†...\n",
      "æ‰§è¡Œå¯å¤ç°çš„æ•°æ®åˆ’åˆ†ï¼ˆå¸¦ç±»åˆ«æ£€æŸ¥ï¼‰...\n",
      "æ•°æ®åˆ’åˆ†: è®­ç»ƒé›†=32000, éªŒè¯é›†=8000, æµ‹è¯•é›†=10000\n",
      "è®­ç»ƒé›† ç±»åˆ«åˆ†å¸ƒ: {1: 6478, 2: 2935, 3: 3175, 4: 3412, 7: 3074, 8: 3750, 9: 2948, 10: 6228}\n",
      "éªŒè¯é›† ç±»åˆ«åˆ†å¸ƒ: {1: 1620, 2: 734, 3: 794, 4: 853, 7: 768, 8: 937, 9: 737, 10: 1557}\n",
      "æµ‹è¯•é›† ç±»åˆ«åˆ†å¸ƒ: {1: 2024, 2: 917, 3: 992, 4: 1066, 7: 961, 8: 1172, 9: 922, 10: 1946}\n",
      "ğŸ§¹ æ¸…æ´—æ–‡æœ¬æ•°æ®...\n",
      "  æ–‡æœ¬æ¸…æ´— 5000/32000 ä¸ªæ ·æœ¬\n",
      "  æ–‡æœ¬æ¸…æ´— 10000/32000 ä¸ªæ ·æœ¬\n",
      "  æ–‡æœ¬æ¸…æ´— 15000/32000 ä¸ªæ ·æœ¬\n",
      "  æ–‡æœ¬æ¸…æ´— 20000/32000 ä¸ªæ ·æœ¬\n",
      "  æ–‡æœ¬æ¸…æ´— 25000/32000 ä¸ªæ ·æœ¬\n",
      "  æ–‡æœ¬æ¸…æ´— 30000/32000 ä¸ªæ ·æœ¬\n",
      "âœ… æ–‡æœ¬æ¸…æ´—å®Œæˆ\n",
      "  æ–‡æœ¬æ¸…æ´— 5000/8000 ä¸ªæ ·æœ¬\n",
      "âœ… æ–‡æœ¬æ¸…æ´—å®Œæˆ\n",
      "  æ–‡æœ¬æ¸…æ´— 5000/10000 ä¸ªæ ·æœ¬\n",
      "âœ… æ–‡æœ¬æ¸…æ´—å®Œæˆ\n",
      "ğŸ“š æ„å»ºè¯æ±‡è¡¨...\n",
      "æ„å»ºè¯æ±‡è¡¨...\n",
      "è¯æ±‡è¡¨æ„å»ºå®Œæˆ: 30000 ä¸ªtoken\n",
      "è¯æ±‡è¡¨è¦†ç›–ç‡: 97.51%\n",
      "ğŸ”¢ å‘é‡åŒ–æ•°æ®...\n",
      "  å·²å‘é‡åŒ– 5000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 10000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 15000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 20000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 25000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 30000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 5000/8000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 5000/10000 ä¸ªæ ·æœ¬\n",
      "ğŸ—‚ï¸ åˆ›å»ºæ•°æ®é›†...\n",
      "å¼€å§‹ä¿å­˜å¤šåˆ†ç±»(1-10æ˜Ÿ)æ•°æ®åˆ°: processed_data_multiclass\n",
      "âœ… è¯æ±‡è¡¨ä¿å­˜åˆ°: processed_data_multiclass\\vocabulary.json\n",
      "âœ… é…ç½®ä¿å­˜åˆ°: processed_data_multiclass\\preprocessing_config.json\n",
      "âœ… Tensoræ•°æ®ä¿å­˜åˆ°: processed_data_multiclass\\all_data.pt (å¤§å°: 391.39 MB)\n",
      "âœ… å…ƒæ•°æ®ä¿å­˜åˆ°: processed_data_multiclass\\preprocessing_metadata.json\n",
      "\n",
      "ğŸ“ éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶:\n",
      "  all_data.pt (400785.3 KB)\n",
      "  preprocessing_config.json (0.3 KB)\n",
      "  preprocessing_metadata.json (0.9 KB)\n",
      "  vocabulary.json (1391.0 KB)\n",
      "ğŸ‰ å¤šåˆ†ç±»(1-10æ˜Ÿ)æ•°æ®å¤„ç†å®Œæˆï¼\n",
      "\n",
      "=== å¤šåˆ†ç±»(1-10æ˜Ÿ)æ•°æ®è§„æ¨¡æ£€æŸ¥ ===\n",
      "è®­ç»ƒåºåˆ—: torch.Size([32000, 512])\n",
      "è®­ç»ƒæ ‡ç­¾: torch.Size([32000])\n",
      "éªŒè¯åºåˆ—: torch.Size([8000, 512])\n",
      "æµ‹è¯•åºåˆ—: torch.Size([10000, 512])\n",
      "æ€»æ ·æœ¬æ•°: 50000\n",
      "âœ… å¤„ç†äº†å®Œæ•´æ•°æ®é›†\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ æ•°æ®å¤„ç†å®Œæˆæ€»ç»“:\n",
      "æˆåŠŸå¤„ç†: 2/2 ç§æ•°æ®æ¨¡å¼\n",
      "âœ… æ‰€æœ‰æ•°æ®æ¨¡å¼å¤„ç†å®Œæˆï¼\n",
      "ğŸ“ ç”Ÿæˆçš„æ•°æ®ç›®å½•:\n",
      "  - processed_data_binary: äºŒåˆ†ç±»\n",
      "  - processed_data_multiclass: å¤šåˆ†ç±»(1-10æ˜Ÿ)\n",
      "\n",
      "=== æ•°æ®è§„æ¨¡æ£€æŸ¥ ===\n",
      "è®­ç»ƒåºåˆ—: torch.Size([32000, 512])\n",
      "è®­ç»ƒæ ‡ç­¾: torch.Size([32000])\n",
      "éªŒè¯åºåˆ—: torch.Size([8000, 512])\n",
      "æµ‹è¯•åºåˆ—: torch.Size([10000, 512])\n",
      "\n",
      "æ€»æ ·æœ¬æ•°: 50000\n",
      "âœ… å¤„ç†äº†å®Œæ•´æ•°æ®é›†\n",
      "\n",
      "=== æ•°æ®è§„æ¨¡æ£€æŸ¥ ===\n",
      "è®­ç»ƒåºåˆ—: torch.Size([32000, 512])\n",
      "è®­ç»ƒæ ‡ç­¾: torch.Size([32000])\n",
      "éªŒè¯åºåˆ—: torch.Size([8000, 512])\n",
      "æµ‹è¯•åºåˆ—: torch.Size([10000, 512])\n",
      "\n",
      "æ€»æ ·æœ¬æ•°: 50000\n",
      "âœ… å¤„ç†äº†å®Œæ•´æ•°æ®é›†\n",
      "\n",
      "ğŸ‰ æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆï¼ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒä¸åŒä»»åŠ¡çš„æ¨¡å‹äº†ï¼\n",
      "   - äºŒåˆ†ç±»æƒ…æ„Ÿåˆ†æ: ä½¿ç”¨ processed_data_binary/\n",
      "   - å¤šåˆ†ç±»æ˜Ÿçº§é¢„æµ‹: ä½¿ç”¨ processed_data_multiclass/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\f5428489\\AppData\\Local\\Temp\\ipykernel_4412\\2919244761.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(data_path)\n",
      "C:\\Users\\f5428489\\AppData\\Local\\Temp\\ipykernel_4412\\3281031234.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(data_path)\n"
     ]
    }
   ],
   "source": [
    "# ä¸»ç¨‹åºå…¥å£\n",
    "if __name__ == \"__main__\":\n",
    "    # åˆ›å»ºç»Ÿä¸€æ•°æ®å¤„ç†æµæ°´çº¿\n",
    "    pipeline = UnifiedDataProcessingPipeline(\n",
    "        max_vocab_size=30000,\n",
    "        max_length=512, \n",
    "        min_freq=2,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # è¿è¡Œå®Œæ•´çš„å¤šæ¨¡å¼å¤„ç†\n",
    "    success = pipeline.run_complete_pipeline()\n",
    "    \n",
    "    if success:\n",
    "        check_data_size('./processed_data_binary/all_data.pt')\n",
    "        check_data_size('./processed_data_multiclass/all_data.pt')\n",
    "        print(\"\\nğŸ‰ æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆï¼ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒä¸åŒä»»åŠ¡çš„æ¨¡å‹äº†ï¼\")\n",
    "        print(\"   - äºŒåˆ†ç±»æƒ…æ„Ÿåˆ†æ: ä½¿ç”¨ processed_data_binary/\")\n",
    "        print(\"   - å¤šåˆ†ç±»æ˜Ÿçº§é¢„æµ‹: ä½¿ç”¨ processed_data_multiclass/\")\n",
    "    else:\n",
    "        print(\"\\nâŒ æ•°æ®å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a896b6-6f0a-4c9a-a378-0f5277237f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
