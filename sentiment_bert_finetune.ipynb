{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20a31e64-a1b7-4d55-9f68-53722b6804ac",
   "metadata": {},
   "source": [
    "## Please Use FSC8 lab outside computers\n",
    "pip3 install transformers\n",
    "pip uninstall torch torchvision torchaudio\r",
    "pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu129\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66ab645a-bfbc-48ca-bbb4-ee0611a6f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Declaration\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.optim import AdamW \n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time, psutil\n",
    "\n",
    "\"\"\"\n",
    "system settings\n",
    "\"\"\"\n",
    "def set_overall_seed(seed=16):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_overall_seed(16)\n",
    "\n",
    "\"\"\"\n",
    "Define a class for importing Dataset\n",
    "\"\"\"\n",
    "class ProcessedIMDbDataset(Dataset):\n",
    "    \"\"\"loaded processed dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, attention_masks, labels, lengths=None):\n",
    "        self.sequences = sequences\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "        self.lengths = lengths if lengths is not None else torch.ones_like(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.sequences[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx],\n",
    "            'lengths': self.lengths[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Model Trainer based on Bert Finetuning\n",
    "\"\"\"\n",
    "class BertSentimentTrainer:\n",
    "    \"\"\" SentimentAnalyzer Based on Bert \"\"\"\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=2, max_length=512):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f'using device {self.device}')\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=num_labels)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.test_loader = None\n",
    "\n",
    "    def _get_memory_usage(self):\n",
    "        # when using GPU\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            gpu_memory = torch.cuda.max_memory_allocated() / 1024**3  # GB\n",
    "            return gpu_memory\n",
    "        else: # using cpu\n",
    "            process = psutil.Process()\n",
    "            return process.memory_info().rss / 1024**3  # GB\n",
    "\n",
    "    def load_data(self, file_path='./all_data.pt', batch_size=20):\n",
    "        print(f\"loading dataset\")\n",
    "\n",
    "        data = torch.load(file_path, weights_only=True)\n",
    "\n",
    "        train_dataset = ProcessedIMDbDataset(\n",
    "            data['train_sequences'],\n",
    "            data['train_masks'], \n",
    "            data['train_labels'],\n",
    "            data['train_lengths']\n",
    "        )\n",
    "        \n",
    "        val_dataset = ProcessedIMDbDataset(\n",
    "            data['val_sequences'],\n",
    "            data['val_masks'],\n",
    "            data['val_labels'], \n",
    "            data['val_lengths']\n",
    "        )\n",
    "        \n",
    "        test_dataset = ProcessedIMDbDataset(\n",
    "            data['test_sequences'],\n",
    "            data['test_masks'],\n",
    "            data['test_labels'],\n",
    "            data['test_lengths']\n",
    "        )\n",
    "\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        print(f\"loaded done, train dataset:{len(train_dataset)}, val_dataset:{len(val_dataset)}, test_dataset:{len(test_dataset)}\")\n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "    def set_finetune_param(self, strategy_name, epochs=4, lr=2e-5, warmup_steps=0):\n",
    "        \"\"\"\n",
    "        setting params for different strategies and get corresponding optimizer&scheduler\n",
    "        \"\"\"\n",
    "        if strategy_name == \"standard\":\n",
    "            # use AdamW with standard LR strategy\n",
    "            optimizer = AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        elif strategy_name == \"differential_lr\":\n",
    "            no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "            optimizer_parameters_by_group = [\n",
    "                # classifier - high\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.model.classifier.named_parameters()],\n",
    "                    \"lr\": lr,\n",
    "                    \"weight_decay\": 0.01\n",
    "                },\n",
    "                # BERT top 4 layers - medium\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.model.bert.encoder.layer[-4:].named_parameters() \n",
    "                              if not any(nd in n for nd in no_decay)],\n",
    "                    \"lr\": lr / 2,\n",
    "                    \"weight_decay\": 0.01\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.model.bert.encoder.layer[-4:].named_parameters() \n",
    "                              if any(nd in n for nd in no_decay)],\n",
    "                    \"lr\": lr / 2,\n",
    "                    \"weight_decay\": 0.0\n",
    "                },\n",
    "                # BERT bottom 8 layers - low\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.model.bert.encoder.layer[:-4].named_parameters() \n",
    "                              if not any(nd in n for nd in no_decay)],\n",
    "                    \"lr\": lr / 10,\n",
    "                    \"weight_decay\": 0.01\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.model.bert.encoder.layer[:-4].named_parameters() \n",
    "                              if any(nd in n for nd in no_decay)],\n",
    "                    \"lr\": lr / 10,\n",
    "                    \"weight_decay\": 0.0\n",
    "                }\n",
    "            ]\n",
    "            optimizer = AdamW(optimizer_parameters_by_group)\n",
    "        else:\n",
    "            print(f\"no implemention for {strategy_name}, using standard strategy.\")\n",
    "            optimizer = AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "\n",
    "        total_steps = len(self.train_loader) * epochs\n",
    "\n",
    "        #scheduler with warmup\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def finetune_by_strategy(self, strategy_name, epochs=4, learning_rate=2e-5, warmup_steps=0, logging_steps=50):\n",
    "        print(f\"start {strategy_name} finetuning\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        initial_memory = self._get_memory_usage()\n",
    "        \n",
    "        optimizer, scheduler = self.set_finetune_param(strategy_name, epochs, learning_rate, warmup_steps)\n",
    "        training_stats = self.train(strategy_name, optimizer, scheduler, epochs, 2, learning_rate, warmup_steps, logging_steps)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        final_memory = self._get_memory_usage()\n",
    "\n",
    "        training_time = end_time - start_time\n",
    "        memory_used = final_memory - initial_memory\n",
    "        \n",
    "        print(f\"{strategy_name} training completed in {training_time:.2f} seconds\")\n",
    "        print(f\"Memory used: {memory_used:.2f} GB\")\n",
    "        \n",
    "        return training_stats, training_time, memory_used\n",
    "\n",
    "        \n",
    "    def train(self, strategy_name, optimizer, scheduler, epochs=4, accumulation_steps=8, learning_rate=2e-5, warmup_steps=0, logging_steps=50):\n",
    "        if self.train_loader is None or self.val_loader is None:\n",
    "            raise ValueError(\"datasets not ready!\")\n",
    "\n",
    "        self.calculate_model_parameters()\n",
    "\n",
    "        training_stats = []\n",
    "        best_val_accuracy = 0\n",
    "        patience = 2\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(\"=\" * 80)\n",
    "\n",
    "            self.model.train()\n",
    "            total_train_loss = 0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_progress = tqdm(self.train_loader, desc=f'training {strategy_name}')\n",
    "            for step, batch in enumerate(batch_progress):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "\n",
    "                loss = loss / accumulation_steps\n",
    "                loss.backward()\n",
    "\n",
    "                total_train_loss += loss.item()\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                train_correct += (predictions == labels).sum().item()\n",
    "                train_total += labels.size(0)\n",
    "\n",
    "                if (step + 1) % accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                if step % logging_steps == 0:\n",
    "                    batch_progress.set_postfix(\n",
    "                        {\n",
    "                        'loss': f\"{loss.item():.4f}\",\n",
    "                        'acc': f\"{train_correct/train_total:.4f}\",\n",
    "                        'mem': f\"{self._get_memory_usage():.2f}GB\"\n",
    "                        }\n",
    "                    )\n",
    "            \n",
    "            if len(self.train_loader) % accumulation_steps != 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(self.train_loader)\n",
    "            train_accuracy = train_correct / train_total\n",
    "            val_accuracy, val_loss = self.evaluate(self.val_loader)\n",
    "            print(f\"training loss: {avg_train_loss:.4f}, training accuracy:{train_accuracy:.4f}\")\n",
    "\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                self.save_model(f'best_bert_model_{strategy_name}')\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"early stop, we've got best model\")\n",
    "                break\n",
    "\n",
    "            training_stats.append(\n",
    "                {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'train_loss': avg_train_loss,\n",
    "                    'train_accuracy': train_accuracy,\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_accuracy': val_accuracy,\n",
    "                    'strategy_name': strategy_name,\n",
    "                    'memory_usage': self._get_memory_usage()\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # at last, we load the best performance model, for further process\n",
    "        self.load_model(f'best_bert_model_{strategy_name}')\n",
    "        return training_stats\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"evaluating\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                correct += (predictions == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return accuracy, avg_loss\n",
    "\n",
    "    def predict(self, texts):\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for text in texts:\n",
    "                encoding = self.tokenizer(\n",
    "                    text,\n",
    "                    truncation=True,\n",
    "                    padding='max_length',\n",
    "                    max_length=self.max_length,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                input_ids = encoding['input_ids'].to(self.device)\n",
    "                attention_mask = encoding['attention_mask'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                pred = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                predictions.append(pred.cpu().item())\n",
    "                probabilities.append(probs.cpu().numpy())\n",
    "\n",
    "        return predictions, probabilities\n",
    "\n",
    "    def calculate_model_parameters(self):\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        \n",
    "        self.total_params = total_params\n",
    "        self.trainable_params = trainable_params\n",
    "        \n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        return total_params, trainable_params\n",
    "    \n",
    "    def overall_evaluation(self):\n",
    "        if self.test_loader is None:\n",
    "            raise ValueError(\"test datasets not ready\")\n",
    "    \n",
    "        self.model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        all_probabilities = []\n",
    "        \n",
    "        # Measure inference time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.test_loader, desc=\"testing\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                probabilities = torch.softmax(logits, dim=1)\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_probabilities.extend(probabilities.cpu().numpy())\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "        class_report = classification_report(all_labels, all_predictions, target_names=['negative', 'positive'])\n",
    "        conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1,\n",
    "            'classification_report': class_report,\n",
    "            'confusion_matrix': conf_matrix,\n",
    "            'predictions': all_predictions,\n",
    "            'probabilities': all_probabilities,\n",
    "            'inference_time': inference_time,\n",
    "            'model_size_mb': self.total_params * 4 / (1024**2),  # 4 bytes per parameter\n",
    "            'total_parameters': self.total_params,\n",
    "            'trainable_parameters': self.trainable_params\n",
    "        }\n",
    "        \n",
    "        print(f\"test accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 score: {f1:.4f}\")\n",
    "        print(f\"Inference time: {inference_time:.2f} seconds\")\n",
    "        print(f\"Model size: {results['model_size_mb']:.2f} MB\")\n",
    "        print(\"\\n class report:\")\n",
    "        print(class_report)\n",
    "        print(\"\\n confusion matrix\")\n",
    "        print(conf_matrix)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def save_model(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    \n",
    "        self.model.save_pretrained(path)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        print(f\"save model to: {path}\")\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"model file not exist!\")\n",
    "            return\n",
    "        \n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        self.model.to(self.device)\n",
    "        print(f\"loaded model from {path}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Visualization Part for bert\n",
    "\"\"\"\n",
    "\n",
    "def plt_training_history_data(all_training_stats):\n",
    "    \"\"\"\n",
    "    use matplotlib to plot training history\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    colors = {'standard': 'blue', 'differential_lr': 'red'}\n",
    "    \n",
    "    for strategy_stats in all_training_stats:\n",
    "        if not strategy_stats:\n",
    "            continue\n",
    "            \n",
    "        strategy_name = strategy_stats[0]['strategy_name']\n",
    "        df_stats = pd.DataFrame(strategy_stats)\n",
    "        color = colors.get(strategy_name, 'green')\n",
    "        \n",
    "        # Training Loss\n",
    "        ax1.plot(df_stats['epoch'], df_stats['train_loss'], \n",
    "                color=color, linestyle='-', label=f'{strategy_name} train')\n",
    "        ax1.plot(df_stats['epoch'], df_stats['val_loss'], \n",
    "                color=color, linestyle='--', label=f'{strategy_name} val')\n",
    "        \n",
    "        # Training Accuracy\n",
    "        ax2.plot(df_stats['epoch'], df_stats['train_accuracy'], \n",
    "                color=color, linestyle='-', label=f'{strategy_name} train')\n",
    "        ax2.plot(df_stats['epoch'], df_stats['val_accuracy'], \n",
    "                color=color, linestyle='--', label=f'{strategy_name} val')\n",
    "    \n",
    "    ax1.set_title('Training and Validation Loss Comparison')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.set_title('Training and Validation Accuracy Comparison')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Final performance comparison\n",
    "    strategies = []\n",
    "    final_val_accuracies = []\n",
    "    final_val_losses = []\n",
    "    \n",
    "    for strategy_stats in all_training_stats:\n",
    "        if not strategy_stats:\n",
    "            continue\n",
    "        strategy_name = strategy_stats[0]['strategy_name']  # 修复：改为strategy_name\n",
    "        final_epoch = strategy_stats[-1]\n",
    "        strategies.append(strategy_name)\n",
    "        final_val_accuracies.append(final_epoch['val_accuracy'])\n",
    "        final_val_losses.append(final_epoch['val_loss'])\n",
    "\n",
    "    if strategies:\n",
    "        x = range(len(strategies))\n",
    "        \n",
    "        ax3.bar(x, final_val_accuracies, color=['blue', 'red'], alpha=0.7)\n",
    "        ax3.set_title('Final Validation Accuracy by Strategy')\n",
    "        ax3.set_xlabel('Strategy')\n",
    "        ax3.set_ylabel('Accuracy')\n",
    "        ax3.set_xticks(x)\n",
    "        ax3.set_xticklabels(strategies)\n",
    "        \n",
    "        ax4.bar(x, final_val_losses, color=['blue', 'red'], alpha=0.7)\n",
    "        ax4.set_title('Final Validation Loss by Strategy')\n",
    "        ax4.set_xlabel('Strategy')\n",
    "        ax4.set_ylabel('Loss')\n",
    "        ax4.set_xticks(x)\n",
    "        ax4.set_xticklabels(strategies)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bert_strategy_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, strategy_name):\n",
    "    \"\"\"plot confusion matrix\"\"\"\n",
    "    class_names=['negative', 'positive']\n",
    "    \n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'BERT {strategy_name} Confusion Matrix')\n",
    "    plt.ylabel('Ground Truth')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.savefig(f'bert_{strategy_name}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_performance_comparison(strategy_results):\n",
    "    \"\"\"Plot performance metrics comparison\"\"\"\n",
    "    strategies = list(strategy_results.keys())\n",
    "    accuracies = [result['accuracy'] for result in strategy_results.values()]\n",
    "    f1_scores = [result['f1_score'] for result in strategy_results.values()]\n",
    "    \n",
    "    x = np.arange(len(strategies))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.7)\n",
    "    bars2 = ax.bar(x + width/2, f1_scores, width, label='F1 Score', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Finetuning Strategy')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Performance Comparison of Different Finetuning Strategies')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(strategies)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.4f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.4f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bert_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Compare performance/cost/parameters with previous LSTM Model\n",
    "\"\"\"\n",
    "def plot_overall_comparison(bert_results, lstm_results):\n",
    "    \"\"\"Plot comparison between BERT and LSTM\"\"\"\n",
    "    # Performance metrics\n",
    "    bert_accuracy = bert_results['accuracy']\n",
    "    bert_f1 = bert_results['f1_score']\n",
    "    lstm_accuracy = lstm_results.get('accuracy', 0)\n",
    "    lstm_f1 = lstm_results.get('f1_score', 0)\n",
    "    \n",
    "    # Computational metrics\n",
    "    bert_training_time = bert_results.get('training_time', 0)\n",
    "    bert_inference_time = bert_results.get('inference_time', 0)\n",
    "    bert_model_size = bert_results.get('model_size_mb', 0)\n",
    "    bert_params = bert_results.get('total_parameters', 0)\n",
    "    \n",
    "    lstm_training_time = lstm_results.get('training_time', 0)\n",
    "    lstm_inference_time = lstm_results.get('inference_time', 0)\n",
    "    lstm_model_size = lstm_results.get('model_size_mb', 0)\n",
    "    lstm_params = lstm_results.get('total_parameters', 0)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Performance comparison\n",
    "    models = ['LSTM', 'BERT']\n",
    "    accuracies = [lstm_accuracy, bert_accuracy]\n",
    "    f1_scores = [lstm_f1, bert_f1]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.7)\n",
    "    ax1.bar(x + width/2, f1_scores, width, label='F1 Score', alpha=0.7)\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_title('Performance Comparison: LSTM vs BERT')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training and inference time comparison\n",
    "    training_times = [lstm_training_time, bert_training_time]\n",
    "    inference_times = [lstm_inference_time, bert_inference_time]\n",
    "    \n",
    "    ax2.bar(x - width/2, training_times, width, label='Training Time (s)', alpha=0.7)\n",
    "    ax2.bar(x + width/2, inference_times, width, label='Inference Time (s)', alpha=0.7)\n",
    "    ax2.set_xlabel('Model')\n",
    "    ax2.set_ylabel('Time (seconds)')\n",
    "    ax2.set_title('Computational Time Comparison')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(models)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Model size comparison\n",
    "    model_sizes = [lstm_model_size, bert_model_size]\n",
    "    \n",
    "    ax3.bar(models, model_sizes, color=['orange', 'green'], alpha=0.7)\n",
    "    ax3.set_xlabel('Model')\n",
    "    ax3.set_ylabel('Model Size (MB)')\n",
    "    ax3.set_title('Model Size Comparison')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Parameter count comparison\n",
    "    parameters = [lstm_params, bert_params]\n",
    "    \n",
    "    ax4.bar(models, parameters, color=['purple', 'brown'], alpha=0.7)\n",
    "    ax4.set_xlabel('Model')\n",
    "    ax4.set_ylabel('Number of Parameters')\n",
    "    ax4.set_title('Parameter Count Comparison')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bert_lstm_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_lstm_results(path):\n",
    "    lstm_results = None\n",
    "    \n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            lstm_results = json.load(f)\n",
    "    except Exception as e:  # 修复：改为Exception as e\n",
    "        print(f\"load lstm results failed: {str(e)}\")\n",
    "        # demo params\n",
    "        lstm_results = {\n",
    "            'accuracy': 0.8718,\n",
    "            'f1_score': 0.8728,\n",
    "            'training_time': 1200,  # 20 minutes\n",
    "            'inference_time': 30,\n",
    "            'model_size_mb': 50,\n",
    "            'total_parameters': 13000000,\n",
    "            'memory_usage': 2.5\n",
    "        }\n",
    "\n",
    "    return lstm_results\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Program Main Process\n",
    "\"\"\"\n",
    "def perform_training():\n",
    "    work_path = os.getcwd()\n",
    "    data_file = 'all_data.pt'\n",
    "    data_file_path = os.path.join(work_path, \"processed_data_binary\", data_file)\n",
    "\n",
    "    trainer = BertSentimentTrainer(\n",
    "        model_name = 'prajjwal1/bert-mini',\n",
    "        num_labels=2,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    trainer.load_data(data_file_path, batch_size=32)  # 最大批量\n",
    "    \n",
    "    finetune_strategies = [\"standard\", \"differential_lr\"]# ,\"gradual_unfreeze\", \"bitfit\"] #bitfit bias-term finetune\n",
    "    \n",
    "    all_training_stats = []\n",
    "    strategy_results = {}\n",
    "    training_times = {}\n",
    "    memory_usage = {}\n",
    "\n",
    "    for strategy in finetune_strategies:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training with strategy: {strategy}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        training_stats, training_time, memory_used = trainer.finetune_by_strategy(\n",
    "            strategy_name=\"standard\",\n",
    "            epochs=2,  # 2个epoch更好收敛\n",
    "            learning_rate=3e-5,\n",
    "            warmup_steps=50,\n",
    "            logging_steps=100\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        all_training_stats.append(training_stats)\n",
    "        training_times[strategy] = training_time\n",
    "        memory_usage[strategy] = memory_used\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        results = trainer.overall_evaluation()\n",
    "        # Add computational metrics to results\n",
    "        results['training_time'] = training_time\n",
    "        results['memory_usage'] = memory_used\n",
    "        strategy_results[strategy] = results\n",
    "        \n",
    "        # Plot confusion matrix for this strategy\n",
    "        plot_confusion_matrix(results['confusion_matrix'], strategy_name=strategy)\n",
    "        \n",
    "        # Save model\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_save_path = f\"sentiment_bert_{strategy}_model_{timestamp}\"\n",
    "        trainer.save_model(model_save_path)\n",
    "\n",
    "\n",
    "    plt_training_history_data(all_training_stats)\n",
    "    plot_performance_comparison(strategy_results)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STRATEGY COMPARISON SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for strategy in finetune_strategies:\n",
    "        results = strategy_results[strategy]\n",
    "        print(f\"\\n{strategy.upper()} Strategy:\")\n",
    "        print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "        print(f\"  F1 Score: {results['f1_score']:.4f}\")\n",
    "        print(f\"  Training Time: {training_times[strategy]:.2f}s\")\n",
    "        print(f\"  GPU Memory Usage: {memory_usage[strategy]:.2f}GB\")\n",
    "        print(f\"  Model Size: {results['model_size_mb']:.2f}MB\")\n",
    "        print(f\"  Parameters: {results['total_parameters']:,}\")\n",
    "    \n",
    "\n",
    "    return trainer, strategy_results, all_training_stats, training_times, memory_usage\n",
    "\n",
    "def test_texts(trainer):\n",
    "    test_samples = [\n",
    "        \"quite a good movie, but the ending could be better!\",\n",
    "        \"it's a fantastic film I've ever enjoyed!\"\n",
    "    ]\n",
    "\n",
    "    predictions, probabilities = trainer.predict(test_samples)\n",
    "\n",
    "    sentiment_mapping = {0 : 'negative', 1 : 'positive'}\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAMPLE TEXT PREDICTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for text, pred, prob in zip(test_samples, predictions, probabilities):\n",
    "        print(f\"testing text: {text}\")\n",
    "        print(f\"prediction: {sentiment_mapping[pred]}, {max(prob[0]):.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        trainer, bert_results, training_stats, training_times, memory_usage = perform_training()\n",
    "    \n",
    "        lstm_results = load_lstm_results(\"./lstm_results.json\")\n",
    "    \n",
    "        if lstm_results:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"COMPREHENSIVE COMPARISON WITH LSTM BASELINE\")\n",
    "            print(\"=\"*80)\n",
    "            # Use the best performing BERT strategy for comparison\n",
    "            best_strategy = max(bert_results.keys(), \n",
    "                              key=lambda x: bert_results[x]['accuracy'])\n",
    "            plot_overall_comparison(bert_results[best_strategy], lstm_results)\n",
    "        \n",
    "        test_texts(trainer)\n",
    "    except Exception as e:\n",
    "        print(f\"error happened: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
