{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab47aa03",
   "metadata": {},
   "source": [
    "# Group Project: æ•°æ®é¢„åŠ è½½&æ¸…æ´—\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a900734c-4d56-4923-ad51-4349706592f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478321d-d359-403a-a86a-ce8071f5179f",
   "metadata": {},
   "source": [
    "## DataDownloader - æ•°æ®ä¸‹è½½ä¸éªŒè¯æ¨¡å—\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "- download_imdb() - ä¸‹è½½IMDbæ•°æ®é›†å‹ç¼©åŒ…å¹¶è§£å‹\n",
    "\n",
    "- validate_data_directory() - éªŒè¯æ•°æ®ç›®å½•ç»“æ„æ˜¯å¦å®Œæ•´\n",
    "\n",
    "- setup_data() - ä¸»å…¥å£ï¼Œç¡®ä¿æ•°æ®å¯ç”¨ï¼ˆä¸‹è½½æˆ–ä½¿ç”¨ç°æœ‰æ•°æ®ï¼‰\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c4ee9f-a990-43bf-bc51-8988b31ce3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDownloader:\n",
    "    \n",
    "    @staticmethod\n",
    "    def download_imdb() -> Path:\n",
    "        \"\"\"ä¸‹è½½IMDbæ•°æ®é›†\"\"\"\n",
    "        data_dir = Path('./aclImdb')\n",
    "        \n",
    "        if data_dir.exists():\n",
    "            print(\"IMDbæ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½\")\n",
    "            return data_dir\n",
    "        \n",
    "        print(\"ä¸‹è½½IMDbæ•°æ®é›†...\")\n",
    "        url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "        tar_path = Path('./aclImdb_v1.tar.gz')\n",
    "        \n",
    "        try:\n",
    "            # ä¸‹è½½æ–‡ä»¶\n",
    "            urllib.request.urlretrieve(url, tar_path)\n",
    "            print(\"ä¸‹è½½å®Œæˆï¼Œè§£å‹ä¸­...\")\n",
    "            \n",
    "            # è§£å‹æ–‡ä»¶\n",
    "            with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "                tar.extractall(path='./')\n",
    "            \n",
    "            # åˆ é™¤å‹ç¼©åŒ…\n",
    "            tar_path.unlink()\n",
    "            print(\"IMDbæ•°æ®é›†å‡†å¤‡å®Œæˆ\")\n",
    "            \n",
    "            return data_dir\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"æ•°æ®é›†ä¸‹è½½å¤±è´¥: {e}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_data_directory(data_dir: Path) -> bool:\n",
    "        \"\"\"éªŒè¯æ•°æ®ç›®å½•ç»“æ„\"\"\"\n",
    "        required_dirs = [\n",
    "            data_dir / 'train' / 'pos',\n",
    "            data_dir / 'train' / 'neg', \n",
    "            data_dir / 'test' / 'pos',\n",
    "            data_dir / 'test' / 'neg'\n",
    "        ]\n",
    "        \n",
    "        for dir_path in required_dirs:\n",
    "            if not dir_path.exists():\n",
    "                return False\n",
    "            if len(list(dir_path.glob('*.txt'))) == 0:\n",
    "                return False\n",
    "                \n",
    "        return True\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_data() -> Path:\n",
    "        \"\"\"è®¾ç½®æ•°æ®ç›®å½•ï¼Œç¡®ä¿æ•°æ®å¯ç”¨\"\"\"\n",
    "        data_dir = Path('./aclImdb')\n",
    "        \n",
    "        if DataDownloader.validate_data_directory(data_dir):\n",
    "            print(\"âœ… æ•°æ®ç›®å½•éªŒè¯é€šè¿‡\")\n",
    "            return data_dir\n",
    "        \n",
    "        print(\"æ•°æ®ç›®å½•ä¸å®Œæ•´ï¼Œé‡æ–°ä¸‹è½½...\")\n",
    "        data_dir = DataDownloader.download_imdb()\n",
    "        \n",
    "        if not DataDownloader.validate_data_directory(data_dir):\n",
    "            raise Exception(\"æ•°æ®ç›®å½•éªŒè¯å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥æ•°æ®é›†\")\n",
    "            \n",
    "        return data_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd27fd7-702e-4e70-badf-3302b20904d7",
   "metadata": {},
   "source": [
    "## TextProcessor - æ–‡æœ¬å¤„ç†æ¨¡å—\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "- robust_text_cleaning(text) - æ ¸å¿ƒæ–‡æœ¬æ¸…æ´—ï¼ŒåŒ…å«ï¼š\n",
    "\n",
    "    1. HTMLå®ä½“è§£ç å’Œæ ‡ç­¾ç§»é™¤\n",
    "    \n",
    "    2. URLå¤„ç†\n",
    "    \n",
    "    3. ç¼©å†™è¯å±•å¼€ï¼ˆå¦‚ \"can't\" â†’ \"can not\"ï¼‰\n",
    "    \n",
    "    4. æ ‡ç‚¹ç¬¦å·æ ‡å‡†åŒ–\n",
    "    \n",
    "    5. å¤§å°å†™ç»Ÿä¸€\n",
    "\n",
    "- process_in_batches() - æ‰¹é‡å¤„ç†æ–‡æœ¬ï¼Œé¿å…å†…å­˜æº¢å‡º\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c6ef375-2e6b-4057-bf48-7c0b8822574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    @staticmethod\n",
    "    def robust_text_cleaning(text: str) -> str:\n",
    "        \"\"\"\n",
    "        é²æ£’çš„æ–‡æœ¬æ¸…æ´—ç®¡é“ - åŒ…å«HTMLå¤„ç†\n",
    "        \"\"\"\n",
    "        # è§£ç HTMLå®ä½“\n",
    "        text = html.unescape(text)\n",
    "        \n",
    "        # ç§»é™¤HTMLæ ‡ç­¾\n",
    "        text = re.sub(r'<[^>]+>', ' ', text)\n",
    "        \n",
    "        # å¤„ç†URL\n",
    "        text = re.sub(r'http\\S+', ' <URL> ', text)\n",
    "        \n",
    "        # å¤„ç†å¸¸è§çš„ç¼©å†™å’Œå¦å®šå½¢å¼\n",
    "        contractions = {\n",
    "            r\"won't\": \"will not\", r\"can't\": \"can not\", r\"n't\": \" not\",\n",
    "            r\"'re\": \" are\", r\"'s\": \" is\", r\"'d\": \" would\", \n",
    "            r\"'ll\": \" will\", r\"'t\": \" not\", r\"'ve\": \" have\",\n",
    "            r\"'m\": \" am\"\n",
    "        }\n",
    "        \n",
    "        for pattern, replacement in contractions.items():\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        \n",
    "        # ä¿ç•™åŸºæœ¬çš„æ ‡ç‚¹ç”¨äºæƒ…æ„Ÿåˆ†æ\n",
    "        text = re.sub(r'[^a-zA-Z\\s\\.!?,;:\\']', ' ', text)\n",
    "        \n",
    "        # å¤„ç†é‡å¤çš„æ ‡ç‚¹\n",
    "        text = re.sub(r'([!?.]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'[!?.]+', r' \\0 ', text)\n",
    "        \n",
    "        # ç»Ÿä¸€ç©ºæ ¼å¤„ç†\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text.strip().lower()\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_in_batches(texts: List[str], process_func, batch_size: int = 1000, process_name: str = \"å¤„ç†\") -> List[str]:\n",
    "        \"\"\"æ‰¹é‡å¤„ç†æ–‡æœ¬æ•°æ®\"\"\"\n",
    "        processed = []\n",
    "        total = len(texts)\n",
    "        \n",
    "        for i in range(0, total, batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            processed_batch = [process_func(text) for text in batch]\n",
    "            processed.extend(processed_batch)\n",
    "            \n",
    "            if i % 5000 == 0 and i > 0:\n",
    "                print(f\"  {process_name} {i}/{total} ä¸ªæ ·æœ¬\")\n",
    "        \n",
    "        print(f\"âœ… {process_name}å®Œæˆ\")\n",
    "        return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f1ae6c-ce66-4cdc-b2f0-aa8dc3dcd24e",
   "metadata": {},
   "source": [
    "## VocabularyBuilder - è¯æ±‡è¡¨ç®¡ç†æ¨¡å—\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "- build_vocabulary(texts) - åŸºäºè®­ç»ƒæ–‡æœ¬æ„å»ºè¯æ±‡è¡¨ï¼š\n",
    "\n",
    "    1. ç»Ÿè®¡è¯é¢‘\n",
    "    \n",
    "    2. è¿‡æ»¤ä½é¢‘è¯\n",
    "    \n",
    "    3. æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼ˆPAD/UNK/BOS/EOSï¼‰\n",
    "    \n",
    "    4. è®¡ç®—è¯æ±‡è¡¨è¦†ç›–ç‡\n",
    "\n",
    "- text_to_sequence(text) - å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—åºåˆ—ï¼š\n",
    "\n",
    "    1. æ·»åŠ å¼€å§‹/ç»“æŸæ ‡è®°\n",
    "    \n",
    "    2. å¤„ç†æœªçŸ¥è¯ï¼ˆOOVï¼‰\n",
    "    \n",
    "    3. åºåˆ—å¡«å……/æˆªæ–­\n",
    "    \n",
    "    4. ç”Ÿæˆæ³¨æ„åŠ›æ©ç \n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "394556fd-0eb8-489a-9742-543c5ac9391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyBuilder:\n",
    "    \n",
    "    def __init__(self, max_vocab_size: int = 30000, min_freq: int = 2):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_freq = min_freq\n",
    "        self.special_tokens = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    def build_vocabulary(self, texts: List[str]) -> None:\n",
    "        \"\"\"æ„å»ºè¯æ±‡è¡¨\"\"\"\n",
    "        print(\"æ„å»ºè¯æ±‡è¡¨...\")\n",
    "        \n",
    "        # ç»Ÿè®¡è¯é¢‘\n",
    "        word_freq = Counter()\n",
    "        for text in texts:\n",
    "            tokens = text.split()\n",
    "            word_freq.update(tokens)\n",
    "        \n",
    "        # è¿‡æ»¤ä½é¢‘è¯\n",
    "        filtered_words = [(word, freq) for word, freq in word_freq.items() \n",
    "                         if freq >= self.min_freq]\n",
    "        \n",
    "        # æŒ‰é¢‘ç‡æ’åºå¹¶é€‰æ‹©å‰Nä¸ªè¯\n",
    "        sorted_words = sorted(filtered_words, key=lambda x: x[1], reverse=True)\n",
    "        selected_words = [word for word, freq in sorted_words[:self.max_vocab_size - len(self.special_tokens)]]\n",
    "        \n",
    "        # æ„å»ºè¯æ±‡è¡¨\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "        \n",
    "        # æ·»åŠ ç‰¹æ®Šæ ‡è®°\n",
    "        for idx, token in enumerate(self.special_tokens):\n",
    "            self.token_to_id[token] = idx\n",
    "            self.id_to_token[idx] = token\n",
    "        \n",
    "        # æ·»åŠ æ™®é€šè¯æ±‡\n",
    "        for idx, word in enumerate(selected_words, start=len(self.special_tokens)):\n",
    "            self.token_to_id[word] = idx\n",
    "            self.id_to_token[idx] = word\n",
    "        \n",
    "        self.vocab_size = len(self.token_to_id)\n",
    "        \n",
    "        # OOVåˆ†æ\n",
    "        total_tokens = sum(word_freq.values())\n",
    "        covered_tokens = sum(freq for word, freq in word_freq.items() \n",
    "                           if word in self.token_to_id)\n",
    "        coverage = covered_tokens / total_tokens * 100\n",
    "        \n",
    "        print(f\"è¯æ±‡è¡¨æ„å»ºå®Œæˆ: {self.vocab_size} ä¸ªtoken\")\n",
    "        print(f\"è¯æ±‡è¡¨è¦†ç›–ç‡: {coverage:.2f}%\")\n",
    "    \n",
    "    def text_to_sequence(self, text: str, max_length: int = 512, \n",
    "                        add_special_tokens: bool = True) -> Tuple[List[int], List[int], int]:\n",
    "        \"\"\"å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•´æ•°åºåˆ—\"\"\"\n",
    "        tokens = text.split()\n",
    "        sequence = []\n",
    "        \n",
    "        # æ·»åŠ å¼€å§‹æ ‡è®°\n",
    "        if add_special_tokens:\n",
    "            sequence.append(self.token_to_id['<BOS>'])\n",
    "        \n",
    "        # è½¬æ¢tokens\n",
    "        for token in tokens:\n",
    "            sequence.append(self.token_to_id.get(token, self.token_to_id['<UNK>']))\n",
    "        \n",
    "        # æ·»åŠ ç»“æŸæ ‡è®°\n",
    "        if add_special_tokens:\n",
    "            sequence.append(self.token_to_id['<EOS>'])\n",
    "        \n",
    "        original_length = len(sequence)\n",
    "        \n",
    "        # å¡«å……æˆ–æˆªæ–­\n",
    "        if len(sequence) < max_length:\n",
    "            sequence.extend([self.token_to_id['<PAD>']] * (max_length - len(sequence)))\n",
    "            attention_mask = [1] * original_length + [0] * (max_length - original_length)\n",
    "        else:\n",
    "            sequence = sequence[:max_length]\n",
    "            attention_mask = [1] * max_length\n",
    "        \n",
    "        return sequence, attention_mask, original_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf84183-a006-4f07-8181-60f5dc69d6a2",
   "metadata": {},
   "source": [
    "## IMDBDataPreprocessor - æ•°æ®é¢„å¤„ç†æ ¸å¿ƒç±»\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "\n",
    "- load_raw_data() - ä»æ–‡ä»¶ç³»ç»ŸåŠ è½½åŸå§‹IMDbæ•°æ®\n",
    "\n",
    "- split_data_with_seeds() - æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ï¼ˆ60%/20%/20%ï¼‰\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "499dafae-f48c-42b2-b3b3-3c2fc5f96aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class IMDBDataPreprocessor:\n",
    "    \"\"\"\n",
    "    IMDbæ•°æ®é¢„å¤„ç†ç®¡é“ï¼ˆæ˜Ÿçº§æ ‡ç­¾ç‰ˆæœ¬ 1-10ï¼‰\n",
    "    \n",
    "    ä¸»è¦åŠŸèƒ½ï¼š\n",
    "      - åŠ è½½IMDbåŸå§‹æ•°æ®ï¼Œå¹¶å°†æ ‡ç­¾è§£æä¸º1..10æ˜Ÿçº§\n",
    "      - å¯å¤ç°çš„æ•°æ®åˆ’åˆ†ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•ï¼‰ï¼Œå½“æŸäº›ç±»åˆ«æ ·æœ¬è¿‡å°‘æ—¶è‡ªåŠ¨å…³é—­åˆ†å±‚æŠ½æ ·\n",
    "      - ä¿æŒåŸæœ‰çš„VocabularyBuilderä¸TextProcessoråˆå§‹åŒ–\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str, max_vocab_size: int = 30000,\n",
    "                 max_length: int = 512, min_freq: int = 2, seed: int = 42):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.max_length = max_length\n",
    "        self.seed = seed\n",
    "\n",
    "        # åˆå§‹åŒ–ç»„ä»¶\n",
    "        self.vocab_builder = VocabularyBuilder(max_vocab_size, min_freq)\n",
    "        self.text_processor = TextProcessor()\n",
    "\n",
    "        # è®¾ç½®éšæœºç§å­\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def load_raw_data(self) -> Tuple[List[str], List[int], List[str], List[int]]:\n",
    "        \"\"\"\n",
    "        åŠ è½½åŸå§‹IMDbæ•°æ®ï¼Œæ ‡ç­¾ä¸º1..10æ˜Ÿçº§ï¼ˆæ³¨æ„ï¼šå®˜æ–¹labeled splitä¸åŒ…å«5ä¸6ï¼‰\n",
    "        è¿”å›ï¼š\n",
    "            train_texts, train_labels, test_texts, test_labels\n",
    "        \"\"\"\n",
    "        print(\"åŠ è½½IMDbæ•°æ®é›†...\")\n",
    "\n",
    "        def load_from_directory(directory: Path) -> Tuple[List[str], List[int]]:\n",
    "            texts, labels = [], []\n",
    "            for label_type in ['pos', 'neg']:\n",
    "                dir_name = directory / label_type\n",
    "                if not dir_name.exists():\n",
    "                    print(f\"Warning: ç¼ºå°‘ç›®å½•: {dir_name}\")\n",
    "                    continue\n",
    "                for file_path in dir_name.glob('*.txt'):\n",
    "                    # æ–‡ä»¶åæ ¼å¼: \"<id>_<rating>.txt\"ï¼Œä¾‹å¦‚ \"12345_7.txt\"\n",
    "                    stem = file_path.stem\n",
    "                    parts = stem.split('_')\n",
    "                    if len(parts) < 2:\n",
    "                        print(f\"è·³è¿‡å¼‚å¸¸æ–‡ä»¶å: {file_path.name}\")\n",
    "                        continue\n",
    "                    rating_str = parts[-1]\n",
    "                    try:\n",
    "                        rating = int(rating_str)  # 1..10\n",
    "                    except ValueError as e:\n",
    "                        print(f\"è·³è¿‡è§£æå¤±è´¥æ–‡ä»¶: {file_path.name} ({e})\")\n",
    "                        continue\n",
    "\n",
    "                    # è¯»å–æ–‡æœ¬\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            text = f.read().strip()\n",
    "                    except Exception as e:\n",
    "                        print(f\"è·³è¿‡æ— æ³•è¯»å–æ–‡ä»¶: {file_path.name} ({e})\")\n",
    "                        continue\n",
    "\n",
    "                    texts.append(text)\n",
    "                    labels.append(rating)\n",
    "\n",
    "            # åˆ†å¸ƒæ£€æŸ¥\n",
    "            dist = Counter(labels)\n",
    "            dist_sorted = dict(sorted(dist.items()))\n",
    "            print(f\"{directory.name} æ˜Ÿçº§åˆ†å¸ƒ: {dist_sorted}\")\n",
    "            return texts, labels\n",
    "\n",
    "        train_dir = self.data_dir / 'train'\n",
    "        test_dir = self.data_dir / 'test'\n",
    "\n",
    "        train_texts, train_labels = load_from_directory(train_dir)\n",
    "        test_texts, test_labels = load_from_directory(test_dir)\n",
    "\n",
    "        print(f\"åŠ è½½äº† {len(train_texts)} è®­ç»ƒæ ·æœ¬, {len(test_texts)} æµ‹è¯•æ ·æœ¬\")\n",
    "        return train_texts, train_labels, test_texts, test_labels\n",
    "\n",
    "    def split_data_with_seeds(self, texts: List[str], labels: List[int],\n",
    "                              test_size: float = 0.2, val_size: float = 0.2\n",
    "                              ) -> Tuple[List[str], List[int], List[str], List[int], List[str], List[int]]:\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨éšæœºç§å­è¿›è¡Œå¯å¤ç°çš„æ•°æ®åˆ’åˆ†ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•ï¼Œ60%/20%/20%ï¼‰\n",
    "        å½“ä»»ä¸€ç±»åˆ«æ ·æœ¬æ•° < 2 æ—¶ï¼Œè‡ªåŠ¨å…³é—­åˆ†å±‚æŠ½æ ·ä»¥é¿å…sklearnæŠ¥é”™ã€‚\n",
    "        \"\"\"\n",
    "        if len(texts) != len(labels):\n",
    "            raise ValueError(\"texts å’Œ labels é•¿åº¦ä¸ä¸€è‡´\")\n",
    "\n",
    "        def _print_dist(name: str, y: List[int]):\n",
    "            c = Counter(y)\n",
    "            print(f\"{name} ç±»åˆ«åˆ†å¸ƒ: {dict(sorted(c.items()))}\")\n",
    "\n",
    "        print(\"æ‰§è¡Œå¯å¤ç°çš„æ•°æ®åˆ’åˆ†ï¼ˆå¸¦ç±»åˆ«æ£€æŸ¥ï¼‰...\")\n",
    "        counts = Counter(labels)\n",
    "        min_count = min(counts.values()) if counts else 0\n",
    "        use_stratify = labels if min_count >= 2 else None\n",
    "        if use_stratify is None:\n",
    "            print(f\"Warning: æœ€å°ç±»åˆ«æ ·æœ¬æ•° {min_count} < 2ï¼Œåˆæ¬¡åˆ’åˆ†ä¸ä½¿ç”¨åˆ†å±‚æŠ½æ ·\")\n",
    "\n",
    "        # å…ˆåˆ’åˆ†æµ‹è¯•é›†\n",
    "        train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "            texts, labels, test_size=test_size, random_state=self.seed,\n",
    "            stratify=use_stratify\n",
    "        )\n",
    "\n",
    "        # å†ä»è®­ç»ƒé›†åˆ’åˆ†éªŒè¯é›†\n",
    "        train_counts = Counter(train_labels)\n",
    "        min_train_count = min(train_counts.values()) if train_counts else 0\n",
    "        use_stratify_val = train_labels if min_train_count >= 2 else None\n",
    "        if use_stratify_val is None:\n",
    "            print(f\"Warning: è®­ç»ƒé›†ä¸­æœ€å°ç±»åˆ«æ ·æœ¬æ•° {min_train_count} < 2ï¼ŒéªŒè¯é›†åˆ’åˆ†ä¸ä½¿ç”¨åˆ†å±‚æŠ½æ ·\")\n",
    "\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            train_texts, train_labels, test_size=val_size, random_state=self.seed,\n",
    "            stratify=use_stratify_val\n",
    "        )\n",
    "\n",
    "        print(f\"æ•°æ®åˆ’åˆ†: è®­ç»ƒé›†={len(train_texts)}, éªŒè¯é›†={len(val_texts)}, æµ‹è¯•é›†={len(test_texts)}\")\n",
    "        _print_dist(\"è®­ç»ƒé›†\", train_labels)\n",
    "        _print_dist(\"éªŒè¯é›†\", val_labels)\n",
    "        _print_dist(\"æµ‹è¯•é›†\", test_labels)\n",
    "\n",
    "        return train_texts, train_labels, val_texts, val_labels, test_texts, test_labels          \n",
    "    \n",
    "### Updated for 1-10StarsVersion by Chen Yapeng end at this line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6034713-4c32-4500-9c86-b3698245eceb",
   "metadata": {},
   "source": [
    "## IMDBDataset - æ•°æ®é›†ç±»\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "\n",
    "- __init__() - åˆå§‹åŒ–æ•°æ®é›†ï¼ŒéªŒè¯æ•°æ®ä¸€è‡´æ€§\n",
    "\n",
    "- __len__() - è¿”å›æ•°æ®é›†å¤§å°\n",
    "\n",
    "- __getitem__() - è·å–å•ä¸ªæ ·æœ¬ï¼Œè¿”å›å­—å…¸æ ¼å¼\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caa48f1a-a2f2-40ee-8c13-128d7ca10d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"IMDbæ•°æ®é›†ç±»\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences: List[List[int]], attention_masks: List[List[int]], \n",
    "                 labels: List[int], lengths: List[int]):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "        self.attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "        \n",
    "        # éªŒè¯æ•°æ®å½¢çŠ¶\n",
    "        assert len(self.sequences) == len(self.labels), \"åºåˆ—å’Œæ ‡ç­¾æ•°é‡ä¸åŒ¹é…\"\n",
    "        assert len(self.sequences) == len(self.attention_masks), \"åºåˆ—å’Œæ³¨æ„åŠ›æ©ç æ•°é‡ä¸åŒ¹é…\"\n",
    "        assert len(self.sequences) == len(self.lengths), \"åºåˆ—å’Œé•¿åº¦æ•°é‡ä¸åŒ¹é…\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.sequences[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx],\n",
    "            'lengths': self.lengths[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fdb0d3-a10b-4094-a182-162acc0aa8b5",
   "metadata": {},
   "source": [
    "## DataSaver - æ•°æ®ä¿å­˜æ¨¡å—\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "\n",
    "- save_processed_data_multiclass() - ä¸»ä¿å­˜å‡½æ•°\n",
    "\n",
    "- _save_vocabulary() - ä¿å­˜è¯æ±‡è¡¨ä¸ºJSON\n",
    "\n",
    "- _save_config() - ä¿å­˜é¢„å¤„ç†é…ç½®\n",
    "\n",
    "- _save_tensor_data() - ä¿å­˜Tensoræ•°æ®ä¸º.ptæ–‡ä»¶\n",
    "\n",
    "- _save_metadata() - ä¿å­˜ç»Ÿè®¡å…ƒæ•°æ®\n",
    "\n",
    "- _verify_files() - éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da1e614-5d4d-4f45-8a7e-cce4aa480016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSaver:\n",
    "\n",
    "    @staticmethod\n",
    "    def save_processed_data_multiclass(datasets: Dict, vocab, metadata: Dict, output_dir: str):\n",
    "        \"\"\"ä¿å­˜æ‰€æœ‰å¤„ç†åçš„æ•°æ®\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"å¼€å§‹ä¿å­˜æ•°æ®åˆ°: {output_path}\")\n",
    "        \n",
    "        # 1. ä¿å­˜è¯æ±‡è¡¨\n",
    "        DataSaver._save_vocabulary(vocab, output_path)\n",
    "        \n",
    "        # 2. ä¿å­˜é…ç½®\n",
    "        DataSaver._save_config(output_path)\n",
    "        \n",
    "        # 3. ä¿å­˜ tensor æ•°æ®\n",
    "        DataSaver._save_tensor_data(datasets, output_path)\n",
    "        \n",
    "        # 4. ä¿å­˜å…ƒæ•°æ®\n",
    "        DataSaver._save_metadata(metadata, output_path)\n",
    "        \n",
    "        # 5. éªŒè¯æ–‡ä»¶\n",
    "        DataSaver._verify_files(output_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _save_vocabulary(vocab, output_path):\n",
    "        \"\"\"ä¿å­˜è¯æ±‡è¡¨\"\"\"\n",
    "        vocab_path = output_path / 'vocabulary.json'\n",
    "        with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(vocab, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"âœ… è¯æ±‡è¡¨ä¿å­˜åˆ°: {vocab_path}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _save_config(output_path):\n",
    "        \"\"\"ä¿å­˜é…ç½®\"\"\"\n",
    "        config_path = output_path / 'preprocessing_config.json'\n",
    "        config = {\n",
    "            'max_vocab_size': 30000,\n",
    "            'max_length': 512,\n",
    "            'min_freq': 2,\n",
    "            'seed': 42,\n",
    "            'saved_time': str(pd.Timestamp.now())\n",
    "        }\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        print(f\"âœ… é…ç½®ä¿å­˜åˆ°: {config_path}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _save_tensor_data(datasets, output_path):\n",
    "        \"\"\"ä¿å­˜ tensor æ•°æ®\"\"\"\n",
    "        data_path = output_path / 'all_data.pt'\n",
    "        \n",
    "        save_data = {\n",
    "            'train_sequences': datasets['train'].sequences,\n",
    "            'train_masks': datasets['train'].attention_masks,\n",
    "            'train_labels': datasets['train'].labels,\n",
    "            'train_lengths': datasets['train'].lengths,\n",
    "            \n",
    "            'val_sequences': datasets['val'].sequences,\n",
    "            'val_masks': datasets['val'].attention_masks,\n",
    "            'val_labels': datasets['val'].labels,\n",
    "            'val_lengths': datasets['val'].lengths,\n",
    "            \n",
    "            'test_sequences': datasets['test'].sequences,\n",
    "            'test_masks': datasets['test'].attention_masks,\n",
    "            'test_labels': datasets['test'].labels,\n",
    "            'test_lengths': datasets['test'].lengths,\n",
    "        }\n",
    "        \n",
    "        torch.save(save_data, data_path)\n",
    "        file_size = data_path.stat().st_size / 1024 / 1024\n",
    "        print(f\"âœ… Tensoræ•°æ®ä¿å­˜åˆ°: {data_path} (å¤§å°: {file_size:.2f} MB)\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _save_metadata(metadata, output_path):\n",
    "        \"\"\"ä¿å­˜å…ƒæ•°æ®\"\"\"\n",
    "        metadata_path = output_path / 'preprocessing_metadata.json'\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str, ensure_ascii=False)\n",
    "        print(f\"âœ… å…ƒæ•°æ®ä¿å­˜åˆ°: {metadata_path}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _verify_files(output_path):\n",
    "        \"\"\"éªŒè¯æ–‡ä»¶æ˜¯å¦åˆ›å»ºæˆåŠŸ\"\"\"\n",
    "        print(\"\\nğŸ“ éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶:\")\n",
    "        for file in output_path.iterdir():\n",
    "            size_kb = file.stat().st_size / 1024\n",
    "            print(f\"  {file.name} ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3593ed-7193-427e-9df1-684fe97d82d3",
   "metadata": {},
   "source": [
    "## DataProcessingPipeline - ä¸»æµæ°´çº¿\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "\n",
    "- run_pipeline() - æ‰§è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ï¼ˆ10ä¸ªæ­¥éª¤ï¼‰\n",
    "\n",
    "- _vectorize_texts() - æ‰¹é‡å‘é‡åŒ–æ–‡æœ¬\n",
    "\n",
    "- _create_datasets() - åˆ›å»ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®é›†\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5b34c6b-730e-4553-8ba1-0b2f9b4e93c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessingPipeline:\n",
    "    \"\"\"å®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿\"\"\"\n",
    "    \n",
    "    def __init__(self, max_vocab_size: int = 30000, max_length: int = 512, \n",
    "                 min_freq: int = 2, seed: int = 42):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.min_freq = min_freq\n",
    "        self.seed = seed\n",
    "        \n",
    "    def run_pipeline(self) -> bool:\n",
    "        \"\"\"è¿è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿\"\"\"\n",
    "        try:\n",
    "            # 1. è®¾ç½®æ•°æ®ç›®å½•\n",
    "            print(\"=== IMDbæ•°æ®é¢„å¤„ç†å¼€å§‹ ===\")\n",
    "            data_dir = DataDownloader.setup_data()\n",
    "            \n",
    "            # 2. åˆå§‹åŒ–é¢„å¤„ç†å™¨\n",
    "            preprocessor = IMDBDataPreprocessor(\n",
    "                data_dir=str(data_dir),\n",
    "                max_vocab_size=self.max_vocab_size,\n",
    "                max_length=self.max_length,\n",
    "                min_freq=self.min_freq,\n",
    "                seed=self.seed\n",
    "            )\n",
    "            \n",
    "            # 3. åŠ è½½åŸå§‹æ•°æ®\n",
    "            print(\"\\nğŸ“¥ åŠ è½½å®Œæ•´åŸå§‹æ•°æ®...\")\n",
    "            train_texts, train_labels, test_texts, test_labels = preprocessor.load_raw_data()\n",
    "            print(f\"åŸå§‹æ•°æ®: {len(train_texts)} è®­ç»ƒæ ·æœ¬, {len(test_texts)} æµ‹è¯•æ ·æœ¬\")\n",
    "            \n",
    "            # 4. æ•°æ®åˆ’åˆ†\n",
    "            print(\"ğŸ“Š å®Œæ•´æ•°æ®åˆ’åˆ†...\")\n",
    "            all_texts = train_texts + test_texts\n",
    "            all_labels = train_labels + test_labels\n",
    "            \n",
    "            train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = \\\n",
    "                preprocessor.split_data_with_seeds(all_texts, all_labels)\n",
    "            \n",
    "            print(f\"æ•°æ®åˆ’åˆ†: è®­ç»ƒé›†={len(train_texts)}, éªŒè¯é›†={len(val_texts)}, æµ‹è¯•é›†={len(test_texts)}\")\n",
    "            \n",
    "            # 5. æ–‡æœ¬æ¸…æ´—\n",
    "            print(\"ğŸ§¹ æ¸…æ´—å®Œæ•´æ–‡æœ¬æ•°æ®...\")\n",
    "            train_texts_clean = TextProcessor.process_in_batches(\n",
    "                train_texts, TextProcessor.robust_text_cleaning, process_name=\"æ–‡æœ¬æ¸…æ´—\"\n",
    "            )\n",
    "            val_texts_clean = TextProcessor.process_in_batches(\n",
    "                val_texts, TextProcessor.robust_text_cleaning, process_name=\"æ–‡æœ¬æ¸…æ´—\"\n",
    "            )\n",
    "            test_texts_clean = TextProcessor.process_in_batches(\n",
    "                test_texts, TextProcessor.robust_text_cleaning, process_name=\"æ–‡æœ¬æ¸…æ´—\"\n",
    "            )\n",
    "            \n",
    "            # 6. æ„å»ºè¯æ±‡è¡¨\n",
    "            print(\"ğŸ“š æ„å»ºå®Œæ•´è¯æ±‡è¡¨...\")\n",
    "            preprocessor.vocab_builder.build_vocabulary(train_texts_clean)\n",
    "            \n",
    "            # 7. æ–‡æœ¬å‘é‡åŒ–\n",
    "            print(\"ğŸ”¢ å‘é‡åŒ–å®Œæ•´æ•°æ®...\")\n",
    "            train_sequences, train_masks, train_lengths = self._vectorize_texts(\n",
    "                train_texts_clean, preprocessor.vocab_builder, preprocessor.max_length\n",
    "            )\n",
    "            val_sequences, val_masks, val_lengths = self._vectorize_texts(\n",
    "                val_texts_clean, preprocessor.vocab_builder, preprocessor.max_length\n",
    "            )\n",
    "            test_sequences, test_masks, test_lengths = self._vectorize_texts(\n",
    "                test_texts_clean, preprocessor.vocab_builder, preprocessor.max_length\n",
    "            )\n",
    "            \n",
    "            # 8. åˆ›å»ºæ•°æ®é›†\n",
    "            print(\"ğŸ—‚ï¸ åˆ›å»ºå®Œæ•´æ•°æ®é›†...\")\n",
    "            datasets = self._create_datasets(\n",
    "                train_sequences, train_masks, train_labels, train_lengths,\n",
    "                val_sequences, val_masks, val_labels, val_lengths,\n",
    "                test_sequences, test_masks, test_labels, test_lengths\n",
    "            )\n",
    "            \n",
    "            # 9. å‡†å¤‡ä¿å­˜æ•°æ®\n",
    "            vocab_info = {\n",
    "                'token_to_id': preprocessor.vocab_builder.token_to_id,\n",
    "                'id_to_token': preprocessor.vocab_builder.id_to_token,\n",
    "                'vocab_size': preprocessor.vocab_builder.vocab_size,\n",
    "                'special_tokens': preprocessor.vocab_builder.special_tokens\n",
    "            }\n",
    "            \n",
    "            metadata = {\n",
    "                'sequence_length_analysis': {\n",
    "                    'mean_length': np.mean(train_lengths),\n",
    "                    'max_length': np.max(train_lengths),\n",
    "                    'min_length': np.min(train_lengths),\n",
    "                    'total_samples': len(train_sequences)\n",
    "                },\n",
    "                'class_distribution': {\n",
    "                    'train': dict(Counter(train_labels)),\n",
    "                    'val': dict(Counter(val_labels)),\n",
    "                    'test': dict(Counter(test_labels))\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # 10. ä¿å­˜æ•°æ®\n",
    "            DataSaver.save_processed_data_multiclass(datasets, vocab_info, metadata, './processed_data_multiclass')\n",
    "            \n",
    "            print(\"ğŸ‰ å®Œæ•´æ•°æ®å¤„ç†å®Œæˆï¼\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å¤„ç†å¤±è´¥: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def _vectorize_texts(self, texts: List[str], vocab_builder: VocabularyBuilder, max_length: int):\n",
    "        \"\"\"å‘é‡åŒ–æ–‡æœ¬æ•°æ®\"\"\"\n",
    "        sequences, masks, lengths = [], [], []\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            seq, mask, length = vocab_builder.text_to_sequence(text, max_length)\n",
    "            sequences.append(seq)\n",
    "            masks.append(mask)\n",
    "            lengths.append(length)\n",
    "            \n",
    "            if i % 5000 == 0 and i > 0:\n",
    "                print(f\"  å·²å‘é‡åŒ– {i}/{len(texts)} ä¸ªæ ·æœ¬\")\n",
    "        \n",
    "        return sequences, masks, lengths\n",
    "    \n",
    "    def _create_datasets(self, train_sequences, train_masks, train_labels, train_lengths,\n",
    "                        val_sequences, val_masks, val_labels, val_lengths,\n",
    "                        test_sequences, test_masks, test_labels, test_lengths):\n",
    "        \"\"\"åˆ›å»ºæ•°æ®é›†\"\"\"\n",
    "        train_dataset = IMDBDataset(train_sequences, train_masks, train_labels, train_lengths)\n",
    "        val_dataset = IMDBDataset(val_sequences, val_masks, val_labels, val_lengths)\n",
    "        test_dataset = IMDBDataset(test_sequences, test_masks, test_labels, test_lengths)\n",
    "        \n",
    "        return {\n",
    "            'train': train_dataset,\n",
    "            'val': val_dataset,\n",
    "            'test': test_dataset\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d222375-a31b-43ad-bd47-6d8d0d30f049",
   "metadata": {},
   "source": [
    "## å·¥å…·å‡½æ•°\n",
    "\n",
    "**åŠŸèƒ½æ–¹æ³•ï¼š**\n",
    "\n",
    "æ£€æŸ¥å¤„ç†åçš„æ•°æ®è§„æ¨¡å’Œç»Ÿè®¡ä¿¡æ¯\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdc50dad-baf5-4d09-914d-24a28ba3b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_size(data_path: str = './processed_data_multiclass/all_data.pt'):\n",
    "    \"\"\"æ£€æŸ¥å¤„ç†äº†å¤šå°‘æ•°æ®\"\"\"\n",
    "    try:\n",
    "        data = torch.load(data_path)\n",
    "        \n",
    "        print(\"\\n=== æ•°æ®è§„æ¨¡æ£€æŸ¥ ===\")\n",
    "        print(f\"è®­ç»ƒåºåˆ—: {data['train_sequences'].shape}\")\n",
    "        print(f\"è®­ç»ƒæ ‡ç­¾: {data['train_labels'].shape}\")\n",
    "        print(f\"éªŒè¯åºåˆ—: {data['val_sequences'].shape}\") \n",
    "        print(f\"æµ‹è¯•åºåˆ—: {data['test_sequences'].shape}\")\n",
    "        \n",
    "        total_samples = (data['train_sequences'].shape[0] + \n",
    "                        data['val_sequences'].shape[0] + \n",
    "                        data['test_sequences'].shape[0])\n",
    "        \n",
    "        print(f\"\\næ€»æ ·æœ¬æ•°: {total_samples}\")\n",
    "        \n",
    "        if total_samples >= 40000:\n",
    "            print(\"âœ… å¤„ç†äº†å®Œæ•´æ•°æ®é›†\")\n",
    "        elif total_samples >= 20000:\n",
    "            print(\"âš ï¸  å¤„ç†äº†éƒ¨åˆ†æ•°æ®é›†\")  \n",
    "        else:\n",
    "            print(\"âŒ æ•°æ®é‡å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥å¤„ç†è¿‡ç¨‹\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"æ£€æŸ¥å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820dfb2c-faa7-4c25-be0e-2fa021e136bf",
   "metadata": {},
   "source": [
    "## ä¸»ç¨‹åºå…¥å£\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "126b164e-e36b-4132-98be-3e3646e53142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== IMDbæ•°æ®é¢„å¤„ç†å¼€å§‹ ===\n",
      "æ•°æ®ç›®å½•ä¸å®Œæ•´ï¼Œé‡æ–°ä¸‹è½½...\n",
      "ä¸‹è½½IMDbæ•°æ®é›†...\n",
      "ä¸‹è½½å®Œæˆï¼Œè§£å‹ä¸­...\n",
      "IMDbæ•°æ®é›†å‡†å¤‡å®Œæˆ\n",
      "\n",
      "ğŸ“¥ åŠ è½½å®Œæ•´åŸå§‹æ•°æ®...\n",
      "åŠ è½½IMDbæ•°æ®é›†...\n",
      "train æ˜Ÿçº§åˆ†å¸ƒ: {1: 5100, 2: 2284, 3: 2420, 4: 2696, 7: 2496, 8: 3009, 9: 2263, 10: 4732}\n",
      "test æ˜Ÿçº§åˆ†å¸ƒ: {1: 5022, 2: 2302, 3: 2541, 4: 2635, 7: 2307, 8: 2850, 9: 2344, 10: 4999}\n",
      "åŠ è½½äº† 25000 è®­ç»ƒæ ·æœ¬, 25000 æµ‹è¯•æ ·æœ¬\n",
      "åŸå§‹æ•°æ®: 25000 è®­ç»ƒæ ·æœ¬, 25000 æµ‹è¯•æ ·æœ¬\n",
      "ğŸ“Š å®Œæ•´æ•°æ®åˆ’åˆ†...\n",
      "æ‰§è¡Œå¯å¤ç°çš„æ•°æ®åˆ’åˆ†ï¼ˆå¸¦ç±»åˆ«æ£€æŸ¥ï¼‰...\n",
      "æ•°æ®åˆ’åˆ†: è®­ç»ƒé›†=32000, éªŒè¯é›†=8000, æµ‹è¯•é›†=10000\n",
      "è®­ç»ƒé›† ç±»åˆ«åˆ†å¸ƒ: {1: 6478, 2: 2935, 3: 3175, 4: 3412, 7: 3074, 8: 3750, 9: 2948, 10: 6228}\n",
      "éªŒè¯é›† ç±»åˆ«åˆ†å¸ƒ: {1: 1620, 2: 734, 3: 794, 4: 853, 7: 768, 8: 937, 9: 737, 10: 1557}\n",
      "æµ‹è¯•é›† ç±»åˆ«åˆ†å¸ƒ: {1: 2024, 2: 917, 3: 992, 4: 1066, 7: 961, 8: 1172, 9: 922, 10: 1946}\n",
      "æ•°æ®åˆ’åˆ†: è®­ç»ƒé›†=32000, éªŒè¯é›†=8000, æµ‹è¯•é›†=10000\n",
      "ğŸ§¹ æ¸…æ´—å®Œæ•´æ–‡æœ¬æ•°æ®...\n",
      "  æ–‡æœ¬æ¸…æ´— 5000/32000 ä¸ªæ ·æœ¬\n",
      "  æ–‡æœ¬æ¸…æ´— 10000/32000 ä¸ªæ ·æœ¬\n",
      "  æ–‡æœ¬æ¸…æ´— 15000/32000 ä¸ªæ ·æœ¬\n",
      "  æ–‡æœ¬æ¸…æ´— 20000/32000 ä¸ªæ ·æœ¬\n",
      "  æ–‡æœ¬æ¸…æ´— 25000/32000 ä¸ªæ ·æœ¬\n",
      "  æ–‡æœ¬æ¸…æ´— 30000/32000 ä¸ªæ ·æœ¬\n",
      "âœ… æ–‡æœ¬æ¸…æ´—å®Œæˆ\n",
      "  æ–‡æœ¬æ¸…æ´— 5000/8000 ä¸ªæ ·æœ¬\n",
      "âœ… æ–‡æœ¬æ¸…æ´—å®Œæˆ\n",
      "  æ–‡æœ¬æ¸…æ´— 5000/10000 ä¸ªæ ·æœ¬\n",
      "âœ… æ–‡æœ¬æ¸…æ´—å®Œæˆ\n",
      "ğŸ“š æ„å»ºå®Œæ•´è¯æ±‡è¡¨...\n",
      "æ„å»ºè¯æ±‡è¡¨...\n",
      "è¯æ±‡è¡¨æ„å»ºå®Œæˆ: 30000 ä¸ªtoken\n",
      "è¯æ±‡è¡¨è¦†ç›–ç‡: 97.51%\n",
      "ğŸ”¢ å‘é‡åŒ–å®Œæ•´æ•°æ®...\n",
      "  å·²å‘é‡åŒ– 5000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 10000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 15000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 20000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 25000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 30000/32000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 5000/8000 ä¸ªæ ·æœ¬\n",
      "  å·²å‘é‡åŒ– 5000/10000 ä¸ªæ ·æœ¬\n",
      "ğŸ—‚ï¸ åˆ›å»ºå®Œæ•´æ•°æ®é›†...\n",
      "å¼€å§‹ä¿å­˜æ•°æ®åˆ°: processed_data_multiclass\n",
      "âœ… è¯æ±‡è¡¨ä¿å­˜åˆ°: processed_data_multiclass\\vocabulary.json\n",
      "âœ… é…ç½®ä¿å­˜åˆ°: processed_data_multiclass\\preprocessing_config.json\n",
      "âœ… Tensoræ•°æ®ä¿å­˜åˆ°: processed_data_multiclass\\all_data.pt (å¤§å°: 391.39 MB)\n",
      "âœ… å…ƒæ•°æ®ä¿å­˜åˆ°: processed_data_multiclass\\preprocessing_metadata.json\n",
      "\n",
      "ğŸ“ éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶:\n",
      "  all_data.pt (400785.6 KB)\n",
      "  preprocessing_config.json (0.1 KB)\n",
      "  preprocessing_metadata.json (0.7 KB)\n",
      "  vocabulary.json (1391.0 KB)\n",
      "ğŸ‰ å®Œæ•´æ•°æ®å¤„ç†å®Œæˆï¼\n",
      "\n",
      "=== æ•°æ®è§„æ¨¡æ£€æŸ¥ ===\n",
      "è®­ç»ƒåºåˆ—: torch.Size([32000, 512])\n",
      "è®­ç»ƒæ ‡ç­¾: torch.Size([32000])\n",
      "éªŒè¯åºåˆ—: torch.Size([8000, 512])\n",
      "æµ‹è¯•åºåˆ—: torch.Size([10000, 512])\n",
      "\n",
      "æ€»æ ·æœ¬æ•°: 50000\n",
      "âœ… å¤„ç†äº†å®Œæ•´æ•°æ®é›†\n",
      "\n",
      "ğŸ¯ ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # åˆ›å»ºæ•°æ®å¤„ç†æµæ°´çº¿\n",
    "    pipeline = DataProcessingPipeline(\n",
    "        max_vocab_size=30000,\n",
    "        max_length=512, \n",
    "        min_freq=2,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # è¿è¡Œå®Œæ•´å¤„ç†\n",
    "    success = pipeline.run_pipeline()\n",
    "    \n",
    "    if success:\n",
    "        # æ£€æŸ¥æ•°æ®è§„æ¨¡\n",
    "        check_data_size()\n",
    "        print(\"\\nğŸ¯ ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼\")\n",
    "    else:\n",
    "        print(\"\\nâŒ æ•°æ®å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a896b6-6f0a-4c9a-a378-0f5277237f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
